








































The Form Within






 
 
 
 
The Form Within copyright © 2013 Karl H. Pribram

All rights reserved. 
 Printed in the United States of America

 First Edition

No portion of this book may be reproduced in any fashion, print, facsimile,
or electronic, or by any method yet to be developed, without the express
written permission of the publisher.

For information about permission to reproduce selections from this book,
write to:

PROSPECTA PRESS 
 P.O. Box 3131 

 Westport, CT 06880
 www.prospectapress.com

Book production by Booktrix
 Cover illustration of “Nested Brain” by Dr. Karl H. Pribram & Katherine

Neville;
 Graphic representation by HMS Graphic Design Photography;

 Cover design by Barbara Aronica-Buck
 Book design by Katherine Neville

Hardcover ISBN: 978-1-935212-80-5
 Ebook ISBN: 978-1-935212-79-9

http://www.prospectapress.com/


WASHNGTON ACADEMY OF SCIENCES

The Washington Academy of Sciences was incorporated in 1898
as an affiliation of the eight Washington D.C. area scientific
societies. The founders included Alexander Graham Bell and
Samuel Langley, Secretary of the Smithsonian Institution. The
number of Affiliated Societies has now grown to over sixty. The
purpose of the Academy has remained the same over the years:
to encourage the advancement of science and “to conduct,
endow, or assist investigation in any department of science.”

The Academy offers distinguished scientists the opportunity to
submit a book to our editors for review of the science therein.
The manuscript receives the same rigorous scientific review
accorded articles to be published in our Journal. If the
reviewer(s) determine(s) that the science is accurate the book
may use the approved seal of the Academy to demonstrate that it
has met the required criteria.

The Academy is particularly proud to award the seal to Dr. Karl
Pribram who, in 2010, received the Academy’s award for his
Distinguished Career in Science.



Table of Contents
Prologue: The Quest

Preface: The Form Within

Formulations
Chapter 1: Correlations

Chapter 2: Metaphors and Models

Chapter 3: A Biological Imperative

Chapter 4: Features and Frequencies

Navigating Our World
Chapter 5: From Receptor to Cortex and Back Again

Chapter 6: Of Objects and Images

Chapter 7: The Spaces We Navigate and Their Horizons

Chapter 8: Means to Action

A World Within
Chapter 9: Pain and Pleasure

Chapter 10: The Frontolimbic Forebrain: Initial Forays

Chapter 11: The Four Fs

Chapter 12: Freud’s Project

Choice
Chapter 13: Novelty: The Capturing of Attention

Chapter 14: Values

Chapter 15: Attitude

Chapter 16: The Organ of Civilization



Chapter 17: Here Be Dragons

Re-Formulations
Chapter 18: Evolution and Inherent Design

Chapter 19: Remembrance of Things Future

Chapter 20: Coordination and Transformation

Chapter 21: Minding the Brain

Applications
Chapter 22: Talk and Thought

Chapter 23: Consciousness

Chapter 24: Mind and Matter

Chapter 25: Meaning

Appendix A: Minding Quanta and Cosmology

Appendix B: As Below, So Above



Prologue

The Quest 
 
 

When Clerk-Maxwell was a child it is written that he had a
mania for having everything explained to him, and that when
people put him off with vague verbal accounts of any
phenomenon he would interrupt them impatiently by saying,
‘Yes; but I want to know the particular go of it!’ Had the
question been about truth, only a pragmatist could have told him
the particular go of it. . . . Truths emerge from facts; but they dip
forward into facts again and add to them; which facts again
create or reveal new truth. . . . And so on indefinitely. The facts
themselves are not true. They simply are. Truth is the function of
beliefs that start and terminate among them.

—William James, Pragmatism: A New Name for Some Old Ways of
Thinking, 1931



Have you noticed the massive interest shown by the scientific
community in studies of mind and brain? Scientific American is publishing
a new journal, Mind; Psychology Today is headlining studies of the
workings of the brain; Popular Science and Discover are filled with tales
of new discoveries that relate mind to brain and 37,000 neuroscientists are
working diligently to make these discoveries possible.

It has not always been this way. When, as an undergraduate at the
University of Chicago in the 1930s, I took a course on the nervous system,
we stopped short of the brain cortex—and what we now know as the
“limbic systems” was dismissed as a most mysterious territory called the
“olfactory brain.”

It was this very lack of knowledge that attracted me: The Form Within
is the story of my adventures in charting this most important world about
which we then knew so little. In the process, the data I gathered and the
colleagues with whom I worked changed what opinions I had brought to
the research and added sophistication to my interpretations. In retrospect
what impresses me—and what I urge my students to appreciate—is the
long time required, sometimes decades of passion, persistence and
patience, for an idea to ripen into a body of hard evidence and a theoretical
formulation that I was finally able to communicate clearly to others.

The Form Within tells the story of my voyage of discovery during
almost a century of research and theory construction. The story is set
within the frame of the discoveries of the previous century, the 19th, which
totally revised our understanding in the Western world of how our brains
work. During the 17th century and earlier, the human brain was thought to
process air, often termed “spirits.” With the advent of the sciences of
chemistry and of electricity, a basic understanding of brain processes was
attained that, to a large extent, resembles our current views. Writings like
those of William James and Sigmund Freud, around the juncture of the
two centuries, will be referenced repeatedly in the following chapters.

My own story has a somewhat different perspective from that which
is currently available in most neuro- and psychological science books and
journals. For example, my interactions with the 20th century “shapers of
viewpoints” in the brain and behavioral sciences—B.F. Skinner and Karl
Lashley, experimental psychologists; Wilder Penfield, neurosurgeon;
Arthur Koestler, author; John Eccles, neurophysiologist and Karl Popper,



philosopher—expose the questions that puzzled us rather than the dogma
that has become associated with their names.

Thus, The Form Within is the story of my quest. It charts a voyage of
discovery through 70 years of breakthroughs in brain and psychological
research. More than a hundred students have obtained their doctoral and
postdoctoral training in my laboratory and gone on to productive and in
some cases outstanding careers. Collaboration with scientists on five
continents has formed and continues to form my views. The voyage is not
over: each month I’ve had to add to, and in some cases revise, this
manuscript. But as a work in progress, The Form Within is to me an
inspiring chronicle of a most exciting voyage through what, often, at the
time, has seemed to be a storm of experimental and theoretical findings.
As you will see, the concept of form—the form within as formative
causation—provides a novel and safe vessel for this journey.



Preface

The Form Within 
  

 

The first thing we have to say respecting what are called new
views . . . is that they are not new, but the very oldest of thoughts
cast into the mold of these new times.

—Ralph Waldo Emerson, “The Transcendentalist,” 1842



A major thrust in our current changing view of humanity has been our
growing understanding of what our brain does and how it does it. As
always, in times of change, controversies have arisen. Not surprisingly,
some of the most exciting findings, and the theories that are derived from
them, are being swept under the rug. In The Form Within I have enjoyed
placing these controversies within the larger context of complexity theory,
framed by two ways of doing science from the time of classical Greece to
the present. Within such a frame I bring to light findings that deserve our
attention which are being ignored, and I clarify the reasons why they are
being ignored.

Critical to any communication of our brain’s complex inner activity
and its interaction with the world we navigate is the concept of “form”:
The Form Within. We are in the midst of an in-form-ation revolution,
begun in the latter half of the 20th century, that is superseding the
Industrial Revolution of the 19th and early 20th centuries. The Industrial
Revolution had created a vast upheaval through the impact of changes that
occurred in our material world. The information revolution that is now
under way is changing the very form, the patterns of how we navigate that
world.

This revolution has many of the earmarks of the Copernican
revolution of earlier times. That revolution inaugurated a series of
scientific breakthroughs such as those by Galileo, Newton, Darwin and
Freud. Though often unintended, all of these and related contributions
shifted humans from center stage to ever more peripheral players in the
scientific scenario.

The current information revolution redresses and reverses this shift:
Once again science is showing that we humans must be perceived as the
agents of our perceptions, the caretakers of the garden that is our earth,
and how we can be actively responsible for our fellow humans. As during
the Copernican revolution, the articulation of new insights is halting,
often labored, and subject to revision and clarification. But the revolution
is under way, and the brain and psychological sciences are playing a
central role in its formulation. This orientation toward the future is what
has made The Form Within so interesting to write.

This frame provides the form in which we communicate scientific
research and theory. Form is the central theme of this book. Form is
defined in Webster’s dictionary as “the essential nature of a thing as



distinguished from its matter.” What has been lacking in the brain
sciences is a science-based alternative to matter-ialism. Form provides
such an alternative.

The dictionary continues its definition. “Form” forms two currents:
form as shape and form as pattern. Twentieth-century brain scientists felt
comfortable in their explanations using descriptions of shape but less so
with descriptions of pattern. Matter has shape. Most brain scientists are
materialists. By contrast, communication by way of the transmission of
information is constituted of pattern. Despite paying lip service to
describing brain function in terms of information processing, few
classrooms and textbooks make attempts to define brain processing in the
strict sense of measures of information as they are used in the
communication and computational sciences. A shift from the materialism
(explanations in terms of matter) of the Industrial Revolution of the 19th

and early 20th centuries, to this “form-al” understanding of in-form-ation
as pattern is heralding the Communications Revolution today.

But measures of information per se are not enough to convey the
impressions and expressions by which we think and communicate. When
the man in the street speaks of information, he is concerned with the
meaning of the information. Meaning is formed by the context, the social,
historical and material context within which we process the information.
Thus, in The Form Within I address meaning as well as information.

The distinction between form as shape and form as pattern has many
ramifications that make up the substance of the arguments and
explanations I will explore throughout this book. For instance, as
Descartes was declaring that there was a difference between “thinking” (as
pattern) and “matter” (as shape), he was also providing us the way to
trans-form shape and pattern by bringing them together within co-
ordinates—what we know today as “Cartesian coordinates.”

The resolution of our question “what does the brain do and how does
it do it” rests on our ability to discern, in specific instances, the
transformations, the changes in coordinates, that relate our brain to its
sensory and motor systems and the ways in which different systems within
the brain relate to each another. The Form Within gives an account of many
of these transformations, especially those that help give meaning to the
way we navigate our world.



The Cortical Primate
Toward the end of the Last Ice Age—about 10,000 years ago—a new

breed of primates began to leave its mark on Earth. These creatures
possessed a large brain cortex that allowed them to refine—that is, to fine-
tune—their experience by reflecting and acting upon it.

In the autumn of 1998, Katherine Neville and I were granted rare
private invitations to visit the prehistoric painted caves at Altamira, in
Spain. We arrived on a cold, blustery day, and we were happy to enter the
shelter of the ancient caves. Despite the many photos we had seen, nothing
prepared us for the power of these images, hand colored more than 12,000
years ago. I immediately recalled my colleagues’ attempts to get
chimpanzees (who can do so many other things very well) to make
paintings. The difference between the crude swishes that the chimpanzees
lobbed onto flat surfaces and these detailed figures of gazelles and oxen
that adorned the rough stone walls and ceilings in the caves revealed, in a
few seconds, the difference between us humans and other creatures. By
what process had swipes of ochre been refined into such exquisite
painting?

1. Altamira cave painting

What also impressed me was the location of the paintings: Today the
floor where we walked is hollowed out, but when the paintings were made,
the ceiling was so low that no one could stand erect. During prehistoric
times, the painters must have lain on their backs in this restricting
passage. Anyone who has tried to cover a ceiling using a paintbrush has
experienced the aches that go along with doing something from an



unnatural position. And by what light were the paintings executed? There
is no evidence in the caves of smoke from torches. How, in that ancient
time, did the artists illuminate those caves? Just how and to what purpose
did they re-form the world of their caves?

Early human beings refined and diversified—formed —their actions
to re-form their world with painting and song. Storytellers began to give
diverse meanings to these diverse refinements by formulating legends and
myths. In turn, their stories became accepted as constraints on behavior
such as laws (modes of conduct) and religious injunctions (from Latin,
religare, “to bind together, to tie up”). Today we subsume these forms of
cultural and social knowing and acting under the heading “the
humanities.”

Along with stories, early human beings began to formulate records of
diverse observations of recurring events that shaped their lives: the daily
cycle of light and darkness as related to the appearance and disappearance
of the sun; the seasonal changes in weather associated with flooding and
depletion of rivers; the movements of the stars across the heavens and the
more rapid movements conjoining some of them (the planets) against the
background of others; the tides and their relationship to the phases of the
moon—and these to the patterns of menstrual cycles. Such records made it
possible to anticipate such events, and subsequently to check these
anticipations during the next cycle of those recurring events. Formulating
anticipation of their occurrence gave meaning to our observations; that is,
they formed predictable patterns. Today we subsume these patterns, these
forms of knowing about and acting on the world we navigate, under the
heading “the sciences.”

There has been considerable dissatisfaction among humanists with
the materialist approach taken by scientists to understanding our human
condition. What has been lacking is a science-based alternative to
materialism. Form as “the essential nature” of our concerns provides us
such an alternative.

Form as Shape and Form as Pattern
Both form as shape and form as pattern have roots in early Greek

mathematics. Today we associate form as shape with the name Euclid, who
wrote the first known axiomatic theory (a theory based on a few



assumptions from which propositions are logically deduced). Euclid’s
assumptions were devoted to geometry. We were taught Euclid’s geometry
in high school and we are thus familiar with it. In geometry (that is, earth-
metrics) we measure—in the British system—what is in our Yards as we
pace them with our Feet, or in the case of the height of horses, with our
Hands. Geometry is a hands-on way of out-lining shapes. In Euclid’s
geometry, parallel lines do not meet.

By contrast, form as pattern is today associated with the name
Pythagoras. Pythagoras is an elusive figure who is known principally
through the writings of the “school” he founded. We are currently
acquainted with Pythagoras through trigonometry, the study of tri-angles
that are archetypes, “forms or patterns from which something develops.“
The term “archetypes” is derived from tepos, “tapping.” Pythagoras noted
that tapping pieces of metal of different sizes gave rise to different sounds.
He later noticed that, in a similar fashion, tapping taut strings of different
lengths gave rise to oscillations in the string that appeared as waves.
Different sounds were produced by strings of different lengths. A string
divided in two gives rise to sounds an octave higher than the original.
Measurement of the length of strings that give rise to different sounds
could be ordered, that is given ciphers or numbers. From the archetype of
waving strings, music was derived.

The cyclic oscillations of the strings that appear as waves vary in
frequency (density) according to the length

of the oscillating strings. An insight that seems ordinary to us but has
had to be repeatedly achieved anew is that the wave-forms can also be
represented by a circle: perhaps noted by the Pythagoreans as the form
taken by sand spread lightly on the surface of a drum. Achieving this
seminal insight will be encountered on several occasions in the chapters of
The Form Within. But we come across it first with Pythagoras.

Thus, once the circle form is recognized, numbers can be assigned to
different parts of the circle. Pythagoras did this by forming a tri-angle
within the circle, a triangle one of whose angles rests on a point on the
circle. Arranging the numbers within a cycle as a circle allowed
Pythagoras to tri-angulate (as in trigonometry) a location on the circle and
describe the location by its angles (by way of their sine and cosine). These
angles could measure contexts beyond those that we can lay our hands on:
Go to the nearest railroad track— the tracks do not obey the Euclidean



postulate that parallel lines do not meet. Rather the tracks appear to meet
at an angle near the horizon which forms a circle.

Pythagoras had therefore two number systems available: the
frequency or density of the wave-forms and the numbers forming each
cycle.

Note again that geometry develops an active, out-lining method of
measurement that is limited in the range being investigated. By contrast,
relationships among numbers engage an in-formative, an imaginative and
inventive proactive procedure (as in trying to assess the meaning of the
apparent convergence of railroad tracks at our horizon). Form as shape and
form as pattern have led to two very different ways of guiding our
approach to the world we navigate.

In the succeeding chapters of The Form Within, I describe a newer,
still different type of formulation: proactive, dynamical “self-organizing”
processes that occur in our brains and lead to progressively refining our
observations, and thus to refining the targets of our actions and the
diversity of our perceptions. This is the very essence of both the
humanities and the sciences.

Sounds forbidding? It is. But accompany me on this fascinating and
amazing journey and you may be surprised at how comprehensible the
many facets of this three-pound universe have become today. And how an
intimate knowledge of your brain will affect the way you transform your
world.

The Story
Most of The Form Within is organized according to topics and issues

of general interest as viewed from the vantage of those of us working
within the sciences of brain, behavior and mind. However, in the first three
chapters, I address issues such as how to proceed in mapping correlations,
a topic totally ignored in the current surge in using fMRI to localize the
relations between brain and behavior; how I came to use metaphors and
models, which is just coming into focus in doing experiments in
psychology; and how I came to describe self-organizing processes in the
fine-fiber webs and the circuitry of the brain.

The main body of the text begins with the topic of perception. Of
special interest is the fact that, contrary to accepted views, the brain does



not process two-dimensional visual perceptions; rather, the brain handles
many more dimensions of perceptual input. This conclusion is reached
because I make a distinction between the functions of the optic array and
those of retinal processing. Furthermore, evidence shows that there is as
much significant input from the brain cortex to the receptors as there is
input from the environment.

In a subsequent chapter, I present evidence to show that the brain’s
control of action is by way of “images of achievement,” not just motor
programs—and the resultant support of the currently dismissed “motor
theory” of language acquisition.

And there is more: The neural processing of pleasure, which accounts
for the intertwining of pain- and temperature-transmitting fibers in the
spinal cord, is shown to be straightforward; so is the evidence for how
brain processes are involved in setting values and making choices, as well
as how brain processes compose, through interleaving cognitive with
emotional/motivational processes, the formation of attitudes.

Finally, a series of chapters is devoted to new views on encompassing
issues such as evolution, memory, consciousness, mind/brain transactions,
and the relation between information processing and meaning. These
chapters challenge, by presenting alternatives, the prevailing views
presented by philosophers of science, especially those concerning the
relation between brain and mind. Much of what composes these and other
chapters of The Form Within cannot be readily found in other sources.

The first three chapters that follow this preface are organized,
somewhat in the order that I utilized them, according to three different
types of procedure that I used to study what the brain does and how it does
it: 1) Correlations, 2) Metaphors and Models, and 3) The Biological
Imperative.

By correlations I mean discoveries of relationships within the same
context, the same frame of reference (the same coordinate systems.) We
humans, and our brains, are superb at discovering correlations, sometimes
even when they are questionable. Such discoveries have abounded during
the two past decades, by way of PET and fMRI image processors,
techniques that have provided a tremendous surge of activity in our ability
to make correlations between our brain, behavioral and mental processes.
But correlations do not explain the processes, the causes and effects that
result in the correlations.



Metaphors and models, by contrast, are powerful tools that we can
use to guide our explorations of process. We can model a brain process
that we are exploring by doing research on a device, or with a formulation
that serves as a metaphor, and we can then test in actual brain research
whether the process that has been modeled actually works. The Form
Within uses telephone communications, computer programs, and optical-
engineered holographic processes wherever relevant. For the most part our
metaphors in the brain/behavior/mind sciences come from engineering and
mathematics.

Brain and behavioral research (such as that obtained through
correlation and the use of metaphors) often develops questions that lead
directly to brain physiological research. My research led to understanding
a biological imperative: the self-organizing property of complex
processing.

In subsequent chapters of The Form Within—the topic-oriented
chapters—I have noted the results of the application of the insights
obtained as they become relevant.

In Summary
1. The Prologue and Preface of The Form Within have stressed that we

are experiencing a period of revolution in science akin to that which
shaped our views during the Copernican revolution. We humans are
reasserting the centrality of our viewpoints and our responsibility
toward the environment within which we are housed.

2. This reassessment of our centrality can be formulated in terms of a
neglect of a science based on pattern that has resulted in a science
based on matter, an overarching materialism.

3. Form deals with the essence of what is being observed, not only its
matter. Form comes in two flavors: form as shape and form as
pattern.

4. Current science, especially the brain and behavioral sciences, has
been comfortable with explanations in terms of form as the shapes of
matter. But contemporary science is only rarely concerned with form
as patterns.

5. The distinction between form as shape and form as pattern is already
apparent in Greek philosophy. Euclid’s geometry, as taught in our



high schools, uses descriptions of form as shapes. Pythagoreans have
dealt with patterns of numbers that describe wave-forms, angles and
circles, as in tonal scales and in trigonometry.

6. In The Form Within, when appropriate, I trace these two different
formulations of research and theory development in the
brain/behavior/experience sciences.

7. Within this context, I develop the theme that the brain/behavior/mind
relationship receives a major new explanatory thrust based on the
observed biological imperative toward self-organization.



Formulations



Chapter 1
Correlations

Wherein I explore the shapes and patterns that inform the brain’s cortex.

I said to them: ‘Let’s work, we’ll think later.’ To have intentions
in advance, a project, a message—no, never! You must begin by
plunging in. If not, it’s death. You can think—but afterwards,
after it’s finished. One form gives rise to another—it flows like
water.

—Joan Miró, Selected Writings and Interviews, edited by Margit Rowell,
1998



During the 1940s, I worked at the Yerkes Laboratory of Primate
Biology with its director, Karl Lashley, Harvard professor and zoologist-
turned-experimental-psychologist. Those interactions with Lashley were
among the most fruitful of my career. In this chapter as well as subsequent
ones, I discuss the fruits of our discussions and research.

I had chosen Jacksonville, Florida, to practice neurosurgery because
of its proximity to the Yerkes Laboratory. Lashley had been looking for a
neurosurgeon to help operate on the brains of some of the chimpanzees
housed at the laboratories. We were both delighted, therefore, when, in
1946, Lashley gave me an opportunity to inaugurate some research.

The time was propitious. Among those working at the laboratory were
Roger Sperry, who later received a Nobel Prize for his experiments
dividing the human brain into left and right hemispheres and was at that
time transplanting the nerves of amphibians. Donald Hebb was there as a
postdoctoral student, writing a book that was to become extremely
influential but a bone of contention between him and Lashley. The
laboratory was investigating the effect of the deprivation of vision in
infancy upon later visual effectiveness, a set of studies to which I made a
brief but seminal contribution. Most animals recover almost completely
from temporary loss of vision in infancy, but humans develop a more
permanent disability called “ambliopia ex anopsia” (weakness of vision
due to failure to use the eyes). Neither Lashley nor Hebb had known this at
the time, but Don Hebb immediately made this developmentally produced
condition a centerpiece of his subsequent work.

Lashley was using techniques for testing primate behavior that he had
refined from those already widely used in clinical neurology and those he
had developed in testing rats. In Lashley’s theoretical writings, based on
his experience in animal research, mostly with rats, he was known for
being a proponent of the view that the brain’s cortex operated pretty much
as an undifferentiated unit. On the basis of his research, he had formulated
“the laws of mass action and equipotentiality,” which referred to the
findings that the change in problem-solving behavior following brain
damage: a) is proportional to the amount of tissue damage and b) that the
location of the damage is essentially irrelevant. Gary Boring, another
Harvard professor of psychology and the historian of experimental
psychology—noted that inadvertently Lashley’s view of brain function



made it easy for psychologists to ignore the details of brain anatomy and
physiology.

Lashley’s view was decidedly different from mine, which was based
on my experience in brain surgery that consisted of removing tumors
whose location had to be found (at a time before EEGs and brain scans) by
observing the specific behavioral changes the tumors had produced.

I had read Lashley’s views about the brain cortex in his 1929 book
Brain Mechanisms and Intelligence, which I had bought second-hand for
ten cents. I doubted that anyone would currently hold such views, even
teasing Lashley after I got to know him by saying that his ideas seemed to
be worth only the ten cents I had paid for them. But Lashley did indeed
still hold these views, and he was ready to defend them. During a decades-
long and rewarding friendship, we debated whether separate behavioral
processes such as perception, thought, attention and volition were
regulated by spatially separate brain systems, “modules” in today’s
terminology, or whether all such behaviors were regulated by patterns of
processes distributed throughout the brain.

Two decades later, in 1960, George Miller, Eugene Galanter and I
would articulate my side of the argument in our book Plans and the
Structure of Behavior as follows:

There is good evidence for the age-old belief that the brain
has something to do with . . . . mind. Or to use less dualistic
terms, when behavioral phenomena are carved at their joints,
there will be some sense in which the analysis will correspond to
the way the brain is put together . . . . The procedure of looking
back and forth between the two fields [psychology and
neurophysiology] is not only ancient and honorable—it is
always fun and occasionally useful.

By contrast, Lashley noted that

Here is the dilemma. Nerve impulses are transmitted over
definite, restricted paths in the sensory and motor nerves and in
the central nervous system from cell to cell, through definite
intercellular connections. Yet all behavior seems to be
determined by masses of excitation, by the form or relations or



proportions of excitation within general fields of activity,
without regard to particular nerve cells. It is the pattern and not
the element that counts. What sort of nervous organization might
be capable of responding to a pattern of excitation without
limited, specialized paths of conduction? The problem is almost
universal in the activities of the nervous system and some
hypothesis is needed to direct further research.

My task was set.

Inaugural Experiments
Lashley and I decided to engage in experiments that addressed this

issue. Lashley resected small strips of cortex, a cyto-architectural (cellular
architectural) unit, while I chose to remove, in one session, practically all
of a region that had a non-sensory input from the thalamus.

To anticipate—the results were as might be expected: Lashley was
delighted to show that his resections had no effect whatsoever on the
behavior of his monkeys, while my resections produced various sensory-
related effects that had to be analyzed anatomically, neurophysio-
logically, and behaviorally by further experimentation.

I began my research by asking what might be the functions of the so-
called silent areas of the brain cortex. Both in the clinic and in the
experimental laboratory vast areas of cortex failed to yield their secrets.
When tumors or vascular accidents damaged these regions, no consistent
results were observed, and when electrical excitation was applied to the
regions, no results at all were obtained.

The clinical evidence especially was puzzling. Sometimes, cognitive
deficits were obtained, and these were attributed to associations that
occurred between sensory inputs or between sensory inputs and higher-
order brain processes. The deficits were always sensory specific: for
instance, visual, tactile, or auditory (called “aphasia” when restricted to
speech). As von Monikov pointed out in 1914, this raised the problem as
to whether such deficits were always due to simultaneous involvement of
the “silent” cortex and the sensory-specific cortex, or whether they could
be produced by damage to the silent cortex alone. I set out to resolve this
issue by working with selective removals of the silent cortex in monkeys
without damaging the primary sensory input to the brain.



The term “association” was given to these silent cortical areas during
the 1880s by Viennese neurologists because, in patients, damage, when it
was extensive, resulted in cognitive disturbances that were not obtained
when damage was restricted to regions that received one or another
specific sensory input. The Viennese neurologists were intellectually
aligned with British “associationist” philosophy. The British philosophers
and, consequently, the Viennese neurologists, claimed that higher-order
mental processes such as cognition are patched together in the brain by
association from lower-order sensory processes such as vision, audition,
tactile and muscle sensibilities. This view is still dominant in British and
American neuroscience and philosophy.

Against the “associative” view is the fact that the cognitive
disturbances are most often selectively related to one or another sensory
modality. This observation led Sigmund Freud to coin the term “agnosia”
(failure to know) as a modifier for such cognitive disturbances: for
instance, visual agnosia, tactile agnosia. Freud did not share the
associationist view. He proposed an alternative, an analysis of differential
processing, in his classical book On Aphasia.

The Intrinsic Cortex of the Brain
The only way to resolve whether cognitive difficul-ties could arise

from damage that is restricted to the silent cortex was to work with
animals, work enabling me to control the extent of removal and test the
animals over a period of years. The similarity of the monkey brain to the
human made the monkey an ideal choice for this research.

Monkeys have an added advantage: once a part of their brain has been
removed, the rest of the brain becomes reorganized and reaches a stable
condition after a few months. The monkeys can then be tested daily for
years in that stable condition, allowing us to pursue a full exploration of
the condition the monkey and his brain are now in. The animals are
precious and we treat them as we would treat any treasured creature.

The results of my initial program were immediately successful. With
my collaborators, I was able to show that parts of the silent cortex are
indeed responsible for organizing sensory-specific cognitive functions,
without any involvement of adjacent primary sensory systems. Monkeys
have a sufficient reach of cortex that lies between the sensory receiving



regions to allow careful removal without inadvertent damage to these
regions or the tracts reaching them from sensory receptors. My
experimental results showed that indeed sensory-specific deficiencies
were produced by resections of cortex that had no direct connections with
sensory receptors or motor controls. I labeled these parts of the cortex
“intrinsic” (following a nomenclature introduced for the thalamus) by
Jersey Rose, professor at the Johns Hopkins University and “extrinsic”
(those parts of the cortex that had “direct” connections with receptors and
effectors). It is now generally accepted that processing in parts of the
“intrinsic” cortex is sensory specific, while in other parts a variety of
inter-sensory and sensory-motor processes are organized.

My success was due to my bringing to bear neurosurgical
techniques that I had learned to apply in humans, techniques
that allowed me to gently make extensive removals of cortex
without inadvertent damage to underlying and adjacent tissue. I
obtained dramatic changes in behavior: the monkeys were
markedly impaired when they had to make choices despite their
unimpaired performances on tests of perception. I then restricted
the extent of removal and demonstrated that the impairment of
choices is not a “global cognitive” difficulty but is related to
one or another sense modality. Today, these more restricted
regions are commonly known as making up the “sensory-specific
association cortex” (an oxymoron).

A brilliantly conceived study performed on humans at the Weizmann
Institute in Israel has supported the conclusions I obtained with monkeys.
Subjects were shown a variety of videos and fMRIs were recorded while
they were watching. No response was asked for. Rafael Malach concludes:

Examining fMRI activations under conditions where no
report or introspective evaluation is requested— such as during
an engaging movie—reveals a surprisingly limited activation . . .
the very same networks which are most likely to carry high order
self-related processes appear to shut off during intense sensory-
motor perception.



Local networks in the posterior visual cortex were the only ones
activated by a visual percept. Simple visual stimulation engaged the
extrinsic occipital region; greater involvement, as in viewing movies,
engages the intrinsic visual systems of the temporal lobe. (Malach, in his
presentation, used the terms “extrinsic” and “intrinsic,” which I had used
decades earlier.) A fronto-parietal spread of activation occurs with self-
related processing such as motor planning, attentional modulation and
introspection.

Mapping the Brain
Mapping the brain’s relationship to “faculties of mind” had been a

favorite pastime of neurologists since the 19th century, which had spawned
the heyday of phrenology.

It all began with the observations of the Viennese neuro-anatomist,
Francis J. Gall (1758–1828). Gall performed autopsies on patients who had
displayed ailments that could possibly be related to their brains and
correlated the brain pathology he found to the disturbed behavior. He then
mapped his resulting correlations and the maps became ever more
intricate as his observations multiplied. This situation was complicated by
decades-long popular fascination during which observations of the
location of bumps on the head were substituted for dissections of the brain
at autopsy.

Although I had been convinced by my own neuro-surgical training
and experience that useful correlations could be made relating specific
behavioral disturbances to specific brain systems, these 19th- and early
20th-century maps of brain-behavioral relationships seemed ridiculous.
Cognitive faculties, emotional attributes and elementary movements were
assigned locations in the brain cortex in a more or less haphazard fashion.

Reporting the Results of My Research
My approach to mapmaking was different. I was trying to pin down

what part of the temporal lobes that Paul Bucy (with whom I took my first
residency in neurosurgery) had removed was responsible for the monkeys’
failure to make choices among simple visual stimuli. I realized that in
order to do the temporal lobe surgery effectively, my maps must specify
what it is that is being mapped—such as the connections from the



thalamus, the halfway house of sensory input to our brain’s cortex.
Otherwise, the results of my surgery would continue to be the same
hodgepodge of “localization” as that based upon brain damage in humans,
which was then still in vogue.

A map of the major highways of the United States looks considerably
different from one of the identical terrain that shows where mineral
resources are located. Likewise, the results of damaging a part of the brain
cortex would differ if one had disrupted a part of the “railroad” (the
connections within the brain) versus the “mineral resources” (the sensory
input to the brain).

In reporting the results of my research I therefore began by
presenting a series of brain maps. I did not have the resources or the
money to prepare slides, so I drew the maps with colored crayons on
window shades. (The maps shown here are more sophisticated than the
ones I actually drew. Unfortunately, those early maps disintegrated after
several decades, and I have not been able to find photographs of them.)
One window shade presented what we knew of the endings in the cortex of
the sensory input through the thalamus. Another window shade presented
the fine architecture of the arrangement of cells in layers of the cortex.
Still another noted the results of brain electrical stimulation. Finally, after
the initial experiments were completed, I was able to show the effects of
experimental surgical removals of cortex on specific behavioral tests.



2. Cytoarchitectural map

3. Mapping regions of the brain according to the effects of experimental surgical removals on
visual choices



4. Map of the functions of the cortex derived from clinical studies

5. Map of the anatomical output from the cortex



6. Map of the input to the cortex from the thalamus

Presenting the Results of Experiments
I arrived at each lecture site with my five window shades, found

places in the lecture hall to hang them, and pulled the maps down when I
needed them. The high point of this display was reached during a
symposium at the 1950 meeting of the American Psychological Society at
Penn State University. The symposium was chaired by Don Hebb, by now
the chairman of the Department of Psychology at McGill University in
Montreal. It was a time when interest among establishment experimental
psychologists in brain research had reached a nadir, to the point where
they had even abolished the Division of Comparative and Physiological
Psychology of the American Psychological Association.

Being good friends, Hans Lukas Teuber from the Department of
Neurology at New York University, Harry Harlow from the University of



Wisconsin and I as participants therefore decided to attack each other as
bluntly as we could with the hope of creating some interest in the
audience.

Much to our surprise, more than 800 psychologists filled the lecture
hall, even sitting on the steps. The participants of the symposium each
made a presentation; I started the symposium and was roundly attacked
first by Harlow for “using only one or two problems to test my monkeys,”
then by Teuber, who merely stated: “Just like a neurosurgeon, to hog all
the space available (my window shades) only to show that we really
haven’t come very far since the days of phrenology.” When it came my
turn to criticize Harlow, I noted that we at Yale at least knew how to
distinguish the front of the brain from the back; and after Teuber had given
a most erudite and very long talk, quoting every bit of research that had
been performed over the past century, I noted that he seemed not to know
the literature, having missed the seminal volume on the functions of the
motor cortex (edited by my neurosurgical mentor Paul Bucy, who shortly
became head of the Department of Neurosurgery at Northwestern
University).

The audience had quickly caught the spirit in which we were making
our attacks, and the lecture hall rang with laughter. A considerable number
of influential experimental psychologists have told me that their interest
in brain research stemmed from that symposium.

I published my critique of 19th-century mapping procedures in a
1954 paper titled “Toward a Science of Neuro-psychology: Method and
Data.” By this time, I felt that brain mapmaking, even when carefully
based on sustainable correlations, was not sufficient to constitute a
science. Just because we can show by electrical recording or by clinical or
surgical damage that a select place in the brain is involved in color vision
does not mean that it is the or even a “center” for processing color. We
need to establish by behavioral techniques (for instance, tests of form
vision) what else those cells in that location in the brain are doing—and
then we must also show, through carefully controlled physiological
experiments (for instance, by electrical stimulation or electrode
recording), how these particular parts of the brain do what they do.

Beyond Correlation



Correlating an individual’s experience or his test behavior with a
restricted brain location is a first step, but is not enough to provide an
understanding of the brain-behavior-experience relationship. Currently, I
am finding it necessary to stress this caveat once again with the advent of
the new non-invasive brain-imaging techniques that are mostly being used
to map correlations between a particular human experience or a particular
test behavior with particular locations in the brain.

Unfortunately, many of the mistakes made during the 19th century are
being made again in the 21st. On the brain side, finding “mirror neurons”
matters much to materialists who feel that matter has now been shown to
mirror our feelings and perceptions. But, as David Marr (now deceased) of
the Massachusetts Institute of Technology pointed out, all we do with such
microelectrode findings is remove the problem of understanding a process
—in this case of imitation and empathy—from ourselves to the brain. We
still are no further in understanding the ”how” of these processes.

At the behavioral level of inquiry there is a similar lack of
understanding. There is almost no effort being made to classify the
behaviors that are being correlated with a particular locus in the brain.
What do the behaviors have in common that are being correlated with
activity in the cingulate gyrus (or in the anterior insular cortex)? And how
are these behaviors grounded in what we know about the physiology of
these parts of the brain?

When relations of commonalities between behavior and parts of the
brain are noted, they are often mistaken. “Everyone knows the limbic
systems of the brain deal with our emotions.” This would be a nice
generalization—except for the observation that when damage to the
human limbic system occurs, a certain kind of memory, not human
emotion is impaired.

And further—much has been made of the fact that the amygdala deal
with “fear.” Fear is based on pain. The threshold for feeling pain remains
intact after the removal of the amygdala. Fear is also based on memory of
pain. Is memory the contribution of the amygdala to fear? If so, what other
memory processes do the amygdala serve? Chapters 13 and 14 explore
these issues

Among other issues, the chapters of The Form Within recount the
research accomplished during the early part of the second half of the 20th



century that provide answers to these issues—answers that were forgotten
or swept under the rug by subsequent generations of investigators.

Mapping correlations, even when done properly, is only a first step in
reaching an understanding of the processes involved in relating brain to
behavior and to mind— correlations do not tell us “the particular go” of a
relationship.

As David Hume pointed out toward the end of the 17th century, the
correlation between a cock crowing and the sun rising needs a Copernican
context within which it can be understood. Here in The Form Within, I
shall provide the context within which the correlations revealed by the
technique of brain imaging need to be viewed in order to grasp the
complex relationships between our experience, our behavior and our brain.
To do this, I have had to provide a richer understanding of the relationship
between our actions and the brain than simply making maps, which is the
epitome of exploring form as shape. Sensitized by my discussions with
Lashley, without overtly articulating what I was doing in my research, I
thus undertook to explore form as pattern.



Chapter 2
Metaphors and Models

Wherein I trace my discovery of holographic-like patterns of brain
processing.

There is still no discipline of mathematical idea analysis from a
cognitive perspective, namely to apply the science of mind to
mathematics—no cognitive science of mathematics.

A discipline of this sort is needed for a simple reason:
Mathematics is deep, fundamental, and essential to the human
experience. As such, it is crying to be understood. It has not
been.

It is up to cognitive science and the neurosciences to do what
mathematics itself cannot do—namely apply the science of mind
to human mathematical ideas.

—George Lakoff and Rafael Núñez, Where Mathematics Comes From



An Early Observation
In the late 1930s, the University of Chicago neurophysiologist Ralph

Gerard demonstrated to those of us who were working in his lab that a cut
made in the brain cortex did not prevent the transmission of an electrical
current across the cut, as long as the severed parts of the brain remained in
some sort of contact. I was fascinated by the observation that a crude
electrical stimulus could propagate across severed nerves. Gerard
suggested that perhaps his experimental observation could account for our
perceptions, but I felt that our perceptual experience was more intricate,
more patterned, and so needed a finer-grain description of electrical
activity to account for it. I discussed this conjecture with Gerard and with
my physics professor, asking them whether anyone had observed an
extended fine-grain electrical micro-process. Gerard did not know of such
a micro-process (nor did my physics professor), but he emphasized that
something holistic—something more than simple connections between
neurons—was important in understanding brain function.

Later, in the 1950s at Yale University, along with my colleagues,
Walter Miles and Lloyd Beck, I was pondering the neural substrate of
vision. While still at Chicago, I had written a medical school thesis on
retinal processing in color sensation, under the supervision of the eminent
vision scientist Stephen Polyak. In that paper I had suggested that beyond
the receptors of the retina, the next stage of our visual processing is
anatomically organized to differentiate the two or three colors to which
our retinas are sensitive into a greater number of more restricted spectral
bandwidths. Now, at Yale, Miles, Beck and I were frustrated by our
inability to apply what I thought I had learned at Chicago about color to an
understanding of vision of patterns, of form. I remember saying to them,
“Wouldn’t it be wonderful if we had something like a spectral explanation
for processing black-and-white patterns?”

Brain Shapes: Gestalts
Not long afterward, the famous German Gestalt psychologist

Wolfgang Köhler told me about his “direct current” hypothesis as the basis
for cortical processing in vision. Here was the holistic “something” which
I could relate to the results of Gerard’s neurophysiological experiment.
Köhler and his fellow Gestalt psychologists believed that instead of



working through the connections between neurons, our perceptions are
reflected in electrical fields in the brain that actually have a geometrical
shape resembling the shape of perceived objects. This view is called
“geometric isomorphism” (iso = same and morph = shape). The Gestalt
psychologists believed, for example, that if one is looking at a square—a
geometrical shape—the electrical fields in one’s brain also have a square
geometrical shape. Our brain representations will literally “picture” the
significant environment of the organism, or at least caricature the
Euclidian shape of the object while we are observing it.

In the mid-1950s, Köhler enthusiastically demonstrated his electrical
fields to me and to my PhD students Mort Mishkin from McGill
University in Montreal (later to become head of the Laboratory of
Neuropsychology at the United States Institutes of Health) and Larry
Weiskrantz from Harvard (later to become head of the Department of
Experimental Psychology at Oxford University) during one of his visits to
my laboratory at the Institute of Living, a mental hospital in Hartford,
Connecticut. Köhler showed us just how the putative direct current (DC)
electrical activity could anatomically account for visual and auditory
perception.

Köhler would drive from Swarthmore College in Pennsylvania, where
he was teaching, to my laboratory. We would put monkeys in a cage and
take them to MIT, where we had access to the necessary auditory-stimulus
control equipment to do our experiments. At MIT he and I were recording
changes in direct current fields recorded from the auditory cortex of
monkeys while we briefly turned on a loudspeaker emitting a tone.

On other occasions we would meet in New York City to work with
patients who were being operated upon for clinical reasons and who gladly
gave their consent to serve as subjects. In these experiments, Köhler and I
would display a piece of reflective white cardboard in front of the
subjects’ eyes while we made DC electrical recordings from the back of
their brains, where the cortex is located that receives the nerve tracts from
the eye. Indeed, as Köhler had predicted, we found that both monkey
brains and human brains responded with a DC shift when we turned on a
tone or displayed a white cardboard.

More Experiments



Somewhat later (1960), another of the students in my laboratory
(Robert Gumnit, an undergraduate at Harvard) performed a similar
experiment to the one that Köhler and I had done on monkeys. In this new
experiment, we used a series of clicking sounds and again demonstrated a
DC shift, recording directly from the surface of the auditory cortex of cats
as shown in the accompanying figure. Thus we successfully verified that
there is a DC shift in the primary receiving areas of the brain when
sensory receptors are stimulated. Furthermore, we demonstrated that the
DC shift is accompanied by a shift from a slow to a fast rhythm of the
EEG, the recording of the oscillatory electrical brain activity. Previously,
human EEG recordings had shown that such a shift in the frequency of the
EEG occurs when a person becomes attentive (for instance, by opening the
eyes to view a scene, after having been in a relaxed state with eyes closed).

A Neurophysiological Test
In order to explore directly Köhler’s hypothesis that DC brain activity

is involved in perception, I created electrical disturbances by implanting
an irritant in the visual cortex of monkeys. To create the irritation, I filled
small silver discs with “amphogel,” aluminum hydroxide cream, a popular
medication used for peptic ulcers at the time. I then placed a dozen of the
discs onto the visual cortex of each of the monkeys’ two hemispheres.
Amphogel (or alternatively, penicillin), when applied to the brain cortex,
produces electrical disturbances similar to those that initiate epileptic
seizures—large slow waves and “spikes”—a total disruption of the normal
EEG patterns. I tested the monkeys’ ability to distinguish very fine
horizontal lines from fine vertical lines. I expected that, once the electrical
recordings from the monkeys’ visual cortex indicated that electrical
“seizures” had commenced, the monkeys’ ability to distinguish the lines
would be impaired or even totally lost.

Contrary to my expectation, the monkeys were able to distinguish the
lines and performed the task without any deficiency. They were able to
“perceive”—that is, to distinguish whether the lines were vertical or
horizontal”—even though the normal pattern of electrical waves recorded
from their brains had been disrupted. Therefore, “geometric
isomorphism”—the matching of the internal shape of electrical brain



activity to an external form”—did not explain what was happening in the
brain, at least with respect to visual perception.

I told Köhler of my results. “Now that you have disproved not only
my theory of cortical function in perception, but everyone else’s as well,
what are you going to do?” he asked. I answered, “I’ll keep my mouth
shut.”

And indeed, since I had no answer to explain my results, when, two
years later, I transferred from Yale to Stanford University, I declined to
teach a course on brain mechanisms in sensation and perception. I had to
wait a few years for a more constructive answer to the problem.

What the Experiments Did Show
Although the experiments I had conducted had effectively

demonstrated that the shape of what we see is not “mirrored” by the shape
of electrical activity in the brain, as Köhler had believed, my research did
show something very surprising and intriguing: When the brain electrical
activity was disrupted by epileptic seizures, it took the monkeys seven
times longer than usual to learn the same tasks. However, once the animals
started to learn, the slope of the learning curve was identical to the
rapidity of learning by normal monkeys. Only the commencement of
learning had been delayed. The abnormally long delay, which also occurs
in mentally retarded children, may be due to an unusually long time taken
to create the proper background brain activity necessary for learning to
start. (More on this shortly.)



6. Direct Current Field responses from auditory area in response to auditory stimulation. (A)
Shift in response to white noise. (B) Shift in response to tone of 4000 Hertz. (C) Shift in response

to white noise returning to baseline before end of auditory stimulation. (D) Response to 50
clicks/sec. returning to baseline before end of stimulation. (From Gumnit, 1960.)

My finding came about as a consequence of the fact that all of the
implantations of aluminum hydroxide cream had not produced an epileptic
focus. My testing, prior to implantation, of the animals that did not show a
seizure pattern was therefore wasted. So, I began to implant the
epileptogenic cream before commencing testing, and to limit the testing to
those monkeys who had already developed the electrical seizure pattern.
Thus I was inadvertently testing the monkeys who had developed seizure
patterns for their ability to learn as well as their ability to perceive.

The delay in learning had to be examined further. So, in the late
1950s, I devised, along with John Stamm, a postdoctoral fellow in my
Hartford lab, another series of experiments in which we imposed a DC
current across the cortex from surface to depth. We found that a cathodal
(negative-going) current delayed learning while an anodal (positive-going)
current enhanced it.

What we had demonstrated in these experiments was that direct
currents in the brain cortex influence attention and learning, but not
perception.

DC Currents and States of Awareness
More recently, during the 1980s, one of the postdoctoral fellows in

my laboratory at Stanford, when he returned to his laboratory in Vienna,
Austria, demonstrated a large downward shift in the DC current recorded
from the scalp during sleep. This was of great interest to me because
Sigmund Freud had described such a shift in his 1895 paper “A Project for
a Scientific Psychology.” I assume that one or more of his colleagues,
using galvanometers, had made such a demonstration, but there is no
actual record that such experiments had been done.

Even more recently, a large upward shift in the DC current was
recorded from Sufi and other Islamic subjects while they were in a trance
that made it possible for them to painlessly put ice picks through their
cheeks and to perform other such ordinarily injurious feats, not only
without pain but also without bleeding or tissue damage. These same



individuals when not in a trance respond with pain, bleeding and tissue
damage as we all do when injured.

Scientific exploration had come up with some most unexpected
rewards: Looking for brain correlates of perception proved to be a dead
end but revealed surprising results that challenge us to understand the
brain processes operating during attention, learning and states of
awareness.

As Gerard had argued during my Chicago days, direct (DC) currents
must be important for something, but as I had surmised, they are too
global to account for the fine structure of the patterns we experience and
remember. What we needed for perception and memory was something
more like micro-currents. Fortunately, those micro-currents were to appear
in a fascinating context during the next decade.

Perception Revisited
It was not until the early 1960s, a few years into my tenure at

Stanford, that the issue of brain processes in perception would confront me
once again. Ernest (Jack) Hilgard, my colleague in psychology at Stanford,
was preparing an update of his introductory psychology text and asked me
to contribute something about the status of our current knowledge
regarding the role that brain physiology played in perception. I answered
that I was very dissatisfied with what we knew. I described the
experiments, conducted both by myself and others, that had disproved
Köhler’s belief that perception could be ascribed to direct current brain
electrical fields shaped like (isomorphic with) envisioned patterns.

Further, I noted that David Hubel and Thorsten Wiesel had recently
shown that elongated stimuli, such as lines and edges, were the most
successful shapes to stimulate neurons in the primary visual receiving
cortex. They had suggested that perception might result from something
like the construction of stick figures from these elementary sensitivities.
However, I pointed out, this proposal wouldn’t explain the fact that much
of our perception involves patterns produced by shadings and textures—
not just lines or edges. For me, the stick-figure concept failed to provide a
satisfactory explanation of how our brain organizes our perceptual
process. Hilgard asked me to give this important issue some further
thought. We met again about a week later, but I still had nothing to offer.



Hilgard, ordinarily a very kind and patient person, seemed just a bit
peeved, and declared that he did not have the luxury of procrastination. He
had to have something to say in his book, which was due for publication.
He asked me to come up with a viable alternative to the ones I had already
disproved or so summarily dismissed.

Engaging Lashley
I could see only one alternative, one that provided a middle way

between the direct currents of Köhler and the lines and stick figures of
Hubel and Wiesel. Years earlier, during the 1940s, when I had been
involved in research at the Yerkes Laboratory, its director Karl Lashley
had suggested that interference patterns among wave fronts in brain
electrical activity could organize not only our perceptions but also our
memory. Lashley, a zoologist, had taken as his model for patterning
perception those developmental patterns that form a fetus from the
blastula stage to the fully developed embryo. Already, at the turn of the
19th century, the American zoologist Jacques Loeb had suggested that
“force lines” guide such development. At about the same period, A.
Goldscheider, a German neurologist, had wondered if such force lines
might also account for how the brain forms our patterns of perceptions.

Lashley’s theory of interference patterns among wave fronts of
electrical activity in the brain had seemed to suit my earlier intuitions
regarding patterned (as opposed to DC) brain electrical activity. However,
Lashley and I had discussed interference patterns repeatedly without
coming up with any idea of what micro-wave fronts might look like in the
brain. Nor could we figure out how, if they were there, they might account
for anything at either the behavioral or the perceptual level.

Our discussions became somewhat uncomfortable with regard to a
book that Lashley’s postdoctoral student, Donald Hebb, was writing at the
time. Lashley declined to hold a colloquium to discuss Hebb’s book but
told me privately that he didn’t agree with Hebb’s formulation. Hebb was
proposing that our perception is organized by the formation of “cell
assemblies” in the brain. Lashley could not articulate exactly what his
objections were: “Hebb is correct in all his details, but more generally,
he’s just oh so wrong.” Hebb was deeply hurt when Lashley refused to



write a preface to his book—which nonetheless came to be a most
influential contribution to the brain/behavioral sciences.

In hindsight, it is clear that Hebb was addressing brain circuitry,
while Lashley, even at such an early date, was groping for an
understanding of the brain’s deeper connectivity and what that might mean
in the larger scheme of psychological processes.

With respect to my colleague Jack Hilgard’s problem of needing an
answer about brain processes in perception in time for his publication
date, I took his query to my lab group, outlining both my objections to
Köhler’s DC fields and to Hubel and Wiesel’s stick figures. I suggested
that since there were no other alternatives available right now, perhaps
Lashley’s proposal of interference patterns might be worth pursuing. Not
totally tongue-in-cheek, I noted that Lashley’s proposal had the advantage
that we could not object to something when we had no clue of what was
involved.

Microprocessing at Last
Within a few days of my encounters with Jack Hilgard, a postdoctoral

fellow in my laboratory, Nico Spinelli, brought us an article by the
eminent neurophysiologist John Eccles that had appeared in 1958 in
Scientific American. In this article, Eccles pointed out that, although we
could only examine synapses—junctions between brain cells—one by one,
the branching of large fiber axons into fine fibers just as they are
approaching a synapse makes it necessary for us to regard the electrical
activity occurring in these finefibered branches as forming a wave front.
This was one of those “aha” moments scientists like me are always hoping
for. I immediately recognized that such wave fronts when reaching
synapses from different directions, would provide exactly those long-
sought-after interference patterns Lash-ley had proposed.

Despite my excitement at this instant realization, I also felt like an
utter fool. The answer to my years of discussion with Lashley about how
such interference patterns among micro-waves could be generated in the
brain had all the time been staring us both in the face—and neither of us
had had the wit to see it.



Brain Waves, Interference Patterns and
Holography

Fortunately for my self-respect, (and only because “P” comes before
“S” on the mailing list) I received my next issue of Scientific American
before Nico Spinelli received his. In that issue, two optical engineers at
the University of Indiana, Emmet Leith and J. Upaticks, described how the
recording of interference patterns on photographic film had tremendously
enhanced the storage capacity and retrieval of images. Images could now
readily be recovered from the “store” by a procedure (described below)
that had actually been developed by the Hungarian mathematician Dennis
Gabor almost two decades earlier. Gabor had hoped to increase the
resolution of electron microscopy. He had called his procedure
“holography.”

Gabor’s mathematical holographic procedure, which Leith had later
used to produce a holographic process, was a miraculous answer to the
very question Jack Hilgard had posed to me about perception just a few
weeks earlier. Everything in a pattern that we perceive—shading, detail
and texture—could be accomplished with ease through the holographic
process.

Russell and Karen DeValois’s 1988 book Spatial Vision as well as my
own 1991 book Brain and Perception would provide detailed accounts of
each of our own experimental results and those of others who, over the
intervening decades, used Gabor’s mathematics to describe the receptive
fields of the visual system.

Unfortunately, at the time Hilgard’s textbook revision was due, these
insights were as yet too untested to provide him with the material he
needed. But making visible the processing of holographic images showed
me the critical role mathematics would play in our ability to grasp and
interpret the complexity of the brain/ mind relationship.



8. Interference patterns formed by wave fronts of synapses

What is the Holographic Process?
Leith actualized (made visible) Gabor’s mathematical holographic

procedure, but other striking examples of this process are visible to us in
nature every day. A holographic process is nothing more than an
interference pattern created by the interaction of two or more waves. For
example, if you throw a pebble into a pond, a series of concentric circles—
waves—will expand outward. If you drop two pebbles side-by-side, two
sets of waves will expand until they intersect. At the point where the
circles of waves cross, they reinforce or diminish each other, creating what
we call an interference pattern. A remarkable aspect of these interference
patterns is that the information of exactly when and where the pebbles hit
the pond is spread throughout the surface of the water—and that the place
and moment that the pebbles had hit the pond can be reconstructed from
the pattern. We accomplish this by simply reversing the process: if we
filmed it, we run the film backward; or, if we used Gabor’s mathematics,
we simply invert the transformation. Thus, the location and time of the
point of origin can always be recovered from the interference pattern.



9. Add caption here

Try It Yourself
You can simply and directly demonstrate how the holographic process

works by using an ordinary slide projector. Project the image of a slide on
a large white screen and then remove the lens of the slide projector. Now
you see nothing on the screen but a big bright white blur of scattered light.
But if you take a pair of ordinary reading glasses and hold them in the
array of light between the projector and the screen, you will see two
images appear on the screen wherever you place the two lenses of your
glasses. No matter where you hold the reading glasses in the scatter field
of the projected light, the two lenses will create two distinct images of
your original slide on the screen. If four lenses are used, four images will
appear on the screen. A hundred lenses would produce a hundred images.
The “information,” the pattern contained in the original image, in the
absence of the lens, becomes spread, scattered. The lens restores the
image. As the demonstration using the projector indicates, lenses such as
those in our eyes or in the slide projector perform a transformation on the
“information” that is scattered in the light: patterns that seem to be
irretrievably annihilated are in fact still present in every portion of the
scattered light, as the evidence from the use of the eyeglass lenses
demonstrates.

Gabor based his holographic procedure on transforming the space and
time coordinates of our experience, within which we navigate every day of
our lives, into a spectrum of interference patterns. My hope from the
earliest days of my research had now been realized: that we might one day
achieve an understanding of form in terms of spectra akin to that which we
had for our perception of spectra of color.

But the form that the answer to my hopes took, and the broader
implication for our understanding of how we navigate our universe (as



indicated in Appendix A), was and still is to me, totally surprising.
The spectral processing of color and the spectral processing of

patterns must become meshed in the brain. Russ and Karen DeValois, on
the basis of their research, have been able to construct a single
neurological network that can process both color and pattern, depending
on how that network is addressed. The addressing can be initiated either by
the visual input or from a higher-order brain process. Their network
accounts for the observation that we rarely see color apart from form.

Chapters 4 to 8 tell the story of how the scientific community has
dealt with the insights derived from these new ways of thinking, ways
derived from the metaphors and models that have been outlined in the
current chapter.

10. A complete diagram of the proposed stage 3. Russ and Karen DeValois’s Spatial Vision



Chapter 3
A Biological Imperative

Wherein I describe the experimental results that provided substance to the
self-organizing processes in the brain.

In 1957 it was possible to say . . . ‘these considerations also lead
us to the suggestion that much of normal nervous function occurs
without impulses but is mediated by graded activity.’ . . . I
referred to Gerard (1941) who influenced me most in this view
which remained for a long time ignored in the conventional
orthodoxy. I propose that a ‘circuit’ in our context of nervous
tissue is an oversimplified abstraction. . . . that, in fact there are
many types of signals and forms of response, often skipping over
neighbors—and acting upon more or less specific classes of
nearby or even remote elements. Instead of the usual terms
‘neural net’ or ‘local circuit’ I would suggest we think of a
neural throng, a form of densely packed social gathering with
more structure and goals than a mob.

—T. H. Bullock, 1981



Brain Patterns
My 1971 book Languages of the Brain begins:

‘I love you.’ It was spring in Paris and the words held the
delightful flavor of a Scandinavian accent. The occasion was a
UNESCO meeting on the problems of research on Brain and
Human Behavior. The fateful words were not spoken by a
curvaceous blonde beauty, however, but generated by a small
shiny metal device in the hands of a Swedish psycholinguist.

The device impressed all of us with the simplicity of its
design. The loudspeaker was controlled by only two knobs. One
knob altered the state of an electronic circuit that represented
the tensions of the vocal apparatus; the other regulated the
pulses generated by a circuit that simulated the plosions of air
puffs striking the pharynx.

Could this simple device be relevant to man’s study of
himself? Might not all behavior be generated and controlled by
a neural mechanism equally simple? Is the nervous system a
“two knob” dual process mechanism in which one process is
expressed in terms of neuroelectric states and the other in terms
of distinct pulsatile operators on those states? That the nervous
system does, in fact, operate by impulses has been well
documented. The existence of neuroelectric states in the brain
has also been established, but this evidence and its significance
to the study of psychology has been slow to gain acceptance even
in neuro-physiology. We therefore need to examine the evidence
that makes a two-process model of brain function possible.

What needed to be established is that brain patterns are formed by
interactions between: 1) a formative “web” of dynamic states that involves
the fine branches of brain cells (called “dendrites,” Latin for “small
branches”) and their connections (membranes, glial cells, chemical
synapses and electrical junctions) and 2) “circuits” composed of large
fibers (axons) that operate in such a way as a) to sample the web, and b) to
convey the resulting samples from one brain region to another and also
convey samples from sensory receptors to the brain and from the brain to
glands and muscles.



In a sense, the web composes the form of a process, while the circuits
address that form, enabling contemplation or action.

The Retina: Key to Formative Brain Processing
In contrast to the rather straightforward use of the experimental

technique of microelectrode recording to understand the form of
processing in the circuits of the brain, the small diameter of the dendritic
branches makes the electrical recordings of the web a real challenge.
Nonetheless, techniques were developed early on, in the 1950s, that
allowed us to map the receptive fields that are formed in the dendritic
web. How these mappings are made is described in the following sections.

An anecdote highlights the issue: In the early 1970s, I attended an
International Physiological Congress where the keynote speaker was the
noted British scientist A. L. Hodgkin. Two decades earlier, he had received
the Nobel Prize, with A. F. Huxley, for the elegant mathematical
description of the results of their experiments on the generation of nerve
impulses.

Hodgkin began his address with the question: “What does one do
after winning the Nobel Prize?”

He had thought a while about this, he said, and realized that he had
not become a neurophysiologist merely to describe how a membrane
generates a nerve impulse. No, he was interested in how the brain works—
how it regulates the way the body operates, and how it makes our
conscious experience possible. What followed his realization, he told us,
was a brief period of depression: alas, he was not a brain surgeon and
didn’t have the tools to address the problems that initially had challenged
him. (His prize-winning experiments were actually done on the very large
nerve fibers of crayfish.) One morning, however, Hodgkin had awakened
with a promising insight: God, he realized, had put a piece of brain outside
the skull, just so that physiologists such as himself could study it without
having to drill holes to get at it!

That piece of “brain” is the retina of the eye. The retina, like the brain
cortex, is made up of layers of nerve cells with the same fine-fibered
profusion of branches. These fine fibers are present between the layers that
reach from the receptors inward, and they also make up a feltwork, a web,
within each of these layers. Hodgkin went to work on the retina at once.



He told us that he was surprised and amazed by what he found—or rather
by what he didn’t find: Over the next decade, in his many studies, he
hardly ever encountered a nerve impulse, except at the very final stage of
processing—the stage where those nerves with large diameters, large
axons, generate signals that are transmitted to the brain. Therefore,
everything we “see” is first processed by the fine fibers of the retina
before any nerve impulses are actually generated.

11. The Fine-Fibered Neuro-Nodal Web in the Retina

Scanning electron micrograph showing the arrangement of nerve fibers in the retina of Necturus.
Fibers (dendrites) arise in the inner segment and course over the outer segment of a cone. Note
that points of contact do not necessarily take place at nerve endings. (From Lewis, 1970.)

The situation that Hodgkin found in the retina is identical to what
occurs in the web of dendrites in the brain cortex: processing occurs in the
fine fibers, and these processes compose and form the patterns of signals
relayed by axons, the large nerve trunks.

Mapping the Neuro-Nodal Web
At about the same time, in the 1950s, that Hodgkin had begun his

work on the retina, I shared a panel at a conference held in 1954 at MIT
with Stephen Kuffler of the Johns Hopkins University. In his presentation,
Kuffler declared that the “All or None Law” of the conduction of nerve
impulses had to be modified into an “All or Something Law”: That the
amplitude and speed of conduction of a nerve impulse is proportional to
the diameter of the nerve fiber. Thus the speed of conduction and size of a
nerve impulse at the fine-fibered presynaptic termination of axon branches
(as well as in the fine-fibered postsynaptic dendrites) is markedly reduced,



necessitating chemical boosters to make connections (or a specialized
tight junction to make an ephaptic electrical connection).

Kuffler focused my thinking on the type of processing going on in the
fine-fibered webs of the nervous system, a type of processing that has been
largely ignored by neuro-scientists, psychologists and philosophers.

Kuffler went further: he devised a technique that allowed us, for the
first time, to map what is going on in these fine-fibered webs of dendrites
in the retina and the brain. Kuffler noted that in the clinic, when we test a
person’s vision, one of our procedures is to map his visual field by having
him tell us where a spot of light is located. The visual field is defined as
that part of the environment that we can see with a single eye without
moving that eye. When we map the shape of the field for each eye, we can
detect changes due to such things as retinal pathology, pituitary tumors
impinging on the place where the optic nerves cross, strokes, or other
disturbances of the brain’s visual pathways.

Kuffler realized that, instead of having patients tell us when and
where they see a spot, we could—even in an anesthetized animal—let their
neurons tell us. The signal we use to identify whether a neuron recognizes
the spot is the change in pattern of nerve impulses generated in an axon.
The “receptive field” of this axon is formed by its finefibered branches
and corresponds precisely, for the axon, to what the “visual field” is for a
person. By this procedure, the maps we obtain of the axon’s dendritic
receptive field vary from brain cell to brain cell, depending upon what
kind of stimulus (e.g., a spot, a line, a grating) we use in order to elicit the
response: as in the case of whole brain maps, the patterns revealed by
mapping depend on the procedure we use to display the micro-maps.

Kuffler’s initial work with receptive fields, followed by that of others
in laboratories in Sweden and then all over the world, established that the
receptive fields of those axons originating in the retina appear like a
bull’s-eye: some of the retina’s receptive fields had an excitatory (“on”)
center, while others had an inhibitory (“off”) center, each producing a
“complementary” surrounding circular band.

When he used color stimuli, the centers of those receptive fields
which were formed by a response to one specific color—for instance, red
—were surrounded by a ring that was sensitive to its complementary (also
called its opponent) color—in the case of red: green; centers responding to
yellow were surrounded by rings sensitive to blue, and so forth. Having



written my medical student’s thesis on the topic of retinal processing of
color stimuli, I remember my excitement in hearing of Kuffler’s
experimental results. For the first time this made it possible for us to
establish a simple classification of retinal processing of color and to begin
to construct a model of the brain’s role in organizing color perception.

In addition, we learned that in more central parts of the brain’s visual
system, in the halfway house of the paths from the retina to the cortex—
the thalamus (“chamber,” in Latin)—the receptive field maps have more
rings, each ring alternating, as before, between excitation and inhibition—
or between complementary colors.

However, in initial trials at the brain’s cortex, the technique of
moving a spot in front of an eye at first failed to produce any consistent
response. It took an accident, a light somewhat out of focus, to produce an
elongated stimulus, to provide excellent and interesting responses:
neurons were selective of the orientation of the stimulus; that is, different
orientations of the elongated “line” stimulus activated different neurons.

Furthermore, for those neurons that responded, the map of the
receptive field became elongated, with some fields having inhibitory
“flanks,” like elongated rings of the bulls-eye, while others without such
flanks looked more like flat planes. We’ve all learned in high school
geometry that by putting together points we can make lines and putting
together lines at various orientations we can compose stick figures and
planes. Perhaps, thought the scientists, those brain processes that involve
our perceptions also operate according to the rules of geometry that we all
had learned in school.

The scientific community became very excited by this prospect, but
this interpretation of our visual process in terms of shape, as is the case
with regard to brain maps, though easy to understand, fails to explain
many aspects of the relationship of our brain to our experience.

In time, another interpretation, which I shall explore in the next
chapters of The Form Within, would uncover the patterns underlying these
shapes. A group of laboratories, including mine, would provide us a more
encompassing view of processing percepts and acts by the brain.

The Two Processes That Form Patterns in Our
Brains



To review briefly: The circuits of our brain consisting of large nerve
fibers, axons, have sufficient girth to sustain the transmission of nerve
impulses produced by complete depolarization—the discharge of the
electrical polarization—of their membranes. The nerve impulses, acting
like sparks, can travel along the nerve over long distances.

The nature of processing in circuits of the brain has been well
documented and is therefore well established within the neuroscience
community. Neuroscientists have focused on nerve impulse transmission
in large nerve fibers because, due to the available experimental tools over
the past century, this aspect of brain function has been more accessible to
us and has yielded results that are easy to interpret and to understand. (To
anticipate a discussion later in this chapter: circuits are of two sorts,
flexible short-range assemblies and fixed long-range connections.)

By contrast, the fine fibers composing the web—the branches or
dendrites of nerve cells—cannot sustain nerve impulse conduction the way
axons can. The electrical activity of dendrites rarely “sparks” but
oscillates between excitation and inhibition; thus, dendritic activity occurs
locally, forming patches or nodes of synchronized oscillating activity.

The dendritic patches, the nodes, are not only produced by electrical
and chemical activity at the junctions (synapses and ephapses) that
connect the fine fibers, but also independently of what is going on at these
junctions: Dendrites have been found to secrete chemical modulators in
conjunction with the glia or “glue” cells that surround them. The patches
of activity that are carried on within the fine fibers in our brain thus form
a neuro-nodal web.

Unlike the longer-range activity of large nerve fibers, the
coordinating influence of dendrites on neighboring regions of the brain
results only from a local spread of excitation. The fact that dendritic
processing is restricted to such a local patch posed a serious problem for
me back in the mid-1960s when I first suggested its importance. How
could the sort of global brain activity that is necessary to our thought,
perception and memory be achieved from such isolated activities in small
patches? Surprisingly, I found the resolution to my quandary in
astrophysics.



12. The Dendritic Web

13. The Cortical Chem-Web Demonstrated by Staining Acetyl Choline Esterase Receptors

14. The Neuro-Nodal Web

Astrophysicists need to be able to explore large reaches of sky, but
telescopes could capture only a small portion at any one viewing. They
finally solved this problem by converting the images obtained through the
telescope using a mathematical equation, the Fourier transformation— the
procedure that Gabor used to invent holography—that will be referred to
again and again in much of the rest of this book. The procedure works by
Fourier-transforming adjacent images obtained by the telescope; patching
them together and performing another transformation, the reverse of the



initial transformation, on the patched whole, to give a usable image of
large expanses of sky. This same procedure is now used routinely in
current image processing in hospitals such as PET scans and fMRI.

When the formative processing web provided by the dendrites has to
be accessed by the rest of the brain, axons relay the results of these
interactions to other parts of the brain or to receptors and effectors in the
body.

The neuroscience community as a whole has until recently tended to
disregard completely what is going on in the fine-fibered branches, the
dendrites, as well as ignoring the importance of what goes on within these
webs, as illustrated above in the neuro-nodal and chem-webs. This despite
the statements such as those by George Bishop, professor at Washington
University in St. Louis, considered at the time to be the dean of
neurophysiologists (during the 1950s), who repeatedly stated that
processing in the nervous system is a function of its fine fibers; the results
of processing are conveyed to other parts of the nervous system and to
sensory and motor systems by circuits of axons. Another influential
neuroscientist, Ted Bullock, professor at the University of California at
San Diego, on the basis of his research experience, put it even more
bluntly and detailed what is actually occurring in the fine-fibered
processes, as described in the quotation that inaugurated this chapter.

His description of neural transactions as “a densely packed social
gathering in which connections often skip neighbors and act upon more or
less specific classes of nearby or even remote elements” fits what
computer scientists describe as the makings of self-organizing systems.

Hopefully, in the past few years the scene has at last begun to change
as heralded, for example, by an excellent review by R. Douglas Fields in
the June/July 2006 issue of Scientific American MIND of the circuit vs. the
web-like processing in the brain cortex.

Flexible Assemblies and Executive Circuits
The dendritic formative process is directly addressed by the circuitry

of the brain, organized along two very different principles: 1) by local
circuits to form flexible cell assemblies that, in turn, are addressed 2) by
long-range “executive” circuits of larger axons.



The short-range flexible assemblies directly sample the neuro-nodal
web in an organization called “heterarchical” by which the members
connected interact on an equal level. These heterarchical organizations are
the basis of self-organizing procedures in the formation of complex
systems.

The long-range, more or less fixed connections form a “hierarchy;”
that is, each level of organization controls a lower level.

In practice, the distinction between flexible assemblies and long-
range circuits is the distinction between gray and white matter in the
cortex. I have made fairly extensive removals limited to gray matter
(performed with a specially designed suction apparatus that would not
sever the white fibers) with only minor effects on behavior. When,
however, the removals invade the white matter underlying the cells and
their short-range connections that compose the gray matter, extensive
deficits in behavior result. (The distinction between resections of gray and
white matter came to my attention when evaluating the effects of lesions
of parts of the prefrontal cortex. My resections were carefully limited to
gray matter; others were not so careful, so their resections invaded the
white matter connecting other parts of the prefrontal cortex, making their
results uninterpretable.)

Once again, the long-range, more or less fixed connections form a
“hierarchy,” that is, each level of organization controls a lower level.

The two principles, heterarchy and hierarchy appear to be ubiquitous
wherever the members of an organization with equal potential for
interaction become organized.

Warren McCulloch, a lifelong friend and one of the founders of
cybernetics, the study of control systems, used to enjoy telling the
following story regarding the risk of making decisions that are dependent
on heterarchical controls:

After the World War I battle of Jutland, in which many British ships
were disastrously sunk, the British and American navies changed from a
hierarchical control organization, in which every decision had to be
referred to the admiralty, to a heterarchical organization in which control
and the formation of a relevant organization was vested in whomever had
information relevant to the context of the situation. During World War II,
two Japanese air squadrons simultaneously attacked an American fleet in
the South Pacific from different directions. These attacks were spotted by



different sections of our fleet, the information was relayed to the relevant
sections of our fleet and, as was the practice, they were taken as
commands. As a result, the two sections of our fleet steamed off in
separate directions to deal with the separate oncoming attacks, leaving the
admiral on his centrally located ship completely unprotected. Fortunately,
both Japanese attacking forces were routed, leaving the fleet intact.
McCulloch liked to suggest that this story might serve as a parable for
when we are of two minds in a situation in which we are facing an
important decision.

I have learned even more about the nature of human heterarchical and
hierarchical self-organizing systems from Raymond Trevor Bradley.
Bradley did his doctoral dissertation at Columbia University in the 1970s,
observing several communes and meticulously documenting the way each
member of such a closely articulated community engaged each of the
other members. Bradley (Charisma and Social Structure: A Study of Love
and Power, Wholeness and Transformation, 1987) found that each
commune was self-organized by a heterarchical structure made up of
personal emotional transactions as well as a hierarchical structure that
guided the motivations of the overall commune as a whole. Further, the
hierarchy was composed of heterarchical cliques that were often connected
by a single bond between a member of one clique and a member of
another.

Those communes that remained stable over a number of years were
characterized by a balanced expression between heterarchical and
hierarchical processing. Bradley’s results held for all communes whether
they had strong leadership, temporary leadership, or no consistent
leadership. If the motivational hierarchy overwhelmed the emotional
heterarchy, the communes tended to be short-lived. Likewise, if the
emotional structure overwhelmed the motivational structure, the commune
also failed to survive.

The striking parallels between social and brain processing brought
Bradley and me together, and we have collaborated (e.g., Pribram, K. H.
and R. T. Bradley, The Brain, the Me and the I; in M. Ferrari and R.
Sternberg, eds., Self awareness: Its Nature and Development, 1998), in
several brain/ behavior/ mind studies over the years.

These parallels—parallels that compose General Systems Theory—
make a good starting point for inquiry. They form the foundation of our



understanding of the relationship between our world within and the world
we navigate—the contents of Chapters 9 to 17.

At the same time, the parallels tell us too little. Just because a brain
system and a social system have similar organizations does not tell us “the
particular go,” the “how” of the relation between brains and societies. For
understanding the “how” we must know the specific trans-formations
(changes in form) that relate the various levels of inquiry to each other—
brain to body and bodies to various forms of society. I take up the nature
of transformations, what it takes to transform one pattern into another, in
Chapters 18 and 19.



Chapter 4
Features and Frequencies

Wherein I juxtapose form as shape and form as pattern as they are
recorded from the brain cortex and describe the usefulness and drawbacks
of each.

We live in a sea of vibrations, detecting them through our senses
and forming impressions of our surroundings by decoding
information encrypted in these fluctuations.

—Ovidiu Lipan, “Enlightening Rhythms,” Science, 2008

It sometimes appears that the resistance to accepting the
evidence that cortical cells are responding to the two-
dimensional Fourier components of stimuli {is due} to a general
unease about positing that a complex mathematical operation
similar to Fourier analysis might take place in a biological
structure like cortical cells. It is almost as if this evoked, for
some, a specter of a little man sitting in a corner of the cell
huddled over a calculator. Nothing of the sort is of course
implied: the cells carry out their processing by summation and
inhibition and other physiological interactions with their
receptive fields. There is no more contradiction between a
functional description of some electronic component being a
multiplier and its being made up of transistors and wired in a
certain fashion. The one level describes the process, the other
states the mechanism.

—Russell DeValois and Karen DeValois, Spatial Vision, 1988



In this chapter I detail for the visual system the difference between
the two modes of scientific exploration discussed in earlier chapters. One
of these modes explores form as shape, while the other explores form as
pattern. Features can be thought of either as building blocks that determine
the shape of our environment or as components of perceived patterns that
we as, sentient, conscious, beings choose to focus on.

Within the research community the contrast between these two views
of perception was wittily joked about as pitting “feature creatures” against
“frequency freaks.” Feature creatures were those who focused on
“detectors” within our sensory systems, of features in our external
environment —a purely “bottom-up” procedure that processes particulars
into wholes. Frequency freaks, by contrast, focused on similarities
between processes in the visual mode and those in the auditory (and
incidentally the somatosensory) mode. For the frequency freaks, the
central idea was that our visual system responds to the spectral, frequency
dimension of stimulation very much as the auditory system responds to
the oscillations that determine the pitch and duration of sound. The
approach of the frequency freaks, therefore, is “top-down”—wholes to
particulars. Within the neuroscience community, the feature creatures still
hold the fortress of established opinion, but frequency freaks are gaining
ascendance with every passing year.

An Early Intuition
My initial thought processes and the research that germinated into

this section of The Form Within reinforced my deeply felt intuition that the
“feature detection” approach to understanding the role of brain processes
in organizing our perceptions was of limited explanatory value.

Michael Polanyi, the Oxford physicist and philosopher of science,
defined an intuition as a hunch one is willing to explore and test. (If this is
not done, it’s just a guess.) Polanyi also noted that an intuition is based on
tacit knowledge; that is, something one knows but cannot consciously
articulate. Matte Blanco, an Argentine psychoanalyst, enhanced this
definition by claiming that “the unconscious” is defined by infinite sets:
consciousness consists of being able to distinguish one set of experiences
from another.



The “feature detection” story begins during the early 1960s, when
David Hubel and Torsten Wiesel were working in Stephen Kuffler’s
laboratory at the Johns Hopkins University. Hubel and Wiesel had
difficulty in obtaining responses from the cortical cells of cats when they
used the spots of light that Kuffler had found so effective in mapping the
responses in the optic nerve. As so often happens in science, their
frustration was resolved accidentally, when the lens of their stimulating
light source went out of focus to produce an elongation of the spot of light.
The elongated “line” stimulus brought immediate results: the electrical
activity of the cat’s brain cells increased dramatically in response to the
stimulus and, importantly, different cells responded selectively to different
orientations of the elon-gated stimulus.

During the late 1960s and early 1970s, excitement was in the air: like
Hubel and Wiesel, many laboratories, including mine, had been using
microelectrodes to record from single nerve cells in the brain. Visual
scientists were able to map the brain’s responses to the stimuli they
presented to the experimental subject. Choosing the stimulus most often
determined what would be the response they would obtain.

Hubel and Wiesel settled on a Euclidian geometry format for
choosing their visual stimulus: the bull’s eye “point” receptive fields that
characterized the retina and geniculate (thalamic) receptive fields could be
shown to form lines and planes when cortical maps were made. I referred
to these results in Chapter 3 as making up “stick figures” and that more
sophisticated attempts at giving them three dimensions had essentially
failed.

Another story, more in the Pythagorean format, and more compatible
with my intuitions, surfaced about a decade later. In 1970, just as my book
Languages of the Brain was going to press, I received a short reprint from
Cambridge University visual science professor Fergus Campbell. In it he
described his pioneering experiments that demonstrated that the form of
visual space could profitably be described not in terms of shape, but in
terms of patterns of their spatial frequency. The frequency was that of
patterns of alternating dark and white stripes whose fineness could be
varied. This mode of description was critical in helping to explain how the
extremely fine resolution of a scene is possible in our visual perception.
Attached to this reprint was a note from Fergus: “Karl—is this what you
mean?” It certainly was!!



After this exciting breakthrough, I went to Cambridge several times
to present my most recent findings and to get caught up with those of
Campbell and of visual scientist Horace Barlow, who had recently moved
back to Cambridge from the University of California at Berkeley, where
we had previously interacted: he presenting his data on Cardinal cells
(multiple detectors of lines) and I presenting mine on the processing of
multiple lines, to each other’s lab groups.

Campbell was showing that not only human subjects, but brain cells
respond to specific frequencies of alternating stripes and spaces. These
experiments demonstrated that the process that accounts for the extremely
fine resolution of human vision is dependent on the function of the brain
cortex. When the Cambridge group was given its own sub-department, I
sent them a large “cookie-monster” with bulging eyes, to serve as a
mascot.

Once, while visiting Fergus’s laboratory, Horace Barlow came
running down the hall exclaiming, “Karl, you must see this, it’s right up
your alley.” Horace showed me changes produced on a cortical cell’s
receptive field map by a stimulus outside of that brain cell’s receptive
field. I immediately exclaimed, “A cortical MacIlvain effect!” I had
remembered (not a bad feat for someone who has difficulty remembering
names) that an effect called by this name had previously been shown to
occur at the retina. Demonstrating such global influences on receptive
fields indicated that the fields were part of a larger brain network or web.
It took another three decades for this finding to be repeatedly confirmed
and its importance more generally appreciated by the neuroscience
community.

These changes in the receptive fields are brought about by top-down
influences from higher-order (cognitive) systems. The changes determine
whether information or image processing is sought. Electrical stimulation
of the inferior temporal cortex changes the form of the receptive fields to
enhance information processing— as it occurs in communication
processing systems. By contrast, electrical stimulation of the prefrontal
cortex changes the form of the receptive fields to enhance image
processing—as it is used in image processing such as PET scans, fMRI
and in making correlations such as in using FFT. The changes are brought
about by altering the width of the inhibitory surround of the receptive
fields. Thus both information and image processes are achieved,



depending on whether communication and computation or imaging and
correlations are being addressed.

15. Cross sections of thalamic visual receptive fields (n) showing the influence of electrical
stimulation of the inferior temporal cortex (i) and the frontal cortex (f)

Encounter at MIT
In the early 1970s, I was invited to present the results of these

experiments at the Massachusetts Institute of Technology. The ensuing
discussion centered not on the results of my experiments but—ironically,
it being MIT— on my use of computers. David Hubel stated that the use of
computers would ruin visual science by taking the research too far from a
hands-on, direct experience necessary to understanding the results of
observation. I retorted that in every experiment, in the mapping of every
cell, I had to use the direct hands-on mapping before I could engage the
computer to quantify my observations. I stated that in order to obtain the
results that I had shown, I needed stable baselines—baselines that could
only be statistically verified by the use of a computer. Only by using these
baselines, could I show that the changes produced by the electrical
stimulation were reliable.

Then my host, Hans-Lukas Teuber, professor and chair of the
Department of Psychology at MIT, rose to support Hubel by saying that
the use of computers would also ruin behavioral psychology. He was an
old friend of mine (our families celebrated each Thanksgiving together),



so I reminded him that my laboratory had been completely computerized
since 1959, when I’d first moved to Stanford. That alone had made it
possible to test 100 monkeys a day on a variety of cognitive (rather than
conditioning) tasks. I pointed out that he should be pleased, since he was a
severe critic of operant conditioning as a model for understanding
psychological processing.

I came away from the MIT lecture disappointed by the indifference to
the remarkable and important experimental results obtained both at
Cambridge and in my laboratory, a disappointment that persisted as the
feature approach to visual processing became overwhelmingly dominant
during the 1990s.

Feature Detection vs. Feature Extraction
Within the feature approach, two very different procedures can be

followed. In one, the emphasis is on detecting some element in the sensory
input, a bottom-up direction of inquiry; in the other, the feature is
extracted from a myriad of sensory and cognitive processes in a top-down
fashion. Extraction involves identification of the feature, essentially a
creative process.

Different cells in the brain’s visual cortex differ in their response to
the orientation, to the velocity or to the acceleration of movement of a
line. We, as visual creatures, can analyze any scene into lines and edges;
however, the maps of the brain’s receptive fields were initially interpreted
not as analyzers of an entire vista but as brain cells that were “detecting”
lines. A detector, as the word implies, needs to be uniquely sensitive to
that which is being detected. A Geiger counter, fitted to detect
radioactivity, would be useless if it were sensitive to everything in an
environment including auditory noise, aromas, changes in visual
brightness, or the contours of a shape.

The feature detection approach had an initial advantage in exploring
form as Euclidian shapes. Maps of the receptive fields recorded from
visual pathways yielded an “irresistible” progression: the maps change
from a circular shape in the retina and the thalamus to an elongated shape
at the cortex. An important aspect of the maps was that they were elicited
only by stimuli of specific orientations (horizontal, vertical and all angles
in between.) These oriented elongated shapes—interpreted as lines and



edges—could then be considered to be a two-dimensional “stick-figure“
process representing features of our visual environment. But, as is well
known, lines need not actually be present for us to perceive a shape.

16. The Kaniza Triangle

For example:
Contrary to what was becoming “established,” we showed in my

laboratory that the supposed “Geiger counter” cells demonstrated
sensitivities to many stimuli far beyond their sensitivity just to the
presentation of lines. Some of these cells also responded to select
bandwidths of auditory stimulation. All of them respond to changes in
luminance (brightness), while other cells respond also to color. As
described in Chapter 2, the Berkeley brain scientists Russell and Karen
DeValois, on the basis of their decades-long experimental results, have
constructed a model showing that the same network of cells can provide
for our experience both of color and of form: When connected in one
fashion, processing leads to the experience of form; when connected in
another fashion, our experience is of color. The fact that a single network
can accomplish both is important because we rarely experience color apart
from form.

Perhaps more surprising was our finding that some of these cells in
the primary visual sensory receiving cortex, when recorded in awake
problem-solving monkeys, respond to whether the monkey’s response to a
visual stimulus had been rewarded or not—and if so, whether the monkey
had pressed one panel or the other in order to receive that reward.

A Show of Hands



To illustrate how such a network might function, I ask the students in
my class to do the following: “All those who are wearing blue jeans,
please raise your hand. Look at the distribution of hands. Now lower them.
Next, all those wearing glasses, please raise your hands. Look at the
distribution of hands and note how different it is from the distribution
produced by those wearing blue jeans.” Again, “All girls, raise your hands;
then all boys.” We quickly observe that each distribution, each pattern, is
different. The features—blue jeans, glasses, gender—were coded by the
various patterns of raised hands, a pattern within a web, not by the
characteristic of only a single individual person,

Brain cells are like people when you get to know them. Each is
unique. All features are distributed among the group of individuals, but
not all the individuals share all the features. Also, as in the saying “birds
of a feather flock together,” brain cells sharing the same grouping of
features tend to form clusters. Thus, one cluster will have more cells that
are sensitive to color; another cluster will have more cells that are
sensitive to movement. Still, there is a great deal of overlap of features
within any cluster. People are the same in many respects. For instance,
some of us have more estrogen circulating in our bodies while others have
more testosterone. But all of us have some of each.

With this arrangement of “features as patterns,” the question
immediately arises as to who asks the question as I did with my students:
Who is wearing blue jeans? This question is known as an example of a
top-down approach to processing, whereas feature detection is a bottom-up
approach. In my laboratory, my colleagues and I spent the better part of
three decades establishing the anatomy and physiology of the routes by
which top-down processing occurs in the brain. Together with the results
of other laboratories, we demonstrated that electrical stimulation of the
sensory-specific “association” regions of the brain cortex changed the
patterns of response of sensory receptors to a sensory input. Influence of
such stimulation was also shown to affect all the “way stations” that the
sensory input traversed en route to the sensory receiving cells in the
cortex.

Even more surprising was the fact that the effects of an auditory or
touch stimulus would reach the retina in about the same amount of time
that it takes for a visual stimulus to be processed. Tongue in cheek, I’ve
noted these results in the phrase: “We live in our sensory systems.” The



experimental findings open the important question as to where all our
processing of sensory input begins, and what is the sequence of
processing. That question will be addressed shortly and again in depth
when we analyze frames and contexts.

What Features Are Featured?
But meanwhile, to return for a moment to the notion of line detectors:

the visual cortex cannot be processing straight lines or edges per se
because the input from the retina is not received in the form of a straight
line. As Leonardo da Vinci showed in the 15th century, the retina is a
hemisphere. A perceived line must therefore be abstracted from a curve.
When you fly from New York to London you travel via Newfoundland,
Iceland and Greenland in an arc that describes the shortest distance
between two points on the globe. On a sphere, the shortest distance on a
globe is not a straight line. Riemann developed a geometry that applies to
curved surfaces. The brain’s visual processing, therefore, must operate
according to Riemannian principle and must be based on Pythagorean arcs,
angles and cones, not on the points and lines of Euclidian geometry.
Experimental psychologist James Gibson and his students at Cornell
University have taken this aspect of vision seriously, but their work needs
to be incorporated by brain scientists in their account of visual sensory
processing.

A Symposium in the Roman Tradition
I was able to bring the attention of the neuroscience community to the

issue raised during the encounter at MIT and to the spatial frequency
formulation of visual receptive fields at a convention of the Society for
Neuroscience held in Minneapolis (in the mid-1970s.) Mortimer Mishkin
—a friend and former doctoral and postdoctoral student of mine during the
1950s—had, in his capacity of program chairman, asked me to organize a
special symposium covering microelectrode studies in visual
neuroscience. The symposium was to be special in that it would be held in
the Roman tradition: the symposiasts arguing their respective views over
flasks of wine!

The society was still of a manageable size (it has since grown to
36,000 members) and attendance at this particular symposium was



restricted to an audience of 300. Wine and cheeses were served to all.
Symbolically (both from a political and a geographical standpoint) I

placed Russell and Karen DeValois from the University of California at
Berkeley to my left, and, to my right, Horace Barlow from Cambridge
University and David Hubel from Harvard. Hubel, together with Torsten
Wiesel, also of Harvard, had recently received the Nobel Prize for their
work on “feature detection,” and I was pleased that he had graciously
consented to participate in what was meant to be a challenging session.

I asked David to go last in our series of presentations, hoping that, if
allowed to imbibe enough wine, he would be sufficiently mellow to
welcome a discussion of any disagreements with his views on the neural
substrate of perception. My plan worked.

Russell and Karen DeValois presented their data, which provided
solid evidence for a frequency description of the pattern of receptive fields
in the visual cortex. They also presented critical evidence against the
formulation that line or edge detection of shapes best describes those
receptive fields. Horace Barlow then presented evidence for how groups of
cells in the brain extract a feature from the multiple features that
characterize a visual scene, as opposed to the view that any single brain
cell responds solely and uniquely to a particular feature such as a line.

Finally, I introduced David Hubel. In my introduction, I noted three
things:

1. that David was a modest person, who would claim that he hadn’t
understood the mathematical aspects of the DeValois presentation—
but I would not let him get away with this because he had taught high
school math before going to medical school;

2. that, in my experience, each cortical neuron encodes several features
of our visual environment, that each neuron is like a person with
many attributes, and thus our ability to recognize features had to
depend on patterns displayed by groups of neurons for further
processing;

3. that, as just presented, Karen and Russ DeValois had done critical
experiments showing that the visual cortical cells were tuned to
spatial frequency rather than to the shapes of “lines” or “edges.”



Hubel immediately announced to the audience, “Karl is right in
everything he says about cortical cells, but I have never seen anything like
what Russell DeValois is talking about.” Russ DeValois responded, “You
are welcome to come to our laboratory at any time and see the evidence
for yourself.” Hubel replied, “But I wouldn’t believe it if I saw it.” I
chimed in, “David I didn’t realize you were a flatlander.” The symposium
was on.

Hubel went on to point out that although cells might respond to many
inputs, he was convinced they responded more to what he called a “critical
sensory stimulus.” As a moderator I felt that I had said enough and didn’t
challenge him on this, but as demonstrated in my laboratory and in many
others, although his statement is correct for the receptive fields in further
processing stations, (where the so-called “grandmother cells” are located)
this characterization is incorrect for receptive fields in the primary
sensory receiving cortex.

Hubel did admit that his views on feature detection had received little
confirmation over the decades since he had first proposed them, but the
identification of so-called “grandmother” cells that responded especially
well to higher-order configurations, such as hands and (grandmothers’)
faces, kept his faith in the feature detector view alive.

The audience and I felt amply rewarded by the exchange. Issues had
been clearly engaged in a friendly and supportive fashion. This was
science at its best.

Several issues had been aired. One, highlighted in Barlow’s
presentation, is, of course, that feature extraction —as described earlier in
this chapter by the metaphor of a show of hands—is eminently plausible
but should not be confused with feature detection.

A second issue, highlighted by the DeValoises, and explored in the
next chapter—sensory processing— involves the frequency domain.

Green Hand Cells
Finally, Hubel’s faith in “grandmother cells” was well placed. As we

proceed from the primary sensory receiving cortex to the temporal lobe of
the brain, we find receptive fields that are “tuned” to an ever “higher
order,” that is, to ever more complex stimuli. The receptive fields of these
“grandmother cells” are nodes in an assembly that is capable of



representing features as higher-order properties abstracted from lower-
order properties. Thus, we can come to understand how a property like
“blue jeans” in the metaphor I’ve used can become abstracted from the
population of persons who wear them. Sales counters at retail stores
handle blue jeans; cells in the brain’s temporal lobe respond to pictures of
grandmother faces and other sensory images such as flowers and hands.

Cells responding primarily to pictures of hands were the first
“grandmother cells” to be discovered. Charles Gross at Princeton
University was the first to record them. I asked Gross, who had attended a
course I was teaching at Harvard and as a consequence became a student
of Larry Weiskrantz at Cambridge and Oxford, whether the face cells
responded to stimuli other than faces. They did, but not as vigorously (as
noted by Hubel in our symposium). However, in my laboratory, we
unexpectedly found, in one monkey who’d had considerable experience
choosing a green rather than a red set of stripes, that his cell’s activity
went wild when he was shown the combination as a green hand!

As had also been found by Gross, in experiments in my laboratory, we
found that the amount of activity elicited from these “hand cells” was
almost as great when we visually presented broad stripes to the monkey.
We presented stripes because they could provide the basis for a frequency
interpretation of how the cells might process a hand.

We can derive two important conclusions from the results of these
experiments:

1. Higher-order processing locations in the brain enable us to make
distinctions among complex stimuli.

2. Our ability to make such distinctions can be learned.

Seeing Colors
James and Eleanor Gibson of Cornell University, using purely

behavioral techniques, had also reached the conclusion that higher-order
distinctions are learned. The Gibsons reported their results in a seminal
publication in which they showed that perceptual learning is due to
progressive differentiation of stimulus attributes, not by forming
associations among them.

Taking Matte Blanco’s definition that our conscious experience is
based on making distinctions, as described in the opening paragraphs of



this chapter, this line of research indicates how our brain processes,
through learning, make possible progressively greater reaches of
conscious experience.

The processes by which such distinctions are achieved were
addressed at another meeting of the Society for Neuroscience—this one in
Boston in the 1980s. Once again, David Hubel gave a splendid talk on how
brain processes can entail progressively greater distinctions among colors.
He noted that, as we go from receptor to visual cortex and beyond, a
change occurs in the coordinates within which the colors are processed:
that processing changes from three colors at the receptor to three opponent
pairs of colors at the thalamus and then to six double-opponent pairs at the
primary sensory cortex. Still higher levels of processing “look at the
coordinates from an inside vantage” to make possible the experiencing of
myriads of colors. A change in coordinates is a transformation, a change in
the code by which the form is processed.

Transformations are the coin of making ever-greater distinctions,
refinements in our conscious experience. After his presentation in Boston,
I spoke with David Hubel and said that what he had just done for color was
exactly what I was interested in doing for form. David said he didn’t think
it would work when it came to form. But of course it did. The prodigious
accomplishments of the 1970s and 80s form the legacy of how visual
angles at different orientations can become developed into a Pythagorean
approach to visual pattern vision. (Hubel and I have never had the
opportunity to discuss the issue further.)

Form as Pattern
A different path of experimentation led me to explore the view of

form as pattern. In my earlier research, while examining the effects of
cortical removals on the behavior of monkeys, I had to extensively
examine the range of responses to stimuli that were affected by removing
a particular area of the brain cortex. Thus, during the 1960s, when I began
to use micro-electrodes, I used not only a single line but two lines in my
receptive field experiments. The relationship of the lines to one another
could thus be explored. Meanwhile, other laboratories began to use
multiple lines and I soon followed suit.



Multiple lines of different width and spacing (that is, stripes) produce
a “flicker” when moving across the visual field. We experience such a
flicker when driving along a tree-lined road: the rapid alternation of such
light and shadow stripes can actually incite a seizure in seizure-prone
people. The rapidity of the flicker, the speed of the alternation between
light and dark can be described mathematically in terms of its frequency.

When we moved such stripes before the eye of a subject in our
experiments, we could change the response of the subject’s cortical
receptive field by changing the frequency of the stimulation in two ways:
by altering the speed at which we moved the stimulus, or by changing the
width of the stripes and the spaces between them.

Mathematically, this relationship among the angles on the retina that
result from stimulation by stripes is expressed as degrees of arc of the
portion of a circle. A circle, as noted earlier, represents a cycle. Degrees of
arc are measures of the density, the rapidity of oscillations between light
and dark in the cycle. Thus, the oscillations are the spectral transform of
the spatial widths of the alternating light and dark stripes. It follows that
changes in the width of the light and dark stripes will change the
frequency spectrum of the brain response.

Those of us who used multiple lines in our experiments have
described our results in terms of “spatial frequency.”Frequency over space
as distinct from frequency over time can be demonstrated, for example, if
we visually scan the multiple panels that form a picket fence or similar
grating. When we visually scan the fence from left to right, each panel and
each gap in the fence projects a pattern of alternating a bright region and a
shadow region onto the curve of our retina. Therefore, each bright region
and each shadow covers an angle of the curve of our retina. The “density”
of angles is defined by how close the stimulating bright and dark regions
(the width of the fence panels and their spacing) are to each other.



17. Low- and High-Frequency Gratings

18. Receptive Field and Contours

In our hearing, within the acoustical domain, we have long been
accustomed to thinking in terms of the temporal frequency of a sound



determining its pitch. In seeing, we now had demonstrated that a visual
pattern, an image, is determined in the same fashion by its spatial
frequency. “Wow! The eye works just like the ear.” This insight— that all
senses might function in a similar mode—was a most dramatic revelation.
My long-sought quest for a way to understand the processing of the visual
as radiation, similar to the processing of color, was at last realized.

Waves: A Caveat
In the early 1960s, when the holographic formulation of brain

processes was first proposed, one of the main issues that had to be
resolved was to identify the waves in the brain that constituted a
hologram. It was not until I was writing this book that the issue became
resolved for me. The short answer to the question as to what brainwaves
are involved is that there are none.

For many years David Bohm had to chastise me for confounding
waves with spectra, the results of interferences among waves. The
transformation is between space-time and spectra. Waves occur in space-
time as we have all experienced. Thus the work of Karen and Russell
DeValois, Fergus Campbell and the rest of us did not deal with the
frequency of waves but with the frequency of oscillations— the frequency
of oscillations expressed as wavelets, as we shall see, between
hyperpolarizations and depolarizations in the fine-fiber neuro-nodal web.

Waves are generated when the energy produced by oscillations is
constrained as in a string—or at a beachfront or by the interface between
wind and water. But the unconstrained oscillations do not produce waves.
Tsunamis (more generally solitons) can be initiated on the east coast of
Asia. Their energy is not constrained in space or time and so is spread over
the entire Pacific Ocean; their effect is felt on the beaches of Hawaii. But
there are no perceptible waves over the expanse of the Pacific Ocean in
between. The oscillations that “carry” the energy can be experienced
visually or kinesthetically, as at a beach beyond the breakers where the
water makes the raft bob up and down in what is felt as a circular motion.
In the brain, it is the constraints produced by sensory inputs or neuronal
clocks that result in various frequencies of oscillations that we record in
our ERPs and EEGs.



Critical Tests
Fergus Campbell provided an interesting sidelight on harmonics, the

relationships among frequencies. The test was critical to viewing the
perception of visual form as patterns of oscillation. When we hear a
passage of music, we hear a range of tones and their harmonics. A
fundamental frequency of a tone is produced when a length of a string is
plucked. When fractions of the whole length of the string also vibrate,
harmonics of the fundamental frequency are produced. When we hear a
passage of music, we hear the fundamental tones with all of their
harmonics. Surprisingly, we also hear the fundamental tones when only
their harmonics are present. This is called experiencing the “missing
fundamental.”

I was able to experience another exciting moment when in the 1970s I
was visiting Cambridge, Fergus Campbell demonstrated that in vision, as
in audition, we also experience the “missing fundamental.” Yes, the eye is
like the ear.

The results of two other sets of experiments are critical to
demonstrating the power of the frequency approach to analyzing visual
patterns. Russell and Karen DeValois and their students performed these
experiments at the University of California at Berkeley. In one set, they
mapped an elongated receptive field of a cortical neuron by moving a
single line in a particular orientation across the visual field of a monkey’s
eye. (This form of mapping had been the basis of the earlier interpretation
that cells showing such receptive fields were “line detectors.”) The
DeValoises went on to widen the line they presented to the monkey until it
was no longer a line but was now a rectangle: The amount of the cell’s
response did not change! The cell that had been described as a line
detector could not discriminate—could not resolve the difference—
between a line and a rectangle.

In this same study, the experimenters next moved gratings with the
same orientation as that of the single line across the visual field of the
monkey. These gratings included a variety of bar widths and spacing
(various spatial frequencies). Using these stimuli, they found that the
receptive field of the cell was tuned to a limited bandwidth of spatial
frequencies. Repeating the experiment, mapping different cortical cells,
they found that different cells responded not only to different orientations
of a stimulus, as had been found previously, but that they were tuned to



different spatial frequencies: the ensemble of cortical cells had failed to
discriminate between a line and a rectangle but had excellent resolving
power in distinguishing between band widths of frequencies.

19. The missing fundamental: note that the patterns do not change: the bottom, as you view it, is
the pattern formed with the fundamental missing

For the second set of experiments the DeValoises elicited a receptive
field by moving a set of parallel lines (a grating) across the monkey’s
receptive field. As noted earlier, one of the characteristics of such fields is
their specificity to the orientation of the stimulus: a change in the
orientation of the lines will engage other cells, but the original cell
becomes unresponsive when the orientation of the grating is changed.

Following their identification of the orientation selectivity of the cell,
the DeValoises changed their original stimulus to a plaid. The pattern of
the original parallel lines was now crossed by perpendicular lines.
According to the line detector view, the cell should still be “detecting” the
original lines, since they were still present in their original orientation.
But the cell no longer responded as it had done. To activate the cell, the
plaid had to be rotated.

The experimenters predicted the amount of rotation necessary to
activate the cell by scanning each plaid pattern, then performing a
frequency transform on the scan using a computer program. Each cell was
responding to the frequency transform of the plaid that activated it, not
detecting the individual lines making up the plaid. Forty-four experiments



were performed, and in 42 of them the predicted amount of, rotation came
within two minutes of arc; in the other two experiments the results came
within one degree of predicting the amount of rotation necessary to make
the cell respond. Russell DeValois, a rather cautious and reserved person,
stated in presenting these data at the meeting of the American
Psychological Association that he had never before had such a clear
experimental result. I was delighted.

20. Tuning functions for two representative striate cortex cells. Plotted are the sensitivity
functions for bars of various widths (squares) and sinusoidal gratings of various spatial

frequencies (circles). Note that both cells are much more narrowly tuned for grating frequency
than for bar width. (From Albrecht et al., 1980, copyright 1980, AAAS. Reprinted by permission.)

Signs of the Times
An excellent example of the strengths and limitations of currently

accepted views regarding the feature detection versus the frequency views
is provided by a Scientific American article (April 2007) entitled “The
Movies in Our Eyes,” authored by Frank Werblin and Botond Roska. The
article describes a series of elegant experiments performed by the authors,
who recorded the electrical activity of ganglion cells of the retina of
rabbits. It is the ganglion cells that give rise to the axons that relay to the
brain the patterns developed during retinal processing of radiant energy.
The authors had first classified the ganglion cells into 12 categories
depending on their receptive fields, their dendritic connections to earlier
retinal processing stages. These connections were demonstrated by
injecting a yellow dye that rapidly spread back through all of the dendrites
of the individual ganglion cell under investigation.



21. Orientation tuning of a cat simple cell to a grating (squares and solid lines) and to
checkerboards of various length/width ratios. In A the responses are plotted as a function of the

edge orientation; in B as a function of the orientation of the Fourier fundamental of each pattern.
It can be seen that the Fourier fundamental but not the edge orientation specifies the cell’s

orientation tuning (From K. K. DeValois et al., 1979. Reprinted with permission.)

The authors recorded the patterns of signals generated in each of the
different types of ganglion cell by a luminous square and also by the
presentation of a frame of one of the author’s illuminated faces. They then
took the resulting patterns and programmed them into an artificial neural
network where they were combined by superposition. The resulting
simulation was briefly (for a few milliseconds) displayed as an example of
a retinal process, a frame of a movie, sent to the brain. In sequence, the
movies showed a somewhat fuzzy depiction of the experimenter’s face and
how it changed as the experimenter talked for a minute.

In their interpretation, the experimenters emphasized the spatial
aspect of the representation of the face and the time taken to sequence the
movies. They also emphasize the finding that the retina does a good deal
of preprocessing before it sends a series of partial representations of the
photic input to the brain for interpretation: “It could be that the movies
serve simply as elementary clues, a kind of scaffolding upon which the
brain imposes constructs” (p. 74). Further: “ . . . the paper thin neural
tissue at the back of the eye is already parsing the visual world into a
dozen discrete components. These components travel, intact and
separately, to distinct visual brain regions—some conscious, some not.
The challenge to neuroscience now is to understand how the brain



interprets these packets of information to generate a magnificent, seamless
view of reality” (p. 79).

However, the authors ignore a mother lode of evidence in their own
findings. The electrical activity, both excitatory and inhibitory, that they
record from the ganglion cells, even from the presentation of a square,
show up as wavelets! This is not surprising since the input to the ganglion
cells is essentially devoid of nerve impulses, being mediated only by way
of the fine-fiber neuro-nodal web of the prior stages of processing—a fact
that is not mentioned in the Scientific American paper. Wavelets are
constrained spectral representations. Commonly, in quantum physics, in
communication theory and in neuroscience the constraint is formed in
space-time. Thus, we have Gabor’s quanta of information.

Werblin and Roska speak of “packets of information” but do not
specify whether these are packets of scientifically defined (Shannon)
information or a more loosely formulated concept. They assume that the
space-time “master movie” produced by the retinal process “is what the
brain receives.” But this cannot be the whole story because a considerable
amount of evidence has shown that cutting all but any 2% of the optic
nerve does not disturb an animal’s response to a visual stimulus. There is
no way that such an ordinary master movie could be contained as such in
just 2% of any portion of the optic tract and simply repeated in the other
98%. Some sort of compression and replication has to be involved.

Given these observations on Werblin and Roska’s experimental
results and the limitations of their interpretations, their presentation can
serve as a major contribution to the spectral view of visual processing as
well as the authors’ own interpretation in terms of a feature view. The
spectral aspect of their data that they ignore is what is needed to fill out
their feature scaffolding to form what they require to compose the
seamless view of the world we navigate.



22. The Retinal Response to a Rectangle

23. Activity of a Second Ganglion Cell Type

Revisiting Geometry and Trigonometry
The strengths and weaknesses of the feature and the frequency

(spectral) views of brain processes in perception provide a fresh look at
the way science works. The experiments that led to the feature view
showed the importance of the orientation of the stimulus. The experiments
upon which the frequency view is based showed that the cortical cells were



not detectors of stimulus “elements” but were responsive to interrelations
among them. Furthermore, the frequency approach demonstrated that the
power and richness of a spectral harmonic analysis could enable a full
understanding of neural processing in perception. Unfortunately, those
holding to the feature approach have tended to ignore and/or castigate the
frequency approach and the results of its experiments. There is no need to
exclude one or the other approach because the results of harmonic analysis
can be translated into statistics that deal with features such as points; and
into vectors that deal with features such as oriented lines.

There is a good argument for the ready acceptance of the feature
detection approach. It is simple and fits with what we learned when taking
plane geometry in high school. But the rules of plane geometry hold only
for flat surfaces, and the geometry of the eye is not flat. Also, the rules of
plane geometry hold only for medium distances, as a look at the meeting
of parallel railroad tracks quickly informs us. A more basic issue has
endeared brain scientists to the feature detection approach they have
taken: the rules of plane geometry begin with points, and 20th century
science insisted on treating events as points and with the statistical
properties of combinations of points which can be represented by vectors
and matrices. The frequency approach, on the other hand, is based on the
concept of fields, which had been a treasured discovery of the 19th
century.

David Marr, Professor of Computational Science at MIT, was a strong
supporter of the feature view. But he could not take his simulations beyond
two dimensions— or perhaps two and a half dimensions. Before his
untimely death from leukemia, he began to wonder whether the feature
detector approach had misled him and noted that the discovery of a cell
that responded to a hand did not lead to any understanding of process. All
the discovery did was shift the scale of hand recognition from the
organism to the organ, the brain.



24. The Receptive Fields of Seven Ganglion Cell Types Arranged Serially

As I was writing the paragraphs of this chapter, it occurred to me that
the resolution to the feature/frequency issue rests on the distinction
between research that addresses the structure of circuits vs. research that
addresses processes at the deeper neuro-nodal level of the fine-fibered
web. The circuits that characterize the visual system consist of parallel
channels, the several channels conveying different patterns of signals,
different features of the retinal process. (My colleagues and I identified
the properties of x and y visual channels that reach the cortex as identical
to the properties of simple and complex cortical cells.) Thus, feature
extraction, the creative process of identifying features, can be readily
supported.

The frequency approach uses the signals recorded from axons to map
the processes going on in the fine- fibered dendritic receptive fields of
those axons. This process can generate features as they are recorded from
axons. Taking the frequency approach does not require a non-quantitative
representation of the dendritic receptive fields, the holoscape of the neuro-
nodal process. Holoscapes can be quantitatively described just as can
landscapes or weather patterns. The issue was already faced in quantum
physics, where field equations were pitted against vector matrices during
the early part of the 20th century. Werner Heisenberg formulated a



comprehensive vector matrix theory and Erwin Schrödinger formulated a
comprehensive set of wave functions to organize what had been
discovered. There was a good deal of controversy as to which formulation
was the proper one. Schrödinger felt that his formulation had greater
representational validity (Anschaulichkeit). Heisenberg, as was Niels
Bohr, was more interested in the multiple uses to which the formulation
could be put, an interest that later became known as the Copenhagen
implementation of quantum theory which has been criticized as being
conceptually vacant. Schrödinger continued to defend the conceptual
richness of his approach, which Einstein shared. Finally, to his great relief,
Schrödinger was able to show that the matrix and wave equations were
convertible, one into the other.

I shared Schrödinger’s feeling of relief when, three quarters of a
century later, I was able to demonstrate how to convert a harmonic
description to a vector description using microelectrode recordings taken
from rats’ cortical cells while their whiskers were being stimulated. My
colleagues and I showed (in the journal Forma, 2004) how vector
representations could be derived from the waveforms of the receptive
fields we had plotted; but showed that the vector representation, though
more generally applicable, was considerably impoverished in showing the
complexity of what we were recording, when compared with the surface
distributions and contour maps from which the vectors were extracted.

25. The Holoscape



26. An example for the three-dimensional representation of the surface distribution and
associated contour map of the electrical response to buccal nerve stimulation. The surface

distributions were derived by a cubic interpolation (spline) procedure. The contour maps were
abstracted from the surface distributions by plotting contours in terms of equal numbers of spikes

per recording interval (100 msec). The buccal nerve stimulation whisked the rat’s whiskers
against teflon discs whose textures (groove widths that determined the spatial frequency of the

disc) varied from 200 to 1000 microns and whose grooves were oriented from 0 to 180 degrees.
Separate plots are shown for each frequency of buccal nerve stimulation: 4Hz, 8Hz, and 12 Hz

(top to bottom).



27. The cosine regressions of the total number of spikes for the orientation of the stimulus for the
4 Hz condition are shown.



28. Vectors

The use of both the harmonic and the vector descriptions of our data
reflect the difference between visualizing the deep and surface structures
of brain processing. Harmonic descriptions richly demonstrate the
holoscape of receptive field properties, then patches—the nodes—that
make up the neuro-nodal web of small-fibered processing. The vector
descriptions of our data describe the sampling by large-fibered axons of
the deep process.

In Summary
The feature detection approach to studying how our brain physiology

enables our perceptions, outlines a Euclidian domain to be explored. This
approach, though initially exciting to the neuroscience community, has
been shown wanting: research in my laboratory and that of others showed



that the receptive fields of cells in the brain’s visual cortex are not limited
to detecting one or another sensory stimulus.

Instead, research in my laboratory, and that of others, showed that a
feature, relevant to a particular situation, is identified by being extracted
from a set of available features.

The frequency approach, based on recording the spectral dimensions
of arcs of excitation subtended over the curvature of the retina, is
essentially Pythagorean. Deeper insights into complex processing are
obtained, insights that can provide the basis for identifying features.

Neither approach discerns the objects that populate the world we
navigate, the subject of Chapter 6.



Navigating Our World



Chapter 5
From Receptor to Cortex and Back Again

Wherein I trace the transformation of perceived patterns as they influence
the brain cortex and as the brain cortex influences them.

If you throw two small stones at the same time on a sheet of
motionless water at some distance from each other, you will
observe that around the two percussions numerous separate
circles are formed; these will meet as they increase in size and
then penetrate and intersect each other, all the while retaining
as their respective centers the spots struck by the stones. And the
reason for this is that the water, though apparently moving, does
not leave its original position. . . . [This] can be described as a
tremor rather than a movement. In order to understand better
what I mean, watch the blades of straw that because of their
lightness float on the water, and observe how they do not depart
from their original positions in spite of the waves underneath
them

Just as the stone thrown in the water becomes the center and
causes various circles, sound spreads in circles in the air. Thus
every body placed in the luminous air spreads out in circles and
fills the surrounding space with infinite likeness of itself and
appears all in all and all in every part.

—Leonardo da Vinci, Optics

Enough has now been said to prove the general law of
perception, which is this, that whilst part of what we perceive
comes through our senses from the objects before us another



part (and it may be the larger part) always comes out of our own
head.

—William James, Principles of Psychology, 1890



Lenses: How Do We Come to Know What Is Out
There?

In the early 1980s the quantum physicist David Bohm pointed out
that if humankind did not have telescopes with lenses, the universe would
appear to us as a holographic blur, an emptiness of all forms such as
objects. Only recently have cosmologists begun to arrive at a similar view.
String theoreticians, the mathematician Roger Penrose and Stephen
Hawking (e.g., in A Universe in a Nutshell) have, for their own reasons,
begun to suspect that Bohm was correct, at least for exploring and
explaining what we can about the microstructure of the universe at its
origin and horizon. But the horizon itself is still a spatial concept, whereas
Bohm had disposed of space entirely as a concept in his lens-less order.

With respect to the brain, I have extended Bohm’s insight to point out
that, were it not for the lenses of our eyes and similar lens-like properties
of our other senses, the world within which we navigate would appear to
us as if we were in a dense fog—a holographic blur, an emptiness within
which all form seems to have disappeared. As mentioned in Chapter 2, we
have a great deal of trouble envisioning such a situation, which was
discovered mathematically in 1948 by Dennis Gabor in his effort to
enhance the resolution of electron microscopy. Today, thanks to the
pioneering work of Emmet Leith in the early 1960s, we now have a
palpable realization of the holographic process using laser optics.

Holography
Leith’s procedure was to shine a beam of coherent (laser) light

through a half-silvered angled mirror that allowed some of the beam to
pass straight through the mirror, and some of it to be reflected from its
surface. The reflected portion of the laser light was beamed at right angles
to become patterned by an object. The reflected and transmitted beams
were then collected on a photographic plate, forming a spectrum made by
the intersections of the two beams. Wherever these intersections occur,
their amplitudes (heights) are either reinforced or diminished depending
on the patterns “encoded” in the beams. A simple model of the
intersections of the beams can be visualized as being very much like the
intersections of two sets of circular waves made by dropping two pebbles
in a pond, as described by Leonardo da Vinci.



When captured on film, the holographic process has several
important characteristics that help us understand sensory processing and
the reconstruction of our experience from memory. First, when we shine
one of the two beams onto the holographic film, the patterns “encoded” in
the other beam can be reconstructed. Thus, when one of the beams has
been reflected from an object, that object can again be viewed. If both
beams are reflected from objects, the image of either object can be
retrieved when we illuminate the other.

An additional feature of holography is that one can retrievably store
huge amounts of data on film simply by repeating the process with slight
changes in the angle of the beams or the frequency of their waves (much
as when one changes channels on a television set). The entire contents of
the Library of Congress can be stored in 1 cubic centimeter in this manner.

Shortly after my inauguration to Emmet Leith’s demonstration of
holography, I was invited to a physics meeting held in a Buddhist enclave
in the San Francisco Bay area. I had noted that the only processing feature
that all perceptual scientists found to be non-controversial was that the
biconcave lens of the eye performed a Fourier transform on the radiant
energy being processed. Jeffrey Chew, director of the Department of
Physics at the University of California at Berkeley, began the conference
with a tutorial on the Fourier relationship and its importance in quantum
physics. He stated that any space-time pattern that we observe can be
transformed into spectra that are composed of the intersections among
waveforms differing in frequency, amplitude, and their relation to one
another, much as Leonardo Da Vinci had observed.

My appetite for understanding the Fourier transformation was
whetted and became amply rewarded.

Fourier’s Discovery
Jean Baptiste Joseph Fourier, a mathematical physicist working at the

beginning of the 19th century, did an extensive study of heat, that is,
radiation below the infrared part of the visible spectrum. The subject had
military significance (and thus support) because it was known that various
patterns of heat could differently affect the bore of a cannon. Shortly
thereafter, the field of inquiry called “thermo-dynamics” became
established to measure the dissipation of heat from steam engines: the



amount of work, and the efficiency (the amount of work minus the
dissipation in heat) with which an engine did the work would soon be
major interests for engineers and physicists. The invention of various
scales that we now know as “thermo-meters”—Fahrenheit, Celsius and
Kelvin—resulted, but what interested Fourier was a way to measure not
only the amount of heat but rather to measure the spectrum of heat. The
technique he came up with has been discovered repeatedly, only to have
slipped away to be rediscovered again in a different context. We met this
technique when we discussed the Pythagoreans, but both Fourier and I had
to rediscover it for ourselves. The technique is simple, but if Fourier and
other French mathematicians had difficulty with it, it may take a bit of
patience on your part to understand it.

What is involved is looking at spectra in two very different ways: as
the intersection among wave fronts and as mapped into a recurring circle.

To analyze the spectrum that makes up white light, we use a prism.
What Fourier came up with to analyze the spectrum of heat can be thought
of as analyzing spectra with a “clock.” The oscillations of molecules that
form heat can be displayed in two ways: 1) an up-and-down motion (a
wave) extended over space and time, or 2) a motion that goes round and
round in a circle as on a clockface that keeps track of the daily cycle of
light and darkness.

Fourier, as a mathematician, realized that he could specify any point
on a circle trigonometrically, that is by triangulating it. Thus he could
specify any point on the circumference of the circle, that is, any point on
an oscillation by its sine and cosine components. The internal angle of the
triangle that specifies the point is designated by the cosine; the external
angle, by the sine.

When he plotted these sine and cosine components on the continuous
up-and-down motion extended over space and time, the sine wave and the
cosine wave are displaced from one another by half a cycle. Where the two
waveforms intersect, those points of intersection are points of interference
(reinforcement or diminution of the height of oscillations) just as at the
intersection of Leonardo’s waves created by pebbles dropped into a pool.
The heights of the points of intersection are numbers that can be used to
calculate relations between the oscillations making up the spectra of heat.
In this manner Fourier digitized an analogue signal.



Thinking back and forth between the two ways by which waveforms
can be plotted is not easy. It took Fourier, a brilliant mathematician,
several decades to work out the problem of how to represent the measure
of heat in these two ways. One of my “happier times” was reading, in the
original French and in English translations, Fourier’s stepwise groping
toward this solution. My reason for being so happy was that I had tried for
myself to understand his solution for over a decade and found not only that
my solution was correct, but that Fourier had gone through the steps
toward the solution much as I had. Further, I have just learned that
quantum physicists are rediscovering this analytic “trick”: they speak of
“modular operators” which divide the processes under investigation into
clock-like repetitious segments. I asked my colleagues, “Like Fourier?”
and received a nod and smile: I had understood.

29. Representing Oscillations as Waveforms and as Circular Forms

Meanwhile, as he was also active in politics during a most turbulent
period in French history, Fourier had been put in jail and released several
times before being appointed head of a prefecture. When Napoleon came
to power, he soon recognized Fourier’s genius and took him to Egypt.
Although French mathematics was the best in Europe, something might
still be learned from the Arabs, whose mathematics had flowered some
thousand years earlier.

Fourier the Man
In addition to being given the opportunity to learn something about

Arab mathematics firsthand, Fourier was put in charge of recording and
describing Egyptian cultural artifacts for the expedition; he and his team
made sketches of the architecture, sculpture and painting. In many
instances, these sketches remain the only record we have of Egyptian
monuments that have subsequently been destroyed. Fourier spent the rest



of his life preparing these sketches for publication and providing financial
and intellectual support for young Egyptologists such as his protégé
Champollion, famed for deciphering Egyptian hieroglyphics by using the
Rosetta stone (brought back to Europe by the same expedition).

On his return to Paris, Fourier first presented his “Analytic Theory of
Heat” that had attained some maturity during his tour in Egypt. Within
two weeks of this presentation, Laplace countered with a refutation
showing that Fourier had not worked out an acceptable proof of his
theoretical formulation. Laplace was correct in his critique of Fourier’s
proof: it took many years before a plausible proof was worked out. But the
overriding importance of the tool that Fourier had devised for the
measurement of the spectrum of heat made it possible for him to continue
his mathematical explorations.

Fourier persisted. He had been appointed head of his local prefecture
and paid for the publication of his fur- ther work using the salary of his
office. Meanwhile, a great many scientists and engineers were using his
technique to solve their problems in a variety of disciplines. After a
decade, when Fourier was awarded a prestigious prize, Laplace began to
support the publication of his work and his election to the French
Academy of Sciences.

Over nearly two centuries, Fourier’s theorem has become widely
regarded as a most useful formula for measuring and predicting recurring
patterns—from those characterizing bull and bear markets in the stock
exchange to possible meltdowns in nuclear reactors. Richard Feynman, the
noted Nobel laureate in physics, stated in his lectures (Feynman, Leighton
and Sands, 1963) that Fourier’s theorem is probably the most far-reaching
principle of mathematical physics.

As expressed by my colleagues at the University of California at
Berkeley, professors of physiology Russell and Karen DeValois, there is
every reason to expect this over-arching formulation to also play a key
role in how we can understand sensory processes: “Linear systems
analysis originated in a striking mathematical discovery by a French
Physicist, Baron Jean Fourier, in 1822 . . . [, which] has found wide
application in physics and engineering for a century and a half. It has also
served as the principal basis for understanding hearing since its
application to audition by Ohm (1843) and Helmholtz (1877). The
successful application of these procedures to the study of visual processes



has come only in the last two decades” (DeValois and DeValois, Spatial
Vision, 1988).

FFT: The Fast Fourier Transform
The mathematical technique that Fourier developed was published in

final form in 1822 as Théorie Analytique de la Chaleur (Analytical Theory
of Heat). Today, the once- controversial formula is the basis of the theory
known as the “Fourier series”—and with its inverse, the “Fourier
transformation”—provides a method whereby we can reduce any
perceived configuration, any pattern, no matter how complex, into a series
of numbers representing intersections among waveforms. Further, any
such series can be restored to its original configuration by performing the
transform again, using its inverse.

What Fourier’s method provides us so beautifully is the ability to
decompose a configuration, as we experience it, into a series of component
parts. For instance, the first component might take in a broad sweep of a
scene, of a sound, or of an EEG, and describe it by a low frequency. The
next component may then focus on somewhat narrower aspects of the
same configuration, and describe it by a somewhat higher frequency. Each
subsequent component resolves the pattern into higher and higher
frequencies. A series of a dozen components usually succeeds in
representing the experienced space-time configuration in sufficiently fine
grain to restore it faithfully when the inverse transform is performed.
(Recall that performing a Fourier transformation twice returns the original
configuration.) The successive components in the Fourier equation
accomplish an increasing resolution, providing a finer grain, such as
texture, to the configuration.

Fourier’s pioneering method has led to many modifications of his
insight, modifications that are grouped under the heading of orthogonal
transformations, that is, transformations whose components do not interact
with each other. In addition, non-linear modifications have been devised,
such as that developed by Norbert Wiener, professor of mathematics at the
Massachusetts Institute of Technology, for analyzing the EEG.

Furthermore, the goal of analyzing patterns into a series of
component parts can now be accomplished by statistics in which each
component in the series is represented by an “order.” Fourth order (that is



four terms in the series) must be used to reach the resolution necessary to
represent a line or the texture of a visual scene. Statistics, based on
enumerating items or things, are usually relatively easy to manipulate in
calculations. Thus, statistical techniques are taught to every student who
aspires to use or to understand the fruits of scientific enterprise. However,
there are drawbacks to the teaching of statistics to the exclusion of other
analytical techniques: in the next chapter I discuss how statistical analysis
hides the forms behind the manipulations, the symmetries better
understood in spectral terms.

This deeper understanding is one reason why frequency aficionados
continue to tout the spectral domain. In addition, there is a most useful
practical application: transforming into the spectral domain (and back out
again) makes the computation of correlations, the basis of “imaging,”
simpler, whether by brain or computer. (The procedure uses “convolution,”
a form of multiplication of the spectral transformations of what needs to
be correlated.)

I remember how we cheered the achievement of a computer program
that would perform the Fourier transformation, the Fast Fourier Transform
(FFT), while I was at the Center for Advanced Studies in the Behavioral
Sciences at Stanford, California, in 1958–59. As a result, its use has
become commonplace in medical image processing as in PET (positron
emission tomography) scans and fMRI (functional Magnetic Resonance
Imaging.) These successes raise the question: Might biological sensory
functions also be based on spectral image processing? The next sections
show that, not only can the Fourier theorem be usefully applied, but that
its application resolves some longstanding problems in the understanding
of the neurobiology of visual perception.

A Retinal Image?
James Gibson, renowned for his work on sensory perception at

Cornell University, was adamant in his rejection of the widely accepted
notion that an image of a scene is displayed on the retina by the optics of
the eye, a view taken seriously by most scientists. This view suggests that
the eye is like a classical camera that is constructed so that the lens
sharply focuses the scene onto the film to produce a two-dimensional
spatial image. The retina is conceived to be like the film in the camera.



This analogy would mean that our three-dimensional perceptual
experience must be constructed from this two-dimensional “retinal
image.”

Gibson pointed out that this conception is faulty if only because there
is constant movement in the relation between our eyes and the scene
before us. His students also regarded as significant the hemispheric shape
of our retina: their sophisticated view is that retinal excitation is described
by cones of the Riemannian geometry of curved space, not by Euclidian
points and lines. So why hasn’t their view been more generally accepted?

For me, the problem arose because, while at the University of
Chicago, I had seen a retinal image, an image that appeared on the retina
of the eye of an ox. Still, I felt that the Gibsonians were on to something
important in placing their emphasis on (Pythagorean three-dimensional
triangular) cones of light subtending degrees of arc on the retina. My
discussions with Gibson did not help to solve the problem, because he
insisted that all of the “information” (that is, the multidimensional
patterns) was already there in the scene, and all the brain’s visual system
needed to do was to “resonate” to that information. I agreed in principle
but argued that we needed to know “the particular go” of how such
resonances would become established. Gibson had proposed a relational,
ecological approach to studying the information contained in the scene
before us; I countered that we need an internal as well as an external
ecology to explain the visual processing of the scene. Whenever I tried to
bring brain facts into the discussion, Gibson laughed and turned off his
hearing aid. I accused him of being the radical behaviorist of perception. It
was all great fun, but did not resolve the issue.

The Aha Experience
Resolution of the question “Is there a retinal image?” came in a flash

while I was crossing a busy street in Edmonton. I had been asked to
present the MacEachron lectures at the University of Alberta, Canada,
lectures which were subsequently (1991) published as a book, Brain and
Perception. The fourth lecture concerned processing by the optical
structures of the eye. I had explained to my audience that the lens of the
eye performs a Fourier transform: on one side of our lens is a space-time
image as we experience it; on the other side of the lens is a holographic-



like spectral distribution. If indeed there is a space-time retinal image, as I
had seen in the ox eye, the input to the eye must be spectral. This
explanation fit nicely with David Bohm’s physics: as I mentioned earlier,
Bohm had noted that if we did not have lenses, the universe would appear
to us as a hologram.

However, Gibson’s intuition regarding a retinal image, and my
uncertainty about it, remained unresolved. As my hosts and I were
crossing a very busy street after the lecture, discussing the issue, the
answer suddenly became clear and we stopped short in the middle of the
street: The retina is not a film. The retina is a multilayered structure that is
sensitive to single quanta, photons of visible radiation. Quantum
properties are multidimensional. Gibson may have been wrong in
emphasizing the idea that there is no image at all, but he was correct in his
intuition regarding visual processing as multidimensional from the start.

The optical image is, as Gibson pointed out, actually an optical flow.
In addition, the image is somewhat “blurred,” diffracted due to the
gelatinous properties of the lens of the eye. (Technically, this blurring is
called an “Airy disk.”) Thus the optical image that appears at the surface
of the retina is somewhat spectral in configuration. The retinal quantum
process completes the transformation into the spectral domain.

Separating the optical image (or flow, the moving image) from the
retinal quantum process is in tune with Gibson’s intuition, which can now
be developed in its entirety as follows:

a. Radiant energy (such as light and heat) becomes scattered, spectral
and holographic, as it is reflected from and refracted by objects in the
universe.

b. The input to our eyes is (in)formed by this spectral, holographic
radiant energy.

c. The optics of our eyes (pupil and lens) then perform a Fourier
transformation to produce a space-time flow, a moving optical image
much as we experience it.

d. At the retina, the beginning of another transformation occurs: the
space-time optical image, having been somewhat diffracted by the
gelatinous composition of the lens, becomes re-transformed into a
quantum-like multidimensional process that is transmitted to the
brain.



Gabor’s Quantum of Information
The processes of the retinal transformation, and those that follow, are

not accurately described by the Fourier equation. I discussed this issue
with Dennis Gabor over a lovely dinner in Paris one evening, and he was
skeptical of thinking of the brain process solely in Fourier terms: “It is
something like the Fourier but not quite.” At the time, Gabor was excited
about showing how the Fourier transform could be approached in stages. (I
would use his insights later in describing the stages of visual processing
from retina to cortex.) But Gabor could not give a precise answer to why it
was not quite how the brain process worked.

Gabor actually did have the answer—but he must have forgotten it or
felt that it did not apply to brain processing. I found, after several years’
search, that the answer “to not quite a Fourier” had been given by Gabor
himself when I discovered that J. C. R. Licklider, then at the
Massachusetts Institute of Technology, had referred in the 1951 Stevens’
Handbook of Experimental Psychology to a paper by Gabor regarding
sensory processing.

Missing from our Paris dinner conversation was Gabor’s development
of his “elementary function” which dated to an earlier interest. He had
published his paper in the IEE, a British journal. I searched for the paper
in vain for several years in the American IEEE until someone alerted me
to the existence of the British journal. The paper was a concise few pages
which took me several months to digest. But the effort was most
worthwhile:

Before he had invented the Fourier holographic procedure, Gabor had
worked on the problem of telephone communication across the
transatlantic cable. He wanted to establish the maximum amount a
message could be compressed without losing its intelligibility. Whereas
telegraphy depends on a simple on-off signal such as the Morse code, in a
telephone message that uses language, intelligibility of the signal depends
on transmission of the spectra that compose speech. The communicated
signals are initiated by vibrations produced by speaking into the small
microphones in the phone.

Gabor’s mathematical formulation consisted of what we now call a
“windowed” Fourier transform, that is, a “snapshot” (mathematically
defined as a Hilbert space) of the spectrum created by the Fourier
transform which otherwise would extend the spectral analysis to infinity.



Neither the transatlantic cable nor the brain’s communication pattern
extends to infinity. The Gabor function provides the means to place these
communication patterns within coordinates where the spectrum is
represented on one axis and space-time on the other. The Fourier transform
lets us look at either space-time or spectrum; the Gabor function provides
us with the ability to analyze a communication within a context of both
space-time and spectrum.

From Retina to Brain
Licklider, in his 1951 Handbook of Experimental Psychology article,

suggests that:

If we could find a convenient way of showing not merely the
amplitudes of the envelopes but the actual oscillations of the
array of resonators, we would have a notation of even greater
generality and flexibility, one that would reduce under certain
idealized assumptions to the spectrum and under others to the
wave form. The analogy to the position-momentum and energy-
time problems that led Heisenberg in 1927 to state his
uncertainty principle . . . has led Gabor (1946) to suggest that
we may find the solution [to the problem of sensory processing]
in quantum mechanics.

30. Cross section of the eye showing biconcave lens

31. Biconvex lens processing wave fronts



32. Airy Disc

Note that Licklider is focusing on the oscillations of resonators (the
hair cells of the inner ear) and that he clearly distinguishes spectrum from
space-time wave forms as is performed by the Fourier procedure. The
analogy to quantum processes is prescient.

Licklider stated that the problems regarding hearing that he had
discussed might find a resolution in Gabor’s treatment of communication.
That resolution is given, by the Gabor elementary function, a “windowed,”
constrained, Fourier transformation. For me, the Gabor function provided
the missing description to understanding the transformation that is taking
place not only in the auditory but also in the visual process between the
retina and the brain’s cortex.

As has often been noted, revolutions in science require many more
steps than are apparent when we are only aware of the final result: We
move forward, only to drop back a few paces when we forget what we have
discovered. This is true for individuals as well as for the community. I
have on several occasions found that I have backed into old ways of
thinking that I thought I had abandoned. My rediscovery of Gabor’s
elementary function, which he himself had momentarily forgotten while
we were in Paris, proved to be a great leap forward in how to think about
sensory processing in the brain.

The example Gabor used was taken from music: the pitch of a single
tone is dependent on the frequency of vibration; the duration of the tone is
measured in time. The complete mathematical description of the tone
must therefore include both its frequency (and in case of harmonics, the
spectrum) and its duration. Gabor’s transformation provides simultaneous
space-time and frequency coordinates, whereas, when using Fourier’s
transformation, we must choose between space-time or a spectrum of
frequencies. The non-spectral coordinate in the Gabor function includes



both space and time because a message not only takes time to be
transmitted but also occupies space on the cable.

In brain science, we also use spatial as well as temporal frequency
(spectral) coordinates to describe our findings. In addition, when we deal
with a spectrum of frequencies per se, we often substitute the term
“spectral density” to avoid a common confusion that Gabor pointed out in
his paper: non-mathematicians, even physicists, often think of frequencies
as occurring only in time, but when mathematicians deal with vibrations
as in the Fourier transform, frequencies are described as composing
spectra and devoid of space and time.

In Gabor’s pioneering 1946 paper, he found the mathematics for his
“windowed” Fourier transform in Werner Heisenberg’s 1925 descriptions
of quantum processes in subatomic physics. In reference to Heisenberg’s
use of mathematics to describe subatomic quantum processes, Gabor
called his elementary unit a “quantum of information.” When, therefore,
in the early 1970s, my laboratory as well as brain scientists the world over
began to find that the receptive fields of cortical cells in the visual and
tactile systems could be described as two-dimensional Gabor functions,
we celebrated the insight that windowed Fourier transforms describe not
only the auditory but other sensory processes as well.



33. OHM and Helmholtz’s Piano Model of Sensory Processing

The form of communication, a mental process, and the form of the
construction of cortical receptive fields, a material physical process, could
now be described by the same formalism. At this level of inquiry, an
identity is established between the operations of mind and of brain. This
identity has led ordinary people, as well as scientists and philosophers, to
talk as if their brain “thinks,” “chooses,” “hurts,” or is “disorganized.” In
the language of philosophy, this is termed misplaced concreteness: It is we
the people who do the thinking, choosing, hurting, and who become
discombobulated. So it is important to specify the level, the scale of
inquiry, at which there is an identity of form (at the receptive field level in
the brain) and where mind (communication) and body (brain) are
different. More on this in Chapters 22–24.

Implications



Gabor warned against thinking that the “quantum of information”
means that communication is actually going on at the quantum scale. I
have also cautioned against assuming that the receptive fields we map in
the sensory cortex are quantum-scale fields. But when communication is
wireless (or uses optical fibers) the medium of communication is
operating at the quantum scale. The same may be true for the brain. I
noted earlier that the retina is sensitive to individual quanta of radiation.
Therefore, for the past decade or more my colleague, Menas Kafatos and I
have explored the variety of scales at which brain processes that mediate
the organization of receptive fields could actually be quantum-scale and
not just quantum-like.

Thus, Mari Jibu and Kunio Yasui, an anesthesiologist and a physicist
from Notre Dame Seishen University in Okayama, Japan, and I have
suggested that the membranes of the fine fibers, the media in which the
receptive fields are mapped, may have quantum-scale properties. The
membranes are made up of phospholipids, oscillating molecules that are
arranged in parallel, like rows of gently waving poplar trees. The lipid,
fatty water- repellent part makes up the middle, the trunks of the ”trees,”
and the phosphorus water-seeking parts make up the inside and outside of
the membrane. These characteristics suggest that water molecules become
trapped and aligned in the phosphorus part of the membrane. Aligned
water molecules can show super-liquidity and are thus super-conductive;
that is, they act as if they had quantum-scale properties.

34. Logons, Gabor Elementary Functions: Quanta of Information



35. Top row: illustrations of cat simple 2-D receptive field profiles. Middle row: best-fitting 2-D
Gabor wavelet for each neuron. Bottom row: residual error of the fit, which for 97% of the cells

studied was indistinguishable from random error in the Chi-squared sense. from: Daugman, J.G.
(1990.)

Another interesting consequence of Gabor’s use of the “quantum of
information” is the very fact that the form of quantum theory becomes
extended to communication. My colleagues in quantum physics at George
Mason University in Northern Virginia claim that this means that the laws
of quantum physics actually cross all scales from the subatomic to cosmic.
My reaction has been: of course, since much of what we know comes to us
via our senses and thus through the “lenses” that process spectra such as
those named “light” and “heat.” But it also means that if quantum laws
operate at every scale, then despite their weirdness, these laws should have
an explanation at an everyday scale of inquiry. My suggestion is that
weirdness originates in the coordinates within which quantum laws are
formulated: the creation of a computational region that has spectrum for
one axis and space-time for the other. Conceptually, explanations seesaw
from spectra (and more often their origins in waveforms) to space-time
and particles. We can sort out the weirdness by coming back to the Fourier
transform that clearly distinguishes space-time explanations from
holographic-spectral explanations.

Holographic Brainwaves?



At a recent conference I was accosted in a hallway by an attendee
who stated that he had great trouble figuring out what the brain waves
were that constituted the dendritic processing web. I was taken aback. I
had not thought about the problem for decades. I replied without
hesitation, “Brain waves! There aren’t any. All we have is oscillations
between excitatory and inhibitory post- (and pre-) synaptic potential
changes.” I went on to explain about Gabor “wavelets” as descriptive of
dendritic fields and so forth.

It’s just like water. When the energy of a volley of nerve impulses
reaches the “shore” of the retina the pre-synaptic activation forms a
“wave-front.” But when activity at the brain cortex is mapped we obtain
wavelets, Gabor’s “quanta of information.” The relationship between the
interference patterns of spectra and the frequencies of patterns of waves is
straightforward, although I have had problems in explaining the
relationship to students in the computational sciences. Waves occur in
space-time. Spectra do not. Spectra result from the interference among
waves—and, by Fourier’s insightful technique, the locus of interference
can be specified as a number that indicates the “height” of the wave at that
point. The number is represented by a Gabor function (or other wavelet.)
Frequency in space-time has been converted to spectral density in the
transform domain.

Wavelets are not instantaneous numbers. As their name implies
wavelets have an onset slope and an offset slope. Think of a musical tone:
you know a little something about where it comes from and a little as to
where it is leading.

Note that we are dealing with dendritic patches limited to one or a
small congregation of receptive fields—like the bobbing of a raft in the
water beyond the shore. There was a considerable effort made to show that
there is no overall Fourier transform that describes the brain’s relationship
to a percept—but those of us who initially proposed the holographic brain
theory always described the transformation as limited to a single or a
small patch of receptive fields. By definition, therefore, the Fourier
transformation is constrained, windowed, making the transformed brain
event a wavelet.

We now need to apply these formulations to specific topics that have
plagued the study of perception— needlessly, I believe. The concept that



the optics of the eye create a two-dimensional “retinal image” is such a
topic.

Gabor or Fourier?
The form of receptive fields is not static. As described in the next

chapter, receptive field properties are subject to top-down (cognitive)
influences from higher-order brain systems. Electrical stimulation of the
inferior temporal cortex changes the form of the receptive fields to
enhance Gabor information processing— as it occurs in communication
processing systems. By contrast, electrical stimulation of the prefrontal
cortex changes the form of the receptive fields to enhance Fourier image
processing—as it is used in image processing such as PET Scans and
fMRI and in making correlations such as in using FFT. The changes are
brought about by altering the width of the inhibitory surround of the
receptive fields. Thus both Gabor and Fourier processes are achieved,
depending on whether communication and computation or imaging and
correlations are being addressed.

And Back Again: Projection
We experience the world in which we navigate as being “out there,”

not within our sense organs or in our brain. This phenomenon is called
“projection.” I experienced an interesting instance of projection that
anyone can repeat in his own bathroom. I noted that the enlarging mirror
(concave 7X) in my bathroom reflected the ceiling light, not at the
mirror’s surface as one would expect, but instead, the image of the light
was hovering in mid-air above the mirror. The image seemed so real that I
tried to grasp it, to get my hand behind it, only to catch thin air.

Our experience of waveforms, as they create images reflected by
mirrors, refracted by prisms or gratings, often seem equally “hard to
grasp.” But the power that generates such waveforms can be experienced
in a very real way, as in a tsunami sweeping ashore. My research has been
motivated by the excitement of “getting behind” the images we experience
—to try to understand how the experience is created. Cosmologists spend
their days and nights trying to “get behind” the images they observe with
their telescopes and other sensing devices. Brain scientists are, for the
most part, not yet aware of the problem.



In order to understand the process by which we perceive images, we
need to regard vision as akin to other senses, such as hearing. Harvard
physiologist and Nobel laureate Georg von Békésy began his classical
experiments, summarized in his 1967 book Sensory Inhibition, at the
Hungarian Institute for Research in Telegraphy in 1927. His initial
experiments were made on the basilar membrane of the cochlea, the
receptor surface of the ear. Békésy began by examining how the arrival of
pressure waves becomes translated into a neural code that reaches our
brain. “The results suggested similarities to Mach’s law of [visual]
brightness contrast. At the time my attempt to introduce psychological
processes into an area that was held to be purely mechanical met with
great opposition on the part of my associates in the Department of Physics.

. . . And yet I continued to be concerned with the differences between
perceptual observations and physical measurements. My respect for
psychological observation was further enhanced when I learned that in
some situations, as in the detection of weak stimuli, the sense organs often
exhibit greater sensitivity than can be demonstrated by any purely physical
procedure. (Ernst Mach was a venerated Viennese scientist/philosopher
active at the turn of the 20th century.)

When I met Békésy at Harvard, he was frustrated by the small size of
the cochlea and was searching for as large a one as he could get hold of.
He knew that I was working (in Florida) with porpoises at the time, and he
asked me to bring him a porpoise cochlea if I came across one of the
animals that had died. I did, but Békésy wanted something still larger.

Békésy had noted at the time—just as we would later come to view
the eye as similar to the ear—that the skin was also like the ear in many
respects. (The mammalian ear is derived from the lateral line system of
fish, a system that is sensitive to vibrations transmitted through water.)
Therefore, Békésy set out to make an artificial ear by aligning five
vibrators that he could tune to different frequencies. He placed this
artificial cochlea on the inside surface of the forearm, and he adjusted the
vibrations so that the phase relations among the vibrations were similar to
those in a real cochlea. What one felt, instead of the multiple frequencies
of the five vibrators, was a single point of stimulation. Békésy showed that
this effect was due to “sensory inhibition;” that is, the influence of the
vibrators on the skin produced “nearest neighbor” effects, mediated by the
network of nerves in the skin, that resulted in a single sensation. Changing



the phase relations among the vibrators changed the location on the
forearm where one felt the point.

Subsequently, in my laboratory at Stanford, a graduate student and I
set to examining whether the brain cortex responded to such individual
vibratory stimuli or to a single point as in the sensation produced in
Békésy’s experiment. We did this by recording from the brain cortex of a
cat while the animal’s skin was being stimulated by several vibrators.
Békésy, who had moved to Hawaii from Harvard (which belatedly offered
him a full professorship after he received his Nobel Prize) served on my
student’s thesis committee; he was delighted with the result of the
experiment: The cortical recording had reflected the cat’s sensory and
cortical processing—presumably responsible for the cat’s perceptions—
not the physical stimulus that had been applied to the skin on the cat’s lips.

On another occasion, still at Harvard, Békésy made two artificial
cochleas and placed them on his forearms. I was visiting one afternoon,
and Békésy, full of excitement, captured me, strapping the devices on my
arms with great enthusiasm. He turned on his vibrators and adjusted the
phase relations among the vibrations within each of the “cochleas” so that
I experienced a point sensation on each arm. Soon, the point sensations
began to alternate from arm to arm: when I felt a point in one arm, I did
not feel it in the other. Interesting.

Békésy had concluded from this experiment that sensory inhibition
must occur not only in our skin but somewhere in the pathways from our
skin to our brain, a conclusion confirmed by the results of our cat
experiments.

But this was not all. Békésy asked me to sit down and wait a while as
the point sensations kept alternating from one arm to the other. I began
reading some of his reprints. Then, suddenly, after about ten minutes, I had
the weirdest feeling: The sensation of a point had suddenly migrated from
my arms and was now at a location in between them, somewhat in front of
me. I was fascinated.

I asked, how could this be? How can I feel, have a tactile sensation,
outside of my body? Békésy replied with a question: “When you do
surgery, where do you feel the tissue you are touching with your forceps
and scalpel?” “Of course,” I answered, “out there where the tissue is—but
the forceps and scalpel provide a solid material connection.” We all have



this experience every time we write with pen or pencil, feeling the surface
of the paper through the mediation of the implement.

But we also hear sounds without any obvious solid material
connection with the source that produces them: for instance, the
loudspeakers in a stereo system. I once heard the sound so realistically
coming out of a fireplace flanked by two stereo-speakers that I actually
looked up the chimney to see if there was another speaker hidden up there.

There are important advantages to our ability to project our
perceptions away from our bodies in this fashion. In another experiment in
which he set out to test the value of our stereo capability, Békésy plugged
his ears and used only one loudspeaker, strapped to his chest, to pick up
sounds in his environment. He tried to cross a street and found it almost
impossible because he could not experience an approaching car until it
was almost literally upon him. The impact of the vibrations on his chest
from the approaching sound of a car hit him so suddenly that he clutched
his chest and was nearly bowled over.

We may experience the world we navigate as being “out there,” but
the apparatus that makes our experience possible is within our skin. My
eyes and brain enable me to see—but the location of what I see is
projected out, away from and beyond those bodily structures that make my
seeing possible. Békésy’s experiments demonstrated the laws of projection
create an anticipatory frame, a protective shell that surrounds us and
allows us to choose an action before an oncoming event overwhelms us.

36. Localization of Perception Between Fingers



While reading Békésy’s notes on his experiments, I saw that he had
scribbled some equations next to the text.

I asked him about them, and he answered that they were some new-
fangled math that he did not fully understand as yet. I felt that, even if he
understood the equations, I certainly would not. To my surprise, that
understanding was to come later: they were the equations describing
Gabor’s application to sensory processing of the mathematics of quantum
physics.

My encounters with Békésy were among the most rewarding in my
career: they greatly enhanced my appreciation of the issues to be faced in
deciphering how we navigate our world.

The experimental results presented here showed that the perceptual
process is far from a simple input of “what’s out there” to the brain.
Rather, by means of the transformations produced by receptors, structures
such as objects are formed from a holographic-like background within
which every-“thing” is distributed “everywhere and everywhen.” The
receptor then transforms these “objects”—in the case of vision by virtue
of the quantum nature of retinal processing—into a constrained
holographic-like process, a set of wavelets that Dennis Gabor called
“quanta of information.” Békésy’s experiments, as well as my own,
demonstrated that, in turn, these brain processes influence receptors as
much as does the input from the-world-out-there that we navigate. More
on this in the next chapter.

In Summary
In this chapter I summarize the hard evidence for why we should pay

attention to the transformations that our receptors and sensory channels
make—rather than thinking of these channels as simple throughputs. This
evidence provides a story far richer than the view of the formation of
perception held by so many scientists and philosophers.

Furthermore, when we separate the transformations produced by our
lenses and lens-like processes from those produced by our retinal
processing, we can demonstrate that the brain processing of visual
perception is actually multidimensional. For other senses I presented the
evidence that “sensory inhibition” serves a similar function. Thus there is



no need for us to deal with the commonly held view that brain processes
need to supplement a two-dimensional sensory input.

Finally, I place an emphasis on the data that show that perception has
the attribute of “projection” away from the immediate locus of the
stimulation. We often perceive objects and figures as “out there,” not at
the surface of the receptor stimulation.

A corollary of projection is introjection. I thus now venture to suggest
that introjection of perceptions makes up a good deal of what we call our
”conscious experience.”



Chapter 6
Of Objects and Images

Wherein I distinguish between objects and images and the brain systems
that process the distinction.

Thus, by our movements we find it is the stationary form of the
table in space, which is the cause of the changing image in our
eyes. We explain the table as having existence independent of
our observation because at any moment we like, simply by
assuming the proper position with respect to it, we can observe
it.

—Hermann von Helmholtz, Optics, 1909/1924

As Lie wrote to Poincaré in a letter in 1882: ‘Riemann and v.
Helmholtz proceeded a step further [than Euclid] and assumed
that space is a manifold or collection of numbers. This
standpoint is very interesting; however, it is not to be considered
definitive.’ Thus, Lie developed the means to analyze rigorously
how points in space are transformed into one another through
infinitesimal transformations—that is, continuous groups of
transformations.

—Arthur I. Miller, Imagery in Scientific Thought, 1984



Movements
The end of the previous chapters left feature creatures with stick

figures and frequency freaks with a potential of wavelets. Stick figures are
two-dimensional, but the world we navigate is four-dimensional. Nor is
that world composed of wavelets; it is filled with objects. Nor is that
world composed of wavelets filled with objects. Early on I had the
intuition that somehow movement might be the key to unveiling the
process by which we are able to perceive objects in a four-dimensional
world. My intuition was based on the observation that adjacent to—and
sometimes overlapping with—those areas of the brain that receive a
sensory input to the brain, there are areas that, when electrically
stimulated, produce movements such as turning the head and eyes, perking
the ears, and moving the head and limbs. Such movements are mediated
by our brain’s circuitry, its surface structure.

But how would our perception of objects be achieved through
movement? While teaching at Georgetown, two of my undergraduate
students were to provide the route by which I came to a first step toward
answering this question. One student kept insisting that he wanted to know
“the how, the particular go of it.” Another student in a different class had a
condition called “congenital nystagmus.” Nystagmus is the normal
oscillation of our eyeballs. In people such as my student, the oscillation is
of much greater amplitude and is therefore quite noticeable. I had had
several patients with this harmless “symptom,” and I told my student that
she had nothing to worry about. This turned out to be a bigger worry to
her: she no longer had an ”ailment” that made her special. During class,
while I was lecturing on another topic, from time to time I also looked at
this student’s big brown oscillating eyes. I began to have the feeling that
those eyes were trying to tell me something. Shortly, while still lecturing,
the first part of the answer came: I recalled that the ability to perceive at
all depends on oscillatory movements.

Experiments have shown us that when the oscillation of the eyes is
nullified, vision disappears within 30 seconds. In the experiments, a small
mirror was pasted on the test subject’s sclera, the white part of the eyeball.
This is not as uncomfortable as it might seem. The sclera is totally
insensitive to touch or pain; it is when the cornea, the colored ring around
the pupil of our eye, is injured, that it hurts so much. A figure is then
projected onto the mirror and reflected from that mirror onto a surface



displayed in front of the subject. Thus, (when corrected for the double
length of the light path to and from the eye) the figure displayed on the
surface in front of the subject mirrors the excursion of the eyeball. Shortly,
the subject experiences a fading of the figure, which then disappears
within about 30 seconds. Hence, as this experiment clearly demonstrates:
no oscillatory movement, no image.

These same results have been repeatedly obtained in experiments
involving other senses. Fine oscillations of our receptors or of the
stimulating environment around us are needed in order for us to see, to
feel by touch, and to hear. These oscillations—vibrations measured in
terms of their frequencies—allow us to experience images in the space-
time that surrounds us.

The nystagmoid oscillations of the eyes act like a pendulum. The arc
that a pendulum describes moves back and forth across a point where the
pendulum comes to rest when it stops swinging. Current mathematical
descriptions call such a point an “attractor.” (Mathematically, the ideal
point attractor is defined as an essential character of a system of group-
theoretical oscillations that in time converge on it.) Thus the oscillations
of the eyes form a point attractor in the neuro-nodal deep structure of fine
fibers that can then serve as a pixel. Pixels are the units from which the
images on your television screen are formed.

Forming a point attractor, a pixel, from oscillations is a
transformation. The transformation converts the frequency of oscillations
in the spectral domain to points in space-time. In short, the transformation
is described by the (inverse) Fourier equation. It is the oscillatory
movements, because of their pendulum-like character, that transform a
distributed holographic-like brain process to one that enables imaging.
Converting from space-time to frequencies and back to space-time is the
procedure that is used to produce images by PET scans and fMRIs in
hospitals.

What’s the Difference?
Our experience of the world is initially with objects, not images. The

distinction between objects and images is fundamental to our
understanding of how we perceive the world around us. We can look at
objects from various perspectives to perceive various profiles, images, of



the objects, but the imaging is secondary to the perception of objects. To
perceive objects, either they or we make large-scale movements.
Helmholtz made this distinction well over a century ago: he stated that
objects are forms that remain unchanged despite movement; images are
those perceptions that change with movement.

To demonstrate the distinction between objects and images, I conduct
a simple demonstration. I have a volunteer, a student or a person attending
a conference, close her eyes and extend her hand palm up. I then tap her
palm with a key. I ask what she experiences. Ordinarily, the answer is “a
poking on my palm.” Next I put the key into her palm, ask her to close her
hand, moving her fingers. Again I ask what she is experiencing. Invariably,
the answer is “a key.” Poking is imaged as happening “to me”; when the
volunteer is moving her hand and fingers, she experiences an object “out
there.”

In the auditory mode, the distinction between image and object is the
basis of the difference between music and speech. We hear music; we
speak speech. Music is centered on the ear; language is centered on the
tongue (lingua in Latin). For instance, tones are images, profiles, which
change as changes take place in the context within which they occur. Such
tones, including vowel sounds, can be described as Gabor functions that
embody what has come before and anticipate what is yet to come. By
contrast, our perception of consonants is that they are like objects in that
they remain stable over a wide range of changing contexts. Simulations
that have been designed with the intent to produce gradual transitions
between consonants such as “b” and “p” or “d” and “t” show that we do
not perceive such gradations in sound. Technically, this phenomenon is
known as “categorical perception.”

As higher-order constructions are composed, both music and
language contain images and objects. Still, the distinction between being
touched by music and actively forming speech is upheld because, at least
in untrained subjects, musical images, tunes, tend to be processed
primarily with the right hemisphere of the brain while ordinary language
is more likely to be processed by the left hemisphere.

Another example of the distinction between our perception of images
and of objects is provided by the sun’s radiant energy that warms and feeds
us. When that radiant energy affects us, we “name” it light or heat, but it is
not such when it occurs in outer space. It takes a considerable amount of



active instrumentation by telescopes and interferometers to demonstrate
that radiation exists as such. Physicists often use terms like “heat” or
“light” to describe radiations of different wavelengths—a shortcut that can
lead to confusion, not only for nonscientists, but, as well, even for the
physicists who have used these labels. By contrast, in psychophysics we
use the term “luminance” to describe radiation that is physically measured
and therefore objective. We use the term “brightness” to describe and
measure what we perceive. Radiant energy must fall on a receptor to be
perceived as light or heat just as the oscillations of water that mediate its
energy must strike the shore to become the breakers with the strength to
topple us. In both cases— water and radiation—the patterns that make up
the perceived objects—breakers and light/heat—are considerably different
from the oscillating energies offshore or out in space.

An Act of Faith
We are able to perceive objects as the result of relative movement of

ourselves within the world we navigate. Thus, objects can be regarded as
constructions by our brain systems that are computing the results of those
movements.

When I use the term “construction,” I do not mean that what we
navigate is not a real world. When I hit my shin on the bed rail, I don’t
doubt, no matter how sleepy I may be, that the navigable world of bed rails
is real. This lack of doubt forms a belief that is based on our early
experience.

I first understood how a belief is involved in our perception of an
objective world while at a conference held at Esalen, the famous West
Coast spa and retreat. The situation was somewhat bizarre, as the theme of
the conference was brain anatomy. During the conference, we were
introduced to some of the New Age exercises. One of these involved all of
us stodgy scientists being blindfolded and going around the campus
feeling bushes, flowers, and one another’s faces. It was a totally different
experience from our usual visual explorations and resulted in my
becoming aware of the distinctiveness of perceptions derived from
different sensory experiences.

It immediately occurred to me that as babies, we experience our
mothers and others around us in different ways with each of our various



sensory systems. Each of these experiences is totally different. As we
quickly learned at Esalen, our visual experience of a person is very
different from our auditory experience of that person, and different still
from our experience of touch or of taste! Somewhere along the line, as we
process these various inputs, we make a leap of faith: all these experiences
are still experiences of the same “someone.” In the scientific language of
psychology, this is called the process of “consensual validation,”
validation among the senses.

As we grow older, we extend this faith in the unity of our experiences
beyond what our various sensory explorations tell us, and we include what
others tell us. We believe that our experiences, when supported by those of
others, point to a “reality.” But this is a faith that is based on disparate
experiences.

The English philosopher Bishop George Berkeley became famous for
raising questions regarding this faith. He suggested that perhaps God is
fooling us by making all of our experience seem to cohere, that we are
incapable of “knowing” reality. Occasionally other philosophers have
shared Berkeley’s view, which is referred to in philosophy as “solipsism.”
I suggest that, distinct from the rest of us, such philosophers simply didn’t
make that consensual act of faith when they were babies.

An extreme form of consensuality is a psychological phenomenon
called “synesthesia.” Most of us share such experiences in milder form:
red is a hot color, blue is cool. Some thrilling auditory and/or visual
experiences actually make us shiver. Those who experience synesthesia
have many more such sensations—salty is cool; peppery is hot— to a
much greater degree.

The usual interpretation is that connections between separate systems
are occurring in their brains, “binding” the two sensations into one—
connections that the rest of us don’t make. Instead, it is as likely that
synesthesia is due to a higher-order abstraction, similar to the process
previously described where I asked those students to raise their hands if
they wear glasses and also blue jeans. Eyeglasses and blue jeans are
conjoined in their response. In my laboratory at Stanford, we found that
many receptive fields in the primary visual cortex respond not only to
visual stimuli but also to limited bandwidths of auditory stimulation. Still
other conjunctions within a monkey’s receptive field encoded whether the
monkey had received a reward or had made an error. Other investigators



have found similar cross-modal conjunctions, especially in motor cortexes
of the brain. A top-down process that emphasizes such cross-modal
properties could account for synesthesia such as the experience of bright
sounds without recourse to connecting, “binding” together such properties
as hearing and seeing from two separate brain locations.

Our faith arising from consensual agreement works well with regard
to our physical environment. Once, when I was in Santiago, Chile, my
colleague, the University of California philosopher John Searle rebutted
the challenge of a solipsist with the statement that he was happy that the
pilot of the plane that brought him through the Andes was not of that
persuasion. The pilot might have tried to take a shortcut and fly right into
a mountain since it “really” didn’t exist.

When it comes to our social environment, consensuality breaks down
more often than not. Ask any therapist treating both wife and husband: one
would never believe these two inhabited the same world. The film
Rashomon portrays non-consensuality beautifully. In later chapters, I will
examine some possibilities as to how this difference between social and
physical perceptions comes about.

I am left with an act of faith that states that there is a real world and
that our brain is fitted to deal with it to some extent. We would not be able
to navigate that real world if this were not so. But at the same time, well
fitted as our brains are, our experience is still a construction of that reality
and can therefore become misfitted by the operation of that brain. It is the
job of brain/behavioral science to lay bare when and how our experience
becomes fitting.

In the section on communication we will look at what has been
accomplished so far. From my standpoint of a half-century of research,
both brain and behavioral scientists have indeed accomplished a great
deal. This accomplishment has depended on understanding how the brain
makes it possible for us to more or less skillfully navigate the world we
live in. This in turn requires us to understand brain functions in relation to
other branches of science. Thus, a multiple front of investigators has
developed under the heading of “cognitive neuroscience.” This heading
that does not do justice to the breadth of interest, expertise and research
that is being pursued. For perception, the topic of the current section of
this book, the link had to be made primarily with physics. In this arena,
theory based on mathematics has been most useful.



An Encounter
Some years ago, during an extended period of discussion and

lecturing at the University of Alberta, I was rather suddenly catapulted
into a debate with the eminent Oxford philosopher Gilbert Ryle. I had just
returned to Alberta from a week in my laboratory at Stanford to be greeted
by my Canadian hosts with: “You are just in time; Gilbert Ryle is
scheduled to give a talk tomorrow afternoon. Would you join him in
discussing ‘The Prospects for Psychology as an Experimental Science’?” I
was terrified by this “honor,” but I couldn’t pass up the opportunity. I had
read Ryle’s publications and knew his memorable phrase describing
“mind” as “the ghost in the machine.” I thought, therefore, that during our
talk we would be safely enveloped in a behaviorist cocoon within which
Ryle and I could discuss how to address issues regarding verbal reports of
introspection. But I was in for a surprise.

Ryle claimed that there could not be any such thing as an
experimental science of psychology—that, in psychology, we had to
restrict ourselves to narrative descriptions of our observations and
feelings. He used the example of crossing a busy thoroughfare filled with
automobiles, while holding a bicycle. He detailed every step: how stepping
off a curb had felt in his legs, how moving the bicycle across cobblestones
had felt in his hands, how awareness of on-coming traffic had constrained
his course, and so forth. His point was that a naturalist approach, using
introspection, was essential if we were to learn about our psychological
processes. I agreed, but added that we needed to supplement the naturalist
procedure with experimental analyses in order to understand “how” our
introspections were formed. Though I countered with solid results of many
experiments I had done on visual perception, I was no match for an Oxford
don. Oxonians are famed for their skill in debating. Ryle felt that my
experimental results were too far removed from experience. I had to agree
that in experimentation we lose some of the richness of tales of immediate
events.

About ten minutes before our time was up, in desperation, I decided
to do just what Ryle was doing: I described my experimental procedure
rather than its results. I compared the laboratory and its apparatus to the
roadway’s curbs and pavements; I enumerated the experimental hazards
that we had to overcome and compared them with the cars on the road; and
I compared the electrodes we were using to Ryle’s bicycle. Ryle graciously



agreed: he stated that he had never thought of experiments in this fashion.
I noted that neither had I, and thanked him for having illuminated the
issue, adding that I had learned a great deal about the antecedents —the
context within which experiments were performed. The audience rose and
cheered.

This encounter with Gilbert Ryle would prove to be an important step
in my becoming aware of the phenomenological roots of scientific
observation and experiment. Ryle’s “ghost” took on a new meaning for
me; it is not to be excised, as in the extremes of the behaviorist tradition;
it is to be dealt with. I would henceforth claim that: We go forth to
encounter the ghost in the machine and find that “the ghost is us.”

Object Constancy
As noted at the beginning of this chapter, immediately adjacent to—

or in some systems even overlapping— the primary sensory receiving
areas of the brain, are brain areas that produce movement of the sensing
elements when electrically excited. Adjacent to and to some extent
overlapping this movement-producing cortex are areas that are involved in
making possible consistent perceptions despite movement. My colleagues
and I, as well as other investigators, have shown these areas to be involved
in perceiving the size and color as well as the form of objects as consistent
(called “size, color and object constancy” in experimental psychology.)

In our experiments we trained monkeys to respond to the smaller of
two objects irrespective of how far they were placed along an alley. The
sides of the alley were painted with horizontal stripes to accentuate its
perceived distance. The monkeys had to pull in the smaller of the objects
in order to be able to open a box containing a peanut. After the monkeys
achieved 90% plus, I removed the cortex surrounding the primary visual
receiving area (which included the visuomotor area). The performance of
the monkeys who had the cortical removals fell to chance and never
improved over the next 1000 trials. They always “saw” the distant object
as smaller. They could not correct for size constancy on the basis of the
distance cues present in the alley.

The Form(ing) of Objects



Our large-scale movements allow us to experience objects.
Groundbreaking research has been done by Gunnar Johanssen in Uppsala,
Sweden, and James Gibson at Cornell University in Ithaca, New York.
Their research has shown how objects can be formulated by moving
points, such as the pixels that we earlier noted were produced by
nystagmoid oscillations of the eyes. When such points move randomly
with respect to each other, we experience just that— points moving at
random. However, if we now group some of those points and consistently
move the group of points as a unit within the background of moving points
— Voilà! We experience an object. A film made by Johanssen and his
students in the 1970s demonstrates this beautifully. The resulting
experience is multidimensional and sufficiently discernible that, with a
group of only a dozen points describing each, the viewer can tell boy from
girl in a complex folk dance.

Though some of the film is made from photographs of real objects,
computer programs generated most of it. I asked Johanssen whether he
used the Fourier transform to create these pictures. He did not know but
said we could ask his programmer. As we wandered down the hall, he
introduced me to one Jansen and two other Johanssens before we found the
Johanssen who had created the films. I asked my question. The
programmer was pleased: “Of course, that’s how we created the film!”

But the Johanssen film shows something else. When a group of
points, as on the rim of a wheel, is shown to move together around a point,
the axel, our perception of the moving points on the rim changes from a
wave form to an object: the “wheel.” In my laboratory we found, rather
surprisingly, that we could replace the “axel” by a “frame” to achieve the
same effect: a “wheel.” In experimental psychology this frame effect has
been studied extensively and found to be reciprocally induced by the
pattern that is being framed.

This experimental result shows that movement per se does not give
form to objects. Rather, the movements must cohere, must become related
to one another in a very specific manner. Rudolfo R. Llinás provided me
with the mathematical expressions of this coherence in personal
discussions and in his 2001 book, I of the Vortex: From Neurons to Self.
My application of his expressions to our findings is that the Gabor
functions that describe sensory input cohere by way of covariation; that
the initial coherence produced by movements among groups of points is



produced by way of contravariation (an elliptical form of covariation); and
that the final step in producing object constancy is the “symmetry group”
invariance produced by a merging of contravariation with covariation.

Symmetry Groups
Groups can be described mathematically in terms of symmetry. A

Norwegian mathematician, Sophus Lie, invented the group theory that
pertains to perception. The story of how this invention fared is another
instance of slow and halting progress in gaining the acceptance of
scientific insights. As described in the quotation at the beginning of this
chapter, early in the 1880s Hermann von Helmholtz, famous for his
scientific work in the physiology of vision and hearing, wrote to Henri
Poincaré, one of the outstanding mathematicians of the day. The question
Helmholtz posed was ”How can I represent objects mathematically?”
Poincaré replied, “Objects are relations, and these relations can be mapped
as groups.” Helmholtz followed Poincaré’s advice and published a paper in
which he used group theory to describe his understanding of object
perception. Shortly after the publication of this paper, Lie wrote a letter to
Poincaré stating that the kind of discrete group that Helmholtz had used
would not work in describing the perception of objects—that he, Lie, had
invented continuous group theory for just this purpose. Since then, Lie
groups have become a staple in the fields of engineering, physics and
chemistry and are used in everything from describing energy levels to
explaining spectra. But only recently have a few of us returned to applying
Lie’s group theory to the purpose for which he created it: to study
perception.

My grasp of the power of Lie’s mathematics for object perception
was initiated at a UNESCO conference in Paris during the early 1970s.
William Hoffman delivered a talk on Lie groups, and I gave another on
microelectrode recordings in the visual system. We attended each other’s
sessions, and we found considerable overlap in our two presentations.
Hoffman and I got together for lunch and began a long and rewarding
interaction.

Symmetry groups have important properties that are shown by
objects. These groups are patterns that remain constant over two axes of
transformation. The first axis assures that the group, the object, does not



change as it moves across space-time. The second axis centers on a fixed
point or stable frame, and assures that changes in size—radial expansion
and contraction—do not distort the pattern.

What We Believe We Know
To summarize: We navigate a world populated by objects. Objects

maintain their form despite movements. What vary are images—that is,
profiles of objects that change with movement. But even our perceptions of
images depend on motion.

Minute oscillations of the relation between receptor and stimulus are
necessary for any perceptual experience to occur. These oscillations
describe paths around stable points—called “point attractors.” Point
attractors are the result of transformations from the oscillations that
describe the sensory input in the spectral domain, to space-time “pixels”
from which images are composed.

When points become grouped by moving together with respect to
other points, the group is perceived as a form—that is, an object. The
properties of objects, the invariance of their perceived form over changes
in their location and apparent size are described by a particular kind of
group called a “symmetry group.”

A more encompassing picture of how we navigate our world can be
obtained by considering a corollary of what I have described so far. Recall
that, in introducing his suggestion to Helmholtz that he use groups,
Poincaré noted that “objects are relations.” Thus, Poincaré’s groups
describe relations among points, among pixels within a frame, a context
that occurs in space and time.

Such contexts are bounded by horizons, the topic of the next chapter.



Chapter 7
The Spaces We Navigate and Their Horizons

Wherein I distinguish a variety of spaces we navigate and discern the
boundaries at several of their horizons.

When it was time for the little overloaded boat to start back to
the mainland, a wind came up and nudged it onto a large rock
that was just below the surface of the water. We began, quite
gently, to sink. . . . All together we went slowly deeper into the
water. The boatman managed to put us ashore on a huge pile of
rocks. . . .

The next day when I passed the island on a bus to Diyarbakir
there, tied to the rock as we had left it, was a very small, very
waterlogged toy boat in the distance, all alone.

—Mary Lee Settle, Turkish Reflections: A Biography of a Place, 1991



Navigating the Spaces We Perceive
Have you considered just how you navigate the spaces of your world?

Fortunately, in our everyday living, we don’t have to; our brain is fine-
tuned to do the job for us. Only when we encounter something exceptional
do we need to pay attention. When we are climbing stairs, we see the stairs
with our eyes. The space seen is called “occulocentric” because it centers
on our eyeballs, but it is our body that is actually climbing the stairs. This
body- centered space is called “egocentric.” The stairs inhabit their own
space: some steps may be higher than others; some are wider, some may
be curved. Stairs are objects, and each has its own “object-centered” space.
Space, as we navigate it, is perceived as three-dimensional. Since we are
moving within it, we must include time as a fourth dimension. We run up
the stairs, navigating these occulocentric, egocentric, and object-centered
spaces without hesitation. The brain must be simultaneously computing
within 12 separate co-ordinates, four dimensions for each time-space.
Cosmologists are puzzling the multidimensionality of “string” and
“brane” theory: why aren’t scientists heeding the 12 navigational
coordinates of brain theory?

Now imagine that the stairs are moving. You are in a hurry, so you
climb these escalator stairs. You turn the corner and start climbing the
next flight of stairs. Something is strange. The stairs seem to be moving
but they are not, they are stationary. You stumble, stop, grab the railing,
then you proceed cautiously. Your occulocentric (visual) cues still indicate
that you are moving, but your egocentric (body) cues definitely shout
“no.” The brain’s computations have been disturbed: occulocentric and
egocentric cues are no longer meshed into a smoothly operating
navigational space. It is this kind of separation—technically called
“dissociation”— that alerts us to the possibility that the three types of
space may be constructed by different systems of the brain.

Eye-Centered and Body-Centered Systems
While in practice in Jacksonville, Florida, I once saw a patient who

showed an especially interesting dissociation after he’d been in an
automobile accident. Every so often, he would experience a brief dizzy
spell, and when it was over, he experienced the world as being upside
down. This experience could last anywhere from a few minutes to several



hours, when it was ended by another dizzy spell, after which his world
turned right side up once more. This patient was an insurance case, and I
had been asked by the insurance company to establish whether he was
faking these episodes. Two factors convinced me that he was actually
experiencing what he described: his episodes were becoming less frequent
and of shorter duration; and his main complaint was that when the world
was upside down, women’s skirts stayed up around their legs, despite
gravity!

In the short time of this office visit I had failed to ask the patient
what proved to be a critical question: Where were your feet when the
world was upside down? Decades later I had the opportunity to find the
answer to this question.

The story begins more than a hundred years ago when George
Stratton, an early experimental psychologist, performed an experiment at
Stanford University. He had outfitted himself with spectacles that turned
the world upside down, just as the world had appeared to my patient.
However, after continuously wearing these glasses for a week during his
everyday activities, Stratton found that his world had turned right side up
again.

Poincaré had stated that objects are relations; therefore, the perceived
occulocentric space, which is made up of objects, is relational, something
like a field whose polarity can change: Thus, “up” versus “down” is, in
fact, a relationship that adjusts to the navigational needs of the individual.
With respect to my question as to where one’s feet are located in such an
upside-down/down-side-up world, why not repeat Stratton’s experiment
and ask whether occulocentric and egocentric spaces become dissociated?
My opportunity arose when two of my undergraduate students at Radford
University in Virginia volunteered to repeat the experiment. One would
wear the prism-glasses; the other would guide him around until his world
was again negotiable, right side up.

Once the perceptions of the student wearing the glasses had
stabilized, I had my chance to ask the long-held question: Where are your
feet? “Down there,” my student said and pointed to his shoes on the
ground. I placed my hand next to his and pointed down, as he had, and he
saw my hand as pointing down.

Then I stepped away from him and placed my hand in the same
fashion, pointing to the ground as I saw it. He, by contrast, saw my hand as



pointing up! As I brought my hand closer to his body, there was a distance
at which he became confused about which way my hand was pointing,
until, when still closer to his body, my hand was perceived as pointing in
the same direction as he had originally perceived it when my hand was
next to his.

The region of confusion was approximately at the distance of his
reach and slightly beyond.

Occulocentric and egocentric spaces are therefore dissociable, and
each of these is also dissociable from object-centered spaces. After brain
trauma, patients have experienced “micropsia,” a condition in which all
objects appear to be miniscule, or “macropsia” where objects appear to be
oversized. In these conditions, occulocentric and egocentric spaces remain
normal.

Conceived as fields, it is not surprising that the objects in these
spaces are subject to a variety of contextual influences. Some of these
influences have been thoroughly investigated, such as the importance of
frames in object perception; others, such as those in the Stratton
experiment have been thoroughly studied, but no explanation has been
given as to how such a process can occur, nor has an explanation been
given for micropsia and macropsia.

I will venture a step toward explanation in noting that the Fourier
theorem and symmetry group theory offer, at the least, a formal
scaffolding for an explanation. The brain process that transforms the
sensory input from the space-time domain optical image to the spectral
domain at the primary visual cortex, and back to the space-time domain by
way of movement, actually should end up with up-down mirror images of
the occulocentric space. This is mathematically expressed in terms of
“real” and “imaginary” numbers—that is, as a real and a virtual image.
Ordinarily we suppress one of these images—probably by movement, but
exactly how has not been studied as yet.

We noted in the previous chapter that symmetry groups have the
characteristic that the group, the object, remains invariant across
expansion and contraction. This means that we ordinarily adjust for what,
in a camera, is the zoom of the lens. In my laboratory, my colleagues and I
were able to change the equivalent of the zoom of receptive fields in the
visual cortex by electrical stimulation of other parts of the cortex and of
the basal ganglia of the brain. Patients experienced such a change in zoom



during surgery at the University of California at San Francisco when their
brains were electrically stimulated in the frontal and in the parieto-
temporal regions of their cortex. Perhaps the adjustment of the zoom,
which in our experiments was effected through lateral inhibition, becomes
disturbed in patients who experience micropsia or macropsia.

Occulocentric space has been studied more thoroughly than the
“spaces” of other senses, primarily because painters have been interested
in portraying it. Painters needed to portray a three-dimensional
perspective on a two-dimensional surface and to take into account the
variety of constancies we perceive for objects, depending on the distance
they are from us. This brings us to the topic of horizons, a topic that shows
how our perceptions depend not only on the brain systems that are
involved but also on how these systems become tuned by the culture in
which we have been raised.

Nature and Nurture
At the dedication ceremonies of Brandeis University, Abraham

Maslow, Wolfgang Köhler and I were discussing perception. Köhler
insisted that the way we perceive is inborn. “If learning is involved, it isn’t
perception,” he declared. With regard to perception, Köhler was what is
referred to in psychology as a “nativist.” Today, it is hard to remember
how entrenched this view was; so much evidence has since been obtained
that indicates just how what we learn influences how we perceive. Much of
this shift in viewpoint is due to the research and writings of Donald Hebb
that drew attention to how dependent our current perception of perception
is based upon what we have learned.

Hebb’s contribution came exactly at the right moment. During the
mid-1950s, sitting at my dining room table, Jerome Bruner wrote a
landmark paper entitled “The New Look in Perception.” Among other
experiments, in this paper, Bruner recounted the demonstration that how
large we perceive a particular coin to be—a quarter, for instance—depends
on how wealthy or poor we are. Such an everyday experience attests to
how much the context of our perception—in this case, what we have
learned about the value of a coin—influences how we perceive it.

These insights were anticipated in anthropology and sociology, where
studies of peoples of different cultures showed marked differences in their



perceptions. For instance, in Somalia there is no appreciation for the color
red but a great differentiation of the spectrum that we ordinarily classify
as green. The brain processes reflecting such differences are now
accessible to study with fMRIs and other brain-imaging techniques.

The argument can still be made that sensitivity to context is inherited.
This is largely correct. In fact, the whole discourse on what is inherited
and what is learned takes on a new significance when inheritance is
considered as a potential rather than a full-blown established capacity
received at birth. The Nobel laureate, behavioral zoologist Konrad Lorenz
made this point with respect to learning: he pointed out that different
species have different potentials for learning this or that skill.

Sandra Scarr, professor of psychology at the University of Virginia,
has gone a step further in suggesting a test for determining “how much” of
a skill is inherited: she has shown that, to the extent to which a perception
or skill comes easily—that is naturally—to us, it is, to that extent,
inherited. Thus, our ability to speak is highly inborn, but our ability to
read is markedly less so. Though we all talk, whether we learn to speak
English or Chinese depends on the culture in which we grow up. How
fluent we may eventually become in Chinese or in English also depends on
learning, which in turn depends on the stage of maturation of our brain
when the learning is undertaken.

Mirrors
How we perceive becomes especially interesting, almost weird, when

we observe ourselves and the world we navigate in mirrors. Like author
and mathematician Lewis Carroll in the experiments he often
demonstrated to children, I have all my life been puzzled by mirrors. How
is it that within the world of mirrors right and left is reversed for us but up
and down is not? How is it that when I observe myself in a mirror, my
mirror image is “in the mirror,” but the scene behind me is mirrored
“behind the mirror?”

Johannes Kepler proferred an answer to these questions in 1604.
Kepler noted that a point on the surface of a scene reflects light that
becomes spread over the surface of the mirror in such a way that when
viewed by the eye, it forms a cone behind the mirror symmetrical to the
point. His diagram illustrates this early insight.



I realized the power of the virtual image while reading about Kepler’s
contribution, lying on a bed in a hotel room. The wall at the side of the bed
was covered by a mirror—and “behind” it was another whole bedroom! An
even more potent and totally confusing experience occurred while I was
visiting a famous “hall of mirrors” in Lucerne, Switzerland. Speak of
virtual horizons! I saw my colleagues and myself repeated so many times
at so many different distances and in so many different directions that I
would certainly have become lost in the labyrinth of images had I not held
onto an arm of one of my companions.

On this occasion, I was given a reasonable “explanation” of the how
of mirror images. I was told that there had been an explanation presented
in Scientific American some time ago which was summarized thus:
ordinarily we have learned to look forward from back to front—so when
we are looking in a mirror we see what is in front of us, behind the mirror,
as if we are coming at the virtual image from behind it.

37. Kepler’s diagram of the reflection of a point (S) on a plane mirror (M—M’) as seen (I) as a
virtual image by a person’s eyes

Support for this explanation comes from an experience many of us
have had when we have tried to teach someone else how to tie a necktie
(especially a bow tie). If you face the person and try to tie the tie, the task
is awkward if not impossible. But if you stand behind the person, look
forward into a mirror, and tie his tie, the job is easy.

The above observations were made on a plane mirror. A concave
mirror presents another wonderful phenomenon. I have a 7X concave
mirror in my bathroom. Looking at close distance to my face, I see its
reflection “in the mirror.” But one day I noted that the bathroom’s ceiling
light was floating in midair, suspended over the bathroom sink.



Investigating the source of the image showed that when I covered the
mirror with my hand the image disappeared.

The examples presented to us by mirrors show that what we see
depends on how we see. The next sections describe some of what we now
know about the “how” of seeing.

Objects in the World We Navigate
How we perceive objects is dependent on brain systems that process

invariance, that is, the property of a perception to be maintained despite
relative movement between the perceived object and the perceiver. An
example is size constancy. In my Stanford laboratory, during the 1970s,
my colleagues and I performed the following experiment with monkeys:
They were taught to pull a string that was attached to a box containing a
peanut. There were two such boxes, one large and one small. The boxes
rolled along a rail within an alley whose sides were painted with broad
horizontal stripes that enhanced the feeling of distance. Each box could be
decoupled from its string so that its placement could readily be exchanged.
The monkeys were trained to pull in the box that appeared to them to be
the larger. They pulled in the larger box when it was close by and also
when it was way up the alley. “Larger” was larger to them at both
distances.

38. Image (I) produced from scene (S) reflected by a concave mirror

We then removed a part of each monkey’s cortex just in front of the
primary visual areas that receive the retinal input and those that regulate
the oscillatory movements of the eyes. The monkeys behaved normally on
all visual tests except one: now they would pull in the smaller box when it
was close while the larger box was far down the alley. At that distance, the
larger box made a smaller optical image on the monkeys’ retina. The



intact monkeys were able to compensate for distance; the monkeys that
had been operated upon could not.

When I tell my students about this experiment, I point out that their
heads look the same size to me whether they sit in the front or the back
row of the room. The students in front did not seem to suffer from
hydrocephalus (water-on-the-brain); the ones in back, most likely were not
microcephalic idiots.

Constancy in our perception holds within a certain range, a range that
varies as a function of distance being observed and the experience of the
observer. The range at which a particular constancy ceases forms a
horizon, a background against which the perception becomes recalibrated.
A classroom provides such a “horizon.” But out on a highway, automobiles
all look pretty much the same size over a longer range in terms of the
distance in our field of vision. What determines the horizon in this case is
the layout of the road and its surroundings. On a straight road in rolling
hills, where distances are truncated by the lay of the land, cars may look
small compared to what they might look like at the same distance on a U
curve, where the entire road is visible. When there is an unfamiliar gap in
visibility, multiple horizons are introduced. The placement of horizons
where constancy breaks down is a function of the context within which an
observation is occurring.

Professor Patrick Heelan, who helps teach my Georgetown class at
times, has pointed out that when painters try to depict three dimensions
onto a two-dimensional surface, size constancy is limited to a certain
range. Beyond that range, other laws of perspective seem to take hold.
Heelan is a theoretical physicist and philosopher of science whose
specialty is identifying how painters construct perspectives that allow us
to “see” three dimensions when a scene is displayed on a two-dimensional
surface.

On the basis of his 30-year study of paintings and of how we perceive
the cosmos, Heelan has concluded that overall visual space appears to us
as hyperbolic. For instance, as I mentioned earlier, when we observe
objects by looking into a concave mirror—the kind that has a
magnification of 7X—objects close to the mirror are magnified in it, but
things far away seem to be projected in front of the mirror between
ourselves and it—and those objects appear to us smaller than they are
when we look at them directly.



Leonardo da Vinci observed that the optics of the eye and the surface
of the retina do not compose a flat plane but a hemisphere, thus the
physiological process that forms our perceptions cannot be a two-
dimensional Euclidian one. Rather, this physiological process forms the
concavity of perceived configuration of hyperbolic space. There is a
change in the amount of concavity depending upon the distance that is
being observed: the closer we focus, the more the lens of the eye becomes
bulged; thus there is greater concavity. As our focus become more distant,
the lens flattens somewhat, which results ins less concavity. These changes
are like changing the magnification of the concave mirror from 7X to 5X
and 3X.

Heelan has proposed that a description of cosmic space as hyperbolic
accounts for a good part of the “moon illusion” which occurs when we
look at the sun as well. When the moon, or sun, is at the earth’s horizon, it
appears enormous. Atmospheric haze can add a bit more to this illusion.
But as the moon rises, away from our experienced earth-space defined by
its horizon, it appears to be much smaller. This change in perception is
identical to what we experience when we look at the 7X concave mirror.

Furthermore, when my collaborator Eloise Carlton modeled the brain
process involved in viewing the rotation of figures developed by Roger
Shepard, my colleague at Stanford, the process consisted of mapping a
geodesic path— the shortest route around a sphere—as in crossing the
Atlantic Ocean from New York to London via Iceland. Mathematically,
this is represented by a Hilbert space of geodesics —a Poincaré group —
which is similar to the mathematics used by Heisenberg to describe
quantum structures in physics and by Gabor in describing the minimum
uncertainty in telephone communications. It is the description of
processing by receptive fields that we have become familiar with
throughout these chapters.



39. Diagram of a horizontal plane at eye level with two orthogonals AB and CD, to the frontal
plane AOC (O is the observer): A’B’ and C’D’ are the visual transforms of AB and CD in a finite

hyperbolic model of visual space. M’N’ marks the transition between the near and the distant
zones. Contours are modeled on computed values. The mapping is not (and cannot be made)

isomorphic with the hyperbolic shapes, but some size and distance relationships are preserved.

This and similar observations raises the issue as to how much of what
we perceive is due to our perceptual apparatus and how much is “really out
there.” Ernst Mach, the influential early 20th-century Viennese physicist,
mathematician and philosopher, whose father was an astronomer, worried
all his life about the problem of how much of the apparent size of the
planets, stars, and other visual phenomena was attributable to the workings
of our perceptual apparatus and how much was “real.” Mach’s legacy has
informed the visual arts and philosophers dealing with perception to this
day.



40. Frontal planes: composite diagram in four sectors: 1. physical frontal plane (FP); visual
transform of FP at distance d (near zone: convex); and 4. visual transform of FP at distance 10d
(distant zone: concave). d is the distance to the true point for the hyperbolic model. Contours are

modeled or computed values for a finite hyperbolic space.

41. Diameter of the pattern subtends an angle of 90 degrees with the viewer. B. The diagram is
not a set of images to be read visually, but a set of mappings to be interpreted with the aid of the

transformation equations: the mappings are not (and cannot be made) isomorphic with the



hyperbolic shapes, but some size relationships are preserved in frontal planes (or what
physically are frontal planes).

In keeping with the outline presented in the Preface, the perceptions
we hold are a consequence of the territory we explore. Within our yards,
and within the context of portraying our living spaces onto a two-
dimensional surface, Euclidian propositions suffice. When we come to
perceive figures that rotate in space and time, higher-order descriptions
become necessary. It is these higher-order descriptions that hold when we
look at what is going on in the brain. The retina of our eye is curved, and
this curvature is projected onto the cortical surface. Thus, any process in
the sensory and brain systems that allows us to perceive an object’s
movement must be a geodesic: a path around a globe.

The Horizons of Our Universe
Current dogma in cosmology holds that our observable universe

began with a “big bang” and is exponentially expanding toward—who
knows what? But this is not the only possible interpretation of our
observations: Roger Penrose proposes that the technique of “conformal
rescaling” which he has so successfully applied to configuring tiles and
other perceptual formations can be profitably applied to configuring the
form of our observed universe. In doing so, Penrose discerns not a big hot
bang at the cosmological origin, but “a pattern like a gentle rain falling on
a still pond, each raindrop making ripples which expand and intersect each
other.” This description is almost word for word the description Leonardo
Da Vinci gave some centuries ago and is essentially a description of a
holographic process. A rescaling at the horizon of the observed cosmos
arrives at a similar holographic pattern.

Two conclusions can be drawn from Penrose’s reinterpretation of
cosmology:

1. It is our interpretation of observations that transforms a perceptual
occurrence into a form compatible with the world we navigate. This
interpretation is dependent on our human ability afforded by our
brain processes.

2. Behind the form we experience as the world we navigate is another, a
potential form that enfolds space, time and (efficient) causality: a



holographic form.



Chapter 8
Means to Action

Wherein I distinguish between the brain systems that process muscular
contractions, movements, and the actions that give meaning to navigating
our world.

The function of the sensory input giving rise to reflex activity . . .
is there to modulate the ongoing activity of the motor network in
order to adapt the activity (the output signal) to the
irregularities of the terrain over which the animal moves. . . .
This sensory-motor transformation is the core of brain function,
that is, what the brain does for a living.

—Rodolfo Llinás, I of the Vortex, 2001



At Issue
Navigating the world we inhabit takes a certain amount of skill. The

skills we have considered so far are those that allow us to move our
sensory receptors in such a way that we can have meaningful perceptions
of the world. Philosopher Charles Peirce, being a pragmaticist, noted that
what we mean by “meaning” is what we mean to do. To account for our
phenomenal experiences, I have added to his statement that meaning also
includes the following: what we mean by meaning is what we mean to
experience. In both cases “meaning” concerns actions. Brain research
performed in my laboratory, and in those of others, has shown us that an
act is not as simple as it may seem.

In the early 1950s, I came to study those brain processes that are
involved in the control of action. I was faced with a century-long tradition
of controversy: Were movements—or were muscles—represented in the
brain’s motor cortex? Anatomical evidence indicated that fibers from our
muscles, and even parts of those muscles, could be traced to the cortex.
This kind of representation is also found in the sensory cortex, where
specific locations of receptors can be traced to specific locations in the
sensory cortex. We call such cortical re-presentations of the geometry of
receptors and effectors in humans “homunculi,” a term which is discussed
in this chapter.

A different view of representation emerges when we electrically
stimulate the motor cortex: now we observe contractions of muscle
groups. Which groups are activated by this stimulation depends on the
position of the limb before stimulation of the cortex is begun. For
example, on one occasion, at the Neuropsychiatric Institute of the
University of Illinois at Chicago, Percival Bailey and Paul Bucy were
electrically stimulating the cortex of a patient, while I sat beside the
patient, reporting the resulting movement of this patient’s arm. Exactly the
same stimulation at the same location produced a flexion of the patient’s
forearm when his arm was extended, but it produced a partial extension
and rotation when his arm started out being flexed. The variations in
movements are centered on joints and are produced by transactions among
connections centered on the point of stimulation of the patient’s motor
cortex.

These differences resulting from variations in experimental technique
were not recognized as such; instead, they led to that just-mentioned



century-long conflict: “Are muscles or movements represented in the
brain’s motor cortex?” My interest in this controversy was ignited quite by
accident. It took me years to resolve it for myself, yet part of the answer is
simple: anatomically, muscles, and even parts of muscles, are connected to
the cortex. Physiologically, by virtue of interactions among cortical nerve
cells, functions are represented. But my experiments led to yet another
level of explanation: behaviorally, actions are represented. The story of
how I came to this realization, and how I then reached an explanation of it,
is another of those adventures that make discovery so richly rewarding.

In 1950, my colleagues at Yale and I were looking at cortical
responses obtained when peripheral nerves were stimulated. At the time,
oscilloscopes were not yet available for physiological studies. Fortunately,
one of my friends, Harry Grundfest at Columbia University, had gotten the
Dumas Company interested in implementing his design for an
oscilloscope, and after waiting for about a year, I was the recipient at Yale
of the second such instrument (the first went to Grundfest at Columbia, of
course). It took my collaborators and me a week or so to find out how to
make the oscilloscope work, but finally Leonard Malis, a fellow
neurosurgeon with a talent for engineering, helped initiate our first
experiment. While he and one of the graduate students, Lawrence Kruger,
were setting up, I was in the basement testing the behavior of monkeys in
another experiment. When I came on the scene, we had absolutely
gorgeous responses on the oscilloscope face: inch-high “spikes” obtained
from the brain whenever we stimulated the sciatic nerve of the monkey.

A Direct Sensory Input to the Motor Cortex
Once over the peak of my elation, I asked where the electrodes had

been placed on the brain. The polite answer I received was “On the cortex,
you dummy.” I asked where on the cortex—and went to see for myself.
What was left of my elation was punctured: the electrodes had been placed
on the pre-central gyrus—the motor cortex! The motor cortex was
supposed to be an output cortex, not one receiving an input from the
sciatic nerve. At the time, and even today, the established idea has been
that the nervous system functions much as a reflex arc: an input to a
processing center from which an output is generated. I held this same view
at the time, so I was understandably upset: These spikes must reflect some



sort of artifact, an artificial, irrelevant byproduct of the experimental
procedure.

While still observing these spikes, I immediately phoned two friends,
experts on stimulating the brain cortex: two neurophysiologists, one at
Johns Hopkins University and the other at the University of Wisconsin,
Clinton Woolsey and Wade Marshall. Both stated that they had seen this
result repeatedly in their experiments using more primitive recording
techniques. Woolsey added that he had even published a footnote in the
Proceedings of the American Physiological Society noting that these
“spikes” must be artifacts—that is, not a real finding but an unwanted
byproduct of the mechanics (such as a short circuit in the electrical
stimulating apparatus) of the procedure. I found this strange: An artifact
that is always present and in several laboratories? We were all puzzled.

42. Diagram of essential connections of the motor system



Wade Marshall came to the laboratory to see for himself what we
were observing. He felt that the “artifact” might be due to a spread of
current within the cortex, starting from the sensory cortex to include the
motor cortex. Together, we used a technique called “spreading cortical
depression” that is induced by gently stroking the cortex with a hemostat.
Using this procedure, within a few seconds any neighboring cortically
originated activity ceases. The electrical stimulation of the sciatic nerve
continued to produce a local brain response, though the response was now
of considerably lower amplitude than it had been before stroking the
cortex. The remaining response was subcortical, initiated by nerve fibers
reaching the cortex prior to our stroking the cortex, which had amplified
the response.

So, I asked: If the input from the sciatic nerve is real, what might be
the pathways by which its signals reach the cortex? Larry Kruger decided
that this would provide him an exciting thesis topic. So, I surgically
removed a monkey’s post-central cortex—the part of the brain known to
receive the sensory input from the sciatic nerve—and therefore a possible
way station to the adjacent pre-central motor cortex. The “artifact” was
still there.

Next I removed a hemisphere of the monkey’s cerebellum, a portion
of the primate brain that was known to receive a sciatic nerve input and to
provide a pathway to the pre-central motor cortex. Again, there was no
change in the “artifact”—the spikes were still there. Finally, I surgically
removed both the post-central cortex and the cerebellar hemisphere. Still
no change in the response in the motor cortex to sciatic stimulation!

During these experiments, we discovered another puzzling fact. It
seemed plausible that some input to the motor cortex might originate in
the muscles. But we found, in addition, that stimulation of the sciatic
nerve fibers originating in the skin also produced a response in the motor
cortex.

Larry Kruger settled the entire issue during his thesis research by
tracing the sciatic input directly to the motor cortex via the same paths in
the brain stem that carried the signals to the post-central sensory cortex.

The research that I’ve summarized in these few paragraphs took more
than five years to complete. So we had plenty of time to think about our
findings.



What Must Be
What we call the motor cortex is really a sensory cortex for

movement. This conclusion fits another anatomical fact that has been
ignored by most brain and behavioral scientists. Notable exceptions were
neurosurgeon Wilder Penfield and neuroscientist Warren McCulloch,
whose maps of the sensory cortex did include the pre-central region. Their
reason was that the anatomical input to the motor cortex arrives there via
the dorsal thalamus (dorsal is Latin for back, thalamus is Latin for
"chamber") that is the halfway house of all sensory input to the entire
brain cortex except the input for smell. The dorsal thalamus is an
extension of the dorsal horn of our spinal cord, the origin of input fibers
from our receptors, not of output fibers to our muscles.

Nineteenth century European scientists had labeled this part of the
cortex “motor” when they discovered that electrically stimulating a part of
the cortex produced movements of different parts of the subject’s body.
When they mapped the location of the cortical origins of all these
stimulus-produced movements, the result was a distorted picture of the
body, a picture which exaggerated the parts with which we make fine
movements, such as the fingers and tongue. Such a map is referred to in
texts as a “homunculus” (a “little human”).

While Malis, Kruger and I at Yale were looking at the input to the
cortex surrounding the central fissure, Harvard neuro-anatomist L. Leksell
was looking at the output from this cortex. This output had been thought to
originate exclusively in the pre-central motor cortex. Now Leksell found
this output to originate as well from the post-central sensory cortex. Those
fibers that originate in our pre-central cortex were the largest, but the
origin of the output as a whole was not limited to these large fibers.

For me, all of these new discoveries about the input to and the output
from the cortex surrounding the central fissure called into question the
prevailing thesis of the time: that the brain was composed of separate
input and output systems. Today, this thesis is still alive and popularly
embodied in what is termed a “perception-action cycle.” However, in our
nervous system, input and output are in fact meshed. The evidence for this
comes not only from the brain research conducted in my laboratory, as
detailed above, but also from discoveries centered on the spinal cord.



The Reflex
During the latter part of the 19th century two physiologists, one

English, Charles Bell, and the other French, François Magendie, conducted
a series of experiments on dogs, cutting the roots of various nerves such as
the sciatic nerve. These nerve roots are located at the point where the
nerves connect to the spinal cord. Some of the fibers making up these
nerves are rooted in the dorsal, or back, region of the spinal cord, the
others in the ventral, or ”stomach,” region. When the dorsal fibers were
cut off at the root, the dogs had no difficulty in moving about but did not
feel anything when the experimenters pinched the skin of their legs with
tweezers. By contrast, when the experimenters cut the ventral roots, the
dogs’ limbs were paralyzed, though the dogs responded normally to being
pinched. The results of these experiments are famously enshrined in
neuroscientific annals as the “Law of Bell and Magendie.”

In the decades just before and after the turn of the 19th to 20th
centuries, Sir Charles Sherrington based his explanation of his studies of
the spinal cord of frogs on the Bell and Magendie law. It was Sherrington
who developed the idea of a reflex—an “input-output arc,” as he termed it
—as the basis for all our nervous system processing. Sherrington clearly
and repeatedly stated that what he called an “arc” was a “fiction,” a
metaphor that allowed us to understand how the nervous system worked:
Reflexes could combine in a variety of ways. For instance, when we flex
an arm, contraction of the flexion reflex must be matched simultaneously
by a relaxation of the extension reflex; when we carry a suitcase both
flexors and extensors are contracted.

These ideas are as fresh and useful today as they were a hundred
years ago despite the fact that Sherrington’s fiction, of an “arc” as the
basis for the reflex, has had to be substantially revised.

Unfortunately, however, today’s textbooks have not kept up with these
necessary revisions. On several occasions, I have written up these
revisions at the request of editors, only to have the publishers of the texts
tell the editors that the revisions are too complex for students to
understand. (I have wondered why publishers of physics texts don’t
complain that ideas, such as Richard Feynman’s brilliant lectures on
quantum electrodynamics, are too complex for students to understand.)

Sherrington’s “fiction” of a reflex as a unit of analysis of behavior
was the staple of the behaviorist period in psychology. My quarrel is not



with the concept of a reflex but with the identification of “reflex” with its
composition as an arc. The organization of the reflex, and therefore of
behavior, is more akin to that of a controllable thermostat. The evidence
for this stems from research performed in the 1950s that I will describe
shortly. The change in conception was from the metaphor of an “arc,”
where behavior is directly controlled by an input, to a metaphor of a
“controllable thermostat,” where behavior is controlled by the operations
of an organism in order to fulfill an aim. This change has fundamental
consequences, not only for our understanding of the brain and of our
behavior, but also for society. Doling out charities is not nearly as
effective as educating a people, setting their aims to achieve competence.

How It All Came About
The evidence that produced such a radical shift in the way we view

our place in the world was gathered during the 1950s. Steven Kuffler,
working in John Eccles’s laboratory in Canberra, Australia, had shown that
about one third of the fibers in the ventral roots of peripheral nerves—the
roots that convey signals to muscles from the spinal cord— end, not in
contractile muscle fibers, as had been believed, but in the sensory
receptors of the muscles. These are the receptors that respond to stretching
of the muscle to which they are attached. The receptor is therefore
activated not only to external stimulation that results in stretching the
muscle but also to signals coming from the spinal cord. This means that
the brain can influence the muscle receptor.

Thus there is no simple way to gauge whether the receptor
stimulation is the consequence of input to our receptors from outside our
bodies or from input to the receptor originating inside the body. When we
tense our muscles while we are standing against a strong wind, the muscle
receptors are influenced in the same way as when we are tense while
taking a test. Only some higher-order determination of the origin of the
tension would tell us whether the tension was produced externally or
internally.

During the remainder of the 1950s, scientists demonstrated this same
sort of internal control of receptor function over all receptors except those
involved in vision. Powerful internal top-down controls originating in the
brain were identified and their effect on the operations of the receptors in



the skin, the nose and the ear were studied. We came to know that the body
is more than a receiver; it is a receiver that is adjustable to the needs of the
body. George Miller, Eugene Galanter and I decided to write a book that
showed the relevance of this revised view of receptor function to the field
of psychology. Plans and the Structure of Behavior is based on the theme
that the unit of behavior is not a reflex arc but an input to a “test-operate-
test-exit” sequence, a TOTE, which is also the unit that makes up a
computer program.

43. Summary diagram of the gamma (y) motor neuron system. Redrawn after Thompson, 1967.

Biological Control Systems
The history of the invention of the thermostat helps make the

biological control systems more understandable. During the 1920s and
1930s, Walter Cannon at Harvard University, on the basis of his research,
had conceptualized the regulation of the body’s metabolic functions in
terms of a steady state that fluctuated around a baseline. We get hungry,
eat, become satiated, metabolize what we have eaten, and become hungry
once more. This cycle is anchored on a base which later research showed
to be the organism’s basal temperature. He called this regulatory process
“homeostasis.” A decade later, during World War II, Norbert Wiener, who



had worked with Cannon, further developed this idea at the Massachusetts
Institute of Technology, where he used it to control a tracking device that
could home in on aircraft.

After the war, engineers at Honeywell applied the same concept to
track temperature: they created the thermostatic control of heating and
cooling devices. In the 1950s, the concept of a thermostat returned to
biology, becoming the model for sensory as well as metabolic processing
in organisms.

In the process of conducting experiments to find out where such
tracking—such controls on inputs—originate, neuroscientists found that,
except in vision, control can be initiated not only from structures in our
brain stem but also by electrical stimulation of higher-order systems
within the brain.

Because our visual system is so important to navigation, I set my
Stanford laboratory to work to determine whether or not our brain controls
our visual receptors as had been demonstrated to be the case for muscle
receptors, touch, smell and hearing. My postdoctoral student Nico Spinelli
and I tested this possibility by implanting a micro-electrode in the optic
nerve of cats, and then, at a later date, stimulating the footpads of the
awake cats, using the end of a pencil. One of us would stimulate the cat’s
paw while the other supervised the electrical activity recorded from the
cat’s optic nerve. We obtained excellent responses as shown on our
oscilloscope and computer recordings.

We next went on to use auditory stimuli consisting of clicking
sounds. Again we were able to record excellent responses from the cat’s
optic nerve. We were surprised to find that the responses in the optic nerve
arriving from tactile and auditory stimulation came at practically the same
time as those we initiated by a dim flash. This is because the input to the
brain from the tactile and auditory stimulation is much faster than the
visual stimuli that have to be processed by the retina. There is so much
processing going on in the fine fibers of the retina (recall that there is no
nerve impulse rapid transmission within the retina) that the delay in optic
stimulation reaching the optic nerve directly is equal to the delay produced
by processing of tactile and auditory stimuli in the brain. We wondered
whether this coincidence in timing might aid lip reading in the hearing
impaired.



When the cat went to sleep, the responses recorded from the optic
nerve ceased. Not only being awake but also not being distracted turned
out to be critical for obtaining the responses. A postdoctoral student,
Lauren Gerbrandt, came to my Stanford laboratory and, excited by what
we had found, tried for over a year to replicate our findings in monkeys.
Sometimes he obtained the responses from the optic nerve; other times he
did not. We were upset and frustrated at not being able to replicate an
experimental result that we had already published.

I urged Gerbrandt to choose some other experiment so that he would
have publishable findings before his tenure in the laboratory was up. He
asked if it would be OK to continue the visual experiments in the evening,
and I offered to assist him. He got his wife to help him bring the monkeys
from their home cages to the laboratory. The results of the experiments
were immediately rewarding. He obtained excellent responses every time,
results that I came to witness. But this run of good luck did not last. His
wife became bored just waiting around for him and asked if it would be all
right for her to bring a friend. Now the responses from the optic nerve
disappeared. I came in to witness what was happening and after a few
nights the answer became clear to us: We asked the women, who had been
chatting off and on, to be quiet. The responses returned. Having
established the cause of the variability in the experimental results, we
moved the experiment to the room where the original daytime experiments
had been performed. Now we found good optic nerve responses whenever
the adjacent hall was quiet. When someone in high heels went by, or
whenever there was any commotion, the responses disappeared. We had
found the cause of the “artifact.”

By recording the electrical activity of subcortical structures during
the experiment, we were then able to show that the distracting noise short-
circuited cortical control at the level of the thalamus, the halfway house of
the visual input to the cortex. Thus, we had not only demonstrated cortical
control over visual input but had also shown the thalamus to be a
necessary brain component involved in the control of the feedback
circuitry.

Controlling the Thermostat



The change from viewing the fundamental unit of behavior as an arc
to viewing it as a thermostat-like feedback inspired George Miller, Eugene
Galanter and me to write Plans and the Structure of Behavior (1960), a
book still regarded as seminal to the cognitive revolution in psychology.
An anecdote suggests why this is so.

I presented the Sechenov lectures at the University of Moscow during
the Soviet period in the 1970s. I. M. Sechenov, for whom the lectures were
named, had done for Russian brain science what Sherrington had done in
establishing the reflex as the basic unit of behavior. During the discussion
period after my lecture, I was asked, “Professor Pribram, did we hear you
correctly, did you say you did not believe that the reflex exists?” I
answered without hesitation, “No, I did not say that. I presented evidence
that the organization of the reflex is not an arc but is more like that of a
thermostat.”

After the session was over, my host and friend, Alexander
Romanovitch Luria, the famous Soviet neurop sychologist, put his arm
around my shoulder and said, “Good work, Karl. If you had said that you
did not believe in the reflex, you would never have been welcome in the
Soviet Union again!” What Luria, Leontiev, the head of Psychology at the
Russian Academy of Sciences, and I were trying to accomplish was to
move Soviet science from a view based exclusively on Pavlovian
conditioning to a stance in which control operations informed and formed
the brain, behavior and society. This view had been developed into a field
of study called “cybernetics.” Some years later—in 1978—Leontiev, in his
welcoming address to the International Psychological Congress in
Moscow, framed our ideas as being based on reflections rather than on
reflexes.

Cybernetics (the art of steering) deals with the way we navigate our
world. The brain is conceived as the organ that enables us to steer a steady
course that we have decided to set for ourselves. Contrary to the tenets of
mid-19th century psychology in the Soviet Union, Europe and elsewhere,
we are not totally at the mercy of our environment. It is not an input-
output world that we are conditioned to travel. We choose. The world is
meaningful because we mean to choose where, when and how we navigate.

My books Plans and the Structure of Behavior (1960) and Languages
of the Brain (1971) were huge successes in the Soviet Union. Once, the
lady in charge of the floor in the hotel where I was staying said she had to



see me rather urgently. I wondered what infraction of the many rules I had
been guilty of. “No, no. All is well. Would you please autograph my copy
of Languages of the Brain, which I enjoyed so much, before I go off
duty?”

By contrast, in America, Plans and the Structure of Behavior received
“interesting” reviews. One reviewer remarked that we had written the book
in California and had probably suffered sunstroke. Another reviewer was
more generous: “Most people do not become senile until they are at least
60,” he stated. “These authors aren’t even 40.”

But one reviewer was helpful: Roger Brown, a noted Harvard linguist,
pointed out that our purpose was to show that humans were not just robots
at the mercy of the environment but sentient organisms, at least somewhat
in control of their fate. Brown indicated that we had perhaps taken a first
step, but had not succeeded in our mission.

George Miller, Gene Galanter and I had thought that by modeling
human planning on a second (the first being the thermostat) metaphor—
computer programming—that we had done more than take a first step:
Both plans and programs are created before they are actually
implemented, and both run their course unable to change as a consequence
of being interrupted. If interrupted, they must be started again from the
beginning. But the sensitivity that a great pianist shows when he covers
over a small mistake even as it occurs, or the accomplished actor who can
cover not only his own missed cues but those of other actors as well, are
not the characteristics of computer programs. Accomplished humans do
not have to start their performances all over again when some small
mistake can be covered by a change in the specifics of the performance.
Something was indeed still missing from our proposal that plans are
identical to computer programs.

Controlling the Thermostatic Control
Over the next decade, a few of us—Hans Lukas Teuber, professor of

psychology at MIT; Horst Mittlestedt of the Max Planck Institute at
Munich; Ross Ashby of the University of London; and I—discussed this
problem on many occasions, and we finally discovered the missing
ingredient.



A home thermostat would be too inflexible if it did not have a knob
or wheel to set the thermostat’s control (technically its set-point) to the
temperature we desire. A controllable thermostat allows us to regulate the
temperature of a room according to our needs. When the sun sets, the
walls of our houses become colder. We begin to radiate the warmth of our
bodies toward those walls instead of having the walls radiate warmth to us.
We can change the setting of our thermostat according to our need. A
control such as this operates by adjusting the separation between two wires
that close a circuit when they touch. Closing the circuit (a digital—off or
on—implementation) turns on the heater or air-conditioner. Ordinarily the
separation between wires is maintained and regulated by the amount of
heat in the room because the wires expand when heated. The control wheel
or knob adds a second factor working in parallel with the heat sensitivity
of the wires: an “external” factor—the control wheel—operates in parallel
to an “internal” factor, the amount of heat in the room.

Roger Sperry and Lucas Teuber named this additional control a
“corollary discharge.” Actually, the controlling process must slightly
precede, that is anticipate, the initiation of movement in order to be
effective. Soon, such anticipatory controls were found in the brain. Using
microelectrodes, the corollary discharge was seen to originate in the
frontal cortex, coincident with the initiation of voluntary eye movements.
The controls operate on a group of cells in the brain stem that are adjacent
to those involved in actually moving the eyes. Shortly, another
experimental result showed that the initiation of all voluntary movement
is anticipated by a “readiness potential”—that is, by activity on the medial
surface of the frontal cortex just in front of the motor cortex. Using direct
current recording, an even earlier electrical change was recorded in the
more anterior parts of the frontal lobe.



44. The TOTE servomechanism modified to include feedforward. Note the parallel processing
feature of the revised TOTE.

The neuroscience community had thus progressed from conceiving of
the elementary unit of behavior as a reflex arc to thinking of it as a
thermostat-like, programmable process that is controllable from separate
parallel sources. Even the biological homeostatic process, the inspiration
for the thermostat, is now known to be rheostatic (rheo is Latin for
“flow”), a programmable, adjustable process.

Each step of this progression in our thinking took about a decade. But
as we shall soon see, even these steps were not enough to fully account for
our intrinsic abilities not only to navigate but also to construct the world
we inhabit. Now we are finally ready to examine the brain systems that
implemented these changes in our views.

Muscles, Movements and Actions
I return therefore to the controversy that initiated my interest in the

motor cortex of the brain: Are muscles or movements represented in the
pre-central motor cortex? To find out, I devised a belt with a cuff within
which one arm of a monkey could be restrained. Four monkeys so outfitted
were each trained to open the lid of a box. The lid had a slot through which
a metal loop protruded. A large nail attached to a leather thong was passed



through the loop. The monkey had to pull the nail out of the loop and raise
the lid in order to obtain a peanut. I timed the duration of the sequence of
responses, and my students and I filmed the movements with a time-lapse
camera that allowed us to inspect the movements in detail.

After each monkey had reached a stable performance, I surgically
removed the entire motor cortex on the side opposite to the hand the
monkey was using. I expected that the sequence of movements would be
severely disturbed. It was not. All monkeys showed some clumsiness —it
took over twice as long for them to retrieve the peanuts —but our film
showed no evidence of the monkeys’ muscle weakness or any impairment
in the sequences of their muscle contractions (that is, of their movements).

As a control procedure, we had originally filmed the monkeys in their
home cage, climbing the sides of the cage and grabbing food away from
other monkeys. Frame-by-frame analysis of these same acts performed
after surgery showed no change in patterns or timing of movements, nor
any muscle weakness.

With two additional monkeys, I performed the same experiment after
they had been trained to open the box with each hand separately. From
these additional monkeys I removed the appropriate cortex on both sides
of the brain—both times with the same result.

I need to note again an important point about the surgery. The cortex
was removed with a fine tube through which suction could be applied. The
amount of suction was controlled by a small thumbhole near the held end
of the tube. The gray matter, composed of the cell bodies of neurons, can
be removed with this instrument without damaging the underlying stiffer
white matter, which is composed of nerve trunks that connect different
parts of the cortex to each other and to subcortical structures. The damage
produced by removing only the gray matter is local, whereas damage to
the white matter produces more serious disabilities because more distant
regions become involved. The brain damage produced in patients by
strokes or accidents always includes the brain’s white matter.

The Act
I came to understand the distinction between movement and action as

a result of these experiments with monkeys, experiments that were
undertaken at the same time as those in which we stimulated the sciatic



nerve. The motor cortex turned out to be not only a sensory cortex for
action—an act turned out to be more of a sensory accomplishment than a
particular set of movements.

Behaviorally, the motor cortex is not limited to programming muscle
contractions or movements. This cortex is involved in making actions
possible. An act is a target, an attractor toward which a movement intends.
I illustrate this for my classes by closing my eyes, putting a piece of chalk
between my teeth, and writing with it on the blackboard. It matters not
whether I use my right hand or my left, my feet or my teeth—processes in
my motor cortex enable what becomes written on the board. What is
written is an achievement that must be imaged to become activated. Such
“images of achievement” are the first steps leading to action, whether
literally, as in walking, or at a higher, more complex level as in speaking
or writing.

The answer to the initial question that started my research on the
motor cortex can be summarized as follows: anatomically, muscles are
represented in the motor cortex; physiologically, movements around joints
are represented there. Behaviorally, our cortical formations operate to
facilitate our actions. How?

For ethologists—zoologists studying behavior, as for instance Konrad
Lorenz and Niko Tinbergen—the study of behavior is often the study of
particular movements, the fixed patterns of a sequence of muscle
contractions made by their subjects. Their interest in neuroscience has
therefore been on the spinal and brain stem structures that organize
movements.

By contrast, my interest, stemming from the experiments on the
motor cortex, has been in line with the behaviorist tradition of psychology,
where behavior is defined as an action. An act is considered to be the
environmental outcome of a movement, or of a sequence of movements.
B. F. (Fred) Skinner remarked that the behavior of his pigeons is the
record made when they pecked for food presented to them according to a
schedule. These records were mainly made on paper, and Skinner defined
behavior as the paper records that he took home with him. They were
records of responses, and Skinner was describing the transactions among
these responses: he described which patterning of responses led to their
repetition and which patterns led to the responses being dropped out. The



interesting question for me was: What brain processes are entailed in these
transactions?

An anecdote highlights my interest. The United Nations University
hosted a conference in Paris during the mid-1970s. Luria, Gabor and other
luminaries, including Skinner, participated. In his address, Skinner
admitted that indeed he had a theory, something that he had denied up to
that time. He stated that his theory was not a stimulus-response theory nor
was it a stimulus-stimulus theory; his theory was a response-
reinforcement theory. Furthermore, he stated, reinforcement was a process.
I felt that he had given the best talk I’d ever heard him give and went up to
congratulate him. Skinner looked at me puzzled: “What did I say that
makes you so happy?” I replied, “That reinforcement is a process. The
process must be going on in the brain.” I pointed to my head. Skinner
laughed and said, “I guess I mustn’t say that again.” Of course he did. In
1989, a year before he died, he wrote:

There are two unavoidable gaps in the behavioral account:
one between the stimulating action of the environment and the
response of the organism and one between consequences and the
resulting change in behavior. Only brain science can fill those
gaps. In doing so it completes the account; it does not give a
different account of the same thing.

Reinforcement Revisited
Skinner’s contribution to the nature of reinforcement in learning

theory has not been fully assimilated by the scientific and scholarly
communities: Skinner and his pupils changed our conception of how
learning occurs from being based on pleasure and pain to a conception
based on self-organization, that is, on the consequences produced by the
behavior itself. Pavlov’s classical conditioning, Thorndike’s “law of
effect,” Hull’s “drive reduction” and Sheffield’s “drive induction”—even
the idea that interest is some sort of drive—are all different from what
Skinner’s operant conditioning revealed. In my laboratory, a colleague and
I raised two kittens from birth to demonstrate the self-maintenance and
self-organizing aspects of behavior. We nurtured one kitten with milk on
its mother’s breast; we raised a littermate on the breast of a non-lactating
cat and fed the kitten through a stomach tube. After six months, there was



practically no difference in the rate of sucking between the two kittens.
Mothers who have tried to wean their infant from sucking on his thumb or
on a pacifier might feel that such an experimental demonstration was
superfluous.

The change in conception of reinforcement from being based on
pleasure and pain to being based on performance per se is important for
our understanding of human learning. Rather than rewarding good
behavior or punishing bad behavior, operant behaviorism suggests that we
“educate” (Latin e-ducere, “lead” or “draw out”) a person’s abilities— that
we nourish the performances already achieved and allow these
performances to grow by leading, not pushing or pulling. In these
situations, pleasure is attained by achievement; achievement is not
attained by way of pleasure. I did not have to look at my score sheets to
find out whether my young male monkeys were learning a task: their erect
penises heralded their improvement—and conversely, when their
performance was lagging, so was their penis.

My good students have used a technique that I used in college. We
review and reorganize our notes about once a month, incorporating what
we have learned that month into our previous material. A final revision
before the final exams relieves one of useless night-long cramming. I say
useless because crammed rote learning needs much rehearsal—as in
learning multiplication tables—to make a lasting impression. What has
been learned by cramming is well forgotten by the next week.

More to Be Explained
The results of my experiments on the motor cortex, and the

conclusions based on them, were published in the 1950s while I was still at
Yale. Our results were readily accepted and even acclaimed: John Fulton,
in whose laboratory I conducted these studies, called the paper “worthy of
Hughlings Jackson,” the noted English neurologist whose insights about
the functions of the motor cortex were derived from studying epileptic
seizures.

My conclusions were that the removals of motor cortex had produced
a “scotoma of action.” In vision, a scotoma (from the Greek scotos,
“darkness”) is a dark or blind spot in the visual field. In Chapter 6 we
reviewed the evidence that some patients can reasonably well navigate



their blind visual field despite being consciously “blind” within that dark
spot or field. The scotoma of action produced by my removals of the
motor cortex had a similar effect: the monkeys’ difficulty was restricted to
a particular task, though they could still perform that task with some loss
of skill, as demonstrated in film records of their behavior and by the
longer time it took them to reach the peanut in the box. In my experiments
the “scotoma of action” referred to a particular task rather than to a
particular area of perceived space as in vision—and not to a difficulty with
any particular muscle group or a particular movement.

Today a great deal has been learned regarding the specific targeting of
an action. Much of the experimental and clinical research has been
performed on “visuomotor” achievements such as grasping an object. The
brain cortex that has been shown involved lies somewhat behind and in
front of the cortex surrounding the central fissure, the cortex involved in
the experiments on primates reviewed above. This may well be because in
primates the central fissure has moved laterally from its position in
carnivores, splitting the visuomotor related cortex into parietal and frontal
parts.

The How of Action
Despite the acclaim given to the results of my early experiments, they

gave me a great deal of trouble: I could not envision a brain process that
would encode an act rather than a movement. Lashley had noted the
problem we must face when he described the instinctive behavior of a
spider weaving a web. Essentially, the same pattern is “woven” in various
environments that may be as diverse as the regular vertical posts on our
porch or the forked branches of trees. Technically, this phenomenon is
called “motor equivalence” and the issue troubled us because we could not
imagine any storage process in the brain that might result in virtually
identical behaviors under many diverse circumstances. In humans, motor
equivalence is demonstrated when we write the same message on a
horizontal or on a vertical surface, or even in the sand with the toes of our
left foot. Many of the actual movements involved are different.

Some seven years passed after the publication describing our
experiments on the motor cortex. During that time, I continued to be at a



total loss for an explanation. I kept searching for a clue as to how a
“scotoma of action” might have been produced by my experiments.

A series of fortunate circumstances provided the answer. In the mid-
1970s, while I was lecturing at the University of Alberta, interaction with
the university’s professors provided a most enriching experience. These
discussions were enhanced by talks about realism with Wolfgang Metzger,
an influential psychologist from Germany who sharpened my views on
how our brains contribute to navigating the reality of the world we
perceive. Much of this chapter is framed within the conclusions I reached
at that time.

As noted in Chapter 5, James Gibson, professor at Cornell University
famous for his experiments and ideas on visual perception, visited Alberta
for prolonged periods while I was there, and we discussed in depth a
number of issues regarding his program that came to be called “ecological
perception.” Gibson was calling attention to various aspects of the
environment, such as horizons and shadows, that determine our
perceptions. In terms of Metzger’s realism, I agreed with all that Gibson
was saying; but at the same time I also insisted that specific brain
processes were necessary to “construct” what we perceive: that ecology
extended inside the organism as well as outside.

There was good reason for Gibson’s stance. Cornell had been the
central bastion of introspective psychology in America. Introspective
psychology was founded on the technique of asking people what they saw,
heard, felt or thought. Behaviorism reacted against introspective
psychology because, using the technique of asking for answers,
psychologists had great difficulty in confirming, between subjects and
even in the same subject, answers to questions asked. This was no way to
construct a science of psychology. Gibson, who had been a pupil of E. B.
Tichner, the towering figure and promulgator of introspection, had
rebelled against introspection just as thoroughly as had those
psychologists who were studying behavior. Gibson’s experimental
approach to perception displayed figures on oscilloscope screens and
specified the changes in the display that resulted in changes of perception.
Shortly, we will see the power of this approach in resolving the issue about
the very functions of the motor cortex that were puzzling me.



A Fortuitous Return to the Motor Cortex
It was Sunday morning of my last week of lectures in Alberta, and I

had been asked to talk about the “motor systems” of the brain. I agreed
and added that there were some problems that perhaps the participants
could help me resolve.

I’m an early riser. What to do on a Sunday morning? I started to make
notes for my lecture: “The Motor Cortex.” Twenty minutes later I threw
the blank sheet into my wastebasket. Another blank sheet: “The Motor
Systems of the Brain.” Another twenty minutes. Into the basket. After an
hour or so, I gave up. As I was leaving in a week, I could usefully pack
books that I would not need anymore and ship them home.

While packing, I came across a translation from Russian of a book by
Nikolas Bernstein, whose work had been highly recommended to me by
Alexander Romanovich Luria at a dinner in Paris. I had skimmed the
Bernstein book at least twice and had found it uninteresting. With regard
to brain function, Bernstein’s insight was that whatever the brain process,
it was nothing like the process of our experience. Not exactly a new
thought for me. But I was to see Luria soon—so what could I say to him? I
sat down to read the book once more and tried to pay special attention to
the behavioral experiments that Bernstein had carried out. There might be
something there that I had missed.



45. (a) Subject in black costume with white tape. (b) Cinematograph of walking. Movement is
from left to right. The frequency is about 20 exposures per sec. Reprinted with permission from
N. Bernstein, The Co-ordination and Regulation of Movements. © 1967 Pergamon Press Ltd.

Eureka!
In Bernstein’s experiments, he had dressed people in black leotards

and filmed them against black backgrounds, performing tasks such as
hammering a nail, writing on a blackboard, or riding a bicycle. He had
taped white stripes onto the arms and legs of the experimental subjects. In
later experiments, Bernstein’s students (and, as previously discussed,
Gunnar Johanssen in Sweden) placed white dots on the joints. After
developing the film, only waveforms made by the white tapes could be
seen. Bernstein had used a Fourier procedure to analyze these waveforms
and was able to predict each subsequent action accurately!!!

I was elated. This was clear evidence that the motor system used
essentially the same procedures as did the sensory systems! My seven
years’ hunt had ended. I gave my lecture on the brain’s motor systems the
next day and was able to face Luria with gratitude the following week.

The Particular Go of It
Next, of course, this idea had to be tested. If processing in the motor

cortex were of a Fourier nature, we should be able to find receptive fields
of cortical cells that respond to different frequencies of the motion of a
limb.

An Iranian graduate student in the Stanford engineering department,
Ahmad Sharifat, wanted to do brain research. He was invaluable in
upgrading and revamping the computer systems of the laboratory as well
as in conducting the planned experiment.

A cat’s foreleg was strapped to a movable lever so that it could be
passively moved up and down. We recorded the electrical activity of single
cells in the motor cortex of the cat and found many cells that responded
primarily, and selectively, to different frequencies of motion of the cat’s
limb. These motor cortex cells responded to frequencies much as did the
cells in the auditory and visual cortex. The exploration had been
successfully completed.



Fixed Action Patterns
My experimental program had resolved the issue of what the primate

motor cortex does. I had also been able to show how the doing, the action,
is accomplished. But, as yet, I had not created a precise formal description
of what actions do for us. The solution to this problem came from the
work of Rodolfo Llinás, professor of neurophysiology at the medical
school of New York University.

Sherrington had had the wisdom to note that his “reflex arc” was a
fiction, a wisdom not shared in current texts. One reason for Sherrington’s
insightful approach might have been a critique by the English neurologist
Graham Brown (no relation to Roger Brown). Brown observed that dogs
showed no difficulty in walking despite having their dorsal roots severed.
In today’s terms, their locomotion appeared to be “preprogrammed.”

Rodolfo Llinás, in his book I of the Vortex, expresses a view
complementary to mine regarding what the brain does. Llinás takes the
view expressed by Graham Brown, instead of that proposed by Sherrington
as his starting point. Brown had emphasized the fact that cutting the
sensory roots of peripheral nerves leaves an animal’s movements intact.
Llinás calls the units of behavior based on Brown’s insight “fixed action
patterns.” The virtue of such patterns is that they anticipate “the next step”
as they become actualized. As I walk, my next step is anticipated as I lift
my foot during the current step. This aspect of behavior cannot be
accounted for by Sherrington’s reflex arc but can readily be handled by the
change of metaphor to the controllable thermostat.

But fixed action patterns encounter their own set of difficulties: I
helped fund a series of experiments in which sensory roots were cut over a
much greater extent than in Graham Brown’s experiments. In those
experiments, though gross movement was still possible, it was seriously
impaired and learning a skill was absent. At a larger scale, fixed action
patterns by themselves do not account for our behavior.

A Generative Process
At these higher levels, Llinás brings in sensory processing. Recall

that, in order to account for our ability to perceive images and objects,
movements are necessary: that in vision, nystagmoid movements are
necessary to perceiving images; and that more encompassing movements



are necessary for us to perceive objects. The result of movement is to
make some aspects of the sensory input “hang together” against a
background, as in Johanssen’s grouping of dots. This hanging together, or
grouping of sensory inputs, is technically called “covariation.” Llinás
brings in covariation among sensory patterns to fill out his view.

Furthermore, Llinás also describes the encompassing organizations of
fixed action patterns, our movements, as “hanging together” much as do
sensory patterns. As described in Chapter 6, the hanging together of action
patterns is described as “contravariation.” The difference between
covariation in sensory patterns and contravariation in action patterns is
that contravariation implies the anticipatory nature of action patterns.

When covariation and contravariation are processed together, the
result generates our perception of objects (Chapter 6) and our
accomplishment of acts. Technically the “coming together” of covariation
and contravariation produces invariances. Objects are invariant across
different profiles—that is, across different images of the sensory input—
while acts are invariant across different movements that compose an act.

Finally, the research on cortical processing of visuo-motor acts
accomplished by Marc Jeannerod and comprehensively and astutely
reviewed in Ways of Seeing by him and Pierre Jacob, provides the
specificity required by the actions guided by their targets, their images of
achievement. These experiments deal with invariants developed by the
transactions between actions and the images upon which they intend to
operate.

Viewing acts as invariants composed across movements and their
intended achievements is the most encompassing step we have reached in
our understanding: brain processes that enable meaningful actions—we
have moved from reflex arc, through controllable thermostat, to what can
be called a generative organization of the brain/mind relationship.

This generative organization can be understood as being produced by
“motor inhibition” by analogy to the process of sensory inhibition. In
Chapter 5, I reviewed Békésy’s contribution on sensory inhibition that
showed how our perceptions are projected beyond ourselves. For the
generative motor process, we can invoke motor inhibition to internalize
(introject) our images of achievement, the targets of our actions. When,
for instance, we write with our pens in our notebooks or type on our
computer keyboards, we produce an achievement. This achievement must



be internally generated in some fashion, and motor inhibition can fulfill
this role. Massive motor inhibition is produced by the cellular interactions
among their fine-fiber connections in the cerebellar hemispheres:
Purkinge cells release GABA, which produces a feedforward inhibition.
Other cells (basket and stellate) can inhibit the Purkinje cells. Thus
multiple stages of inhibition can sculpt the target—form the achievement.

Imitation and Imagination
In our book Plans and the Structure of Behavior, we refer to several

different sorts of Plans. Some we call “tactics.” John Searle has called
these “intentions-in-action.” In primates, including humans, such tactics
are controlled by systems involving the “motor and sensory cortex” of the
pre- and post-central gyrus (discussed in Chapter 5). We also identified the
anterior, the pre-frontal cortex, as being involved in the initiation of
longer-range “strategies,” Sear-le’s “prior intentions.” Current brain-
imaging techniques are validating these earlier conclusions that were
based on evidence obtained from brain injuries and electrical recordings
and stimulations. Intentions are attractors, targets— “images of
achievement.”

For the past decade, scientists have been documenting the existence
of “mirror processing” in the brain. These experiments have shown that
many of the same brain cells and brain regions are active under several
related circumstances: when an action is undertaken; when that action is
imagined but not undertaken; and when the person or animal is watching
someone else perform the same act. These results are extremely important
for comprehending how we understand each other—for empathy and for
our ability to anticipate each other’s intentions.

Initially these experimental results were restricted to Broca’s area—
the part of the frontal lobe of the brain that was believed to be involved in
the expression of speech— and to a separate area in the inferior parietal
(back) part of the brain. But as my colleagues and I had demonstrated back
in the 1960s, by comparing the arrangement of thalamo-cortical
connections between carnivores and primates, these two parts of the brain,
front and back, are really one, and have been split apart in the primate
brain by the intrusion of the primary sensory-motor cortex that surrounds
the central fissure. More important, mirror processing has been shown to



occur in other parts of the frontal cortex, depending on what is being
mirrored or imagined—for instance, a skill or an emotional attitude.

As with most of today’s brain research, such important results fill an
aspect of the brain/behavior/experience gap by way of correlations. In
themselves, they do not provide a possible “mechanism” for how these
brain processes operate. The evidence (reviewed in the present chapter)
that leads to the conclusion that acts are based on environmental targets
does provide such an explanation. Intentions, imitations and imaginings
all share the attribute of projection; that is, they adapt our actions to
“realities” in the environment by actively selecting and incorporating
sensory input. The research that explored the basis for “images of
achievement” therefore goes a step further than the discovery of “mirror
neurons” in specifying a possible (probable?) process by which such
“mirrors” can be achieved.

When I first visited the Soviet Union, I imagined what living in a
communist country would be like. I found, much to my surprise, that the
Soviets actually dealt with “filthy lucre” (money) and even had savings
accounts! This and other, less dramatic surprises changed my trans-acting
the daily process of getting from one place to another, of obtaining
revenues from translations of my books, and of paying bills at restaurants,
incorporating the financial aspects of the Soviet system into my actions.
After a while, I could imagine and project in further transactions what my
hosts would be requiring. Note that this sequence is not a perception-
action sequence; it is an incorporation of perception to modify ongoing
action and an incorporation of action into an ongoing perception.

The attractor, the target, the achievement, in these situations is
imagined. In other situations the target is imitating another person as
babies do when playing peek- a-boo. The Fourier-like process, in the
richness of its processing and storage capacity, furnishes a viable (neuro-
nodal) medium within which the brain can proficiently organize the skills
that form our actions, imaginings and imitations.

Speaking
The discovery that our human brain’s motor cortex encodes

achievement—not movement per se—has signifi-cant import for our
understanding of language.



One theory of how our speech develops suggests that a baby’s
babbling gradually, through practice, allows the emergence of words and
sentences. This is called the “motor theory of language production.”
Speaking, when regarded as a skill, makes such a theory plausible.
However, linguists have expressed a considerable amount of skepticism,
based on their observations of the development of language in infants.

I vividly remember discussing this issue with Roman Jakobson, dean
of classical linguistics at Harvard, with whom I worked repeatedly in
venues as far apart as Moscow, the Center for Advanced Studies in Palo
Alto, and the Salk Institute in LaJolla, California. Jakobson pointed out to
me what is obvious: a child’s development of his ability to understand
language precedes by many months his ability to speak. Also, once the
ability to speak emerges, and a caretaker tries to mimic, in baby talk, the
child’s erroneous grammatical construction, the child will often try to
correct the adult. These observations indicate that the child has a much
better perceptual grasp than a motor performance ability in the use of
language.

The motor theory of language can be reinstated, however, when we
conceive of language as a “speech act,” as John Searle, the University of
California at Berkeley philosopher, has put it. According to my research
results reviewed in this chapter, the motor systems of the brain encode
“acts,” not movements. In this regard, speech acts become achievements,
targets that can be carried out with considerable latitude in the movements
involved. Perception of the target is the essence of the execution of an act,
including the act of speaking. Babies learn to target language before they
can comprehensibly speak it. The outdated “motor theory” of speech needs
to be reformulated as an “action theory” of speech.

Linguist Noam Chomsky and psychologist George Miller each have
called our attention to the fact that we can express (achieve) as speech acts
the same intended meaning in a variety of grammatically correct ways.
Chomsky has developed this observation as indicating that there is a deep,
as well as a surface, structure to language.

I extend Chomsky’s insight to suggest that the variety of languages
spoken by those who know several languages provides evidence for the
existence of an ultra-deep language process. This ultra-deep structure of
language processing occurring in the brain does not resemble the surface



expressions of language in any way. I will consider this view more fully in
Chapter 22.

In Summary
Our brain processes are not composed of input-output cycles. Rather,

they are composed of interpenetrating meshed parallel processes: the
motor systems of our brain work in much the same way as do our sensory
systems. Our sensory systems depend on movement to enable us to
organize the perception of images and objects. Conversely, our brain’s
motor systems depend upon imaging the intended achievement of our
actions.

Objects remain invariant, constant, across the variety of their
profiles—the variety of their images. Actions remain invariant across the
variety of movements that can produce them. Speech acts, intended
meanings, are invariant not only across a variety of expressions but also
across a variety of languages.



A World Within



Chapter 9
Pain and Pleasure

Wherein the rudiments of the “form within” that initiate pain and pleasure
are shown to derive from stem cells lining the central cavities of the brain.
Pain is shown to be homeostatically controlled. And a much-needed
physiological explanation for the generation of pleasure is developed.

We’re having a heat wave,
 A tropical heat wave,

 The temperature’s rising,
 It isn’t surprising

 She certainly can can-can.

—Marilyn Monroe singing “Heat Wave” in There’s No Business Like Show
Business, 1954. Lyrics by Edward Holland, Jr., Lamont Herbert Dozier and

Brian Holland.



So far, I have dealt with our direct relationship with the world we
navigate. Targets and intentions characterize this relationship, which is
executed by output to muscles and mediated by input from sensory
receptors. I have also noted that this relationship occurs within a context.
The context comes in two forms: the inner world of our bodies and the
social world of communication and culture.

The questions asked in this chapter deal with the inner world of our
bodies: What is the brain physiology of pain? And of persistent suffering?
What is the brain physiology of pleasure? There were few answers to these
questions when I began my research. In 1943, I asked Percival Bailey, the
classifier of brain tumors with whom I had received a fellowship, what I
might read. His brief answer was: “There is nothing.”

In the mid-1950s, Jerry Bruner invited me to teach a session in his
Harvard class. He started the class by asking me what my research goals
were. I answered that I would like to know the brain underpinnings of
emotion as clearly as (I thought I knew) those that organize our
perceptions. I had already begun the relevant research at Yale, but those
results had not become integrated into my (or the establishment’s)
understanding. For the most part the terms “feelings,” “emotions,” and
“motivations” were used interchangeably. “Pain” was identified with mild
electric shock that produced aversive behavior and “pleasure” with
morsels of food that evoked approach. The neurophysiology of pain was in
doubt, and no one even dared think about what the brain physiology of
pleasure might be.

I did pay heed to William James’s visceral theory of emotions despite
Walter Cannon’s and Karl Lashley’s critiques; I discussed the theory with
Wilder Penfield, who stated that he had occasionally produced heart-rate
changes when he electrically stimulated the region of the anterior insula of
the brain. I knew of von Economo’s and also of Papez’s proposals that the
hippocampal-cingulate cortex circuit was anatomically suited to deal with
the circularity and persistence of emotions. And there were the
experimental results of Paul Bucy and Heinrich Klüver who showed that
bilateral removal of the entire temporal lobe produced tame monkeys. But
during the mid-1940s, when my research began, none of this had been put
together in any orderly way—nor had we evidence to test the anatomical
proposals of von Economo and Papez, or to take the Klüver-Bucy findings
further.



The following chapters chart the voyage of discovery that I undertook
to clarify, for myself and for the scientific and lay community as a whole,
what the relationship between feelings, emotions, motivations, pain and
pleasure might be. The course was not always straightforward: there were
storms and calms. In order to recount these in any readable fashion, I start
with pain and pleasure.

The stories that have unfolded about how these discoveries were
made, and to which I have had the opportunity to contribute, are as
exciting as those that I read as a child. Paul DeKruif ’s Microbe Hunters
told about the discoveries made by scientists who were my father’s
contemporaries and whose work helped him classify bacteria and fungi.
Also the explorations of Admiral Byrd, whose book on Antarctica I read at
least a dozen times when I was ten years old, made me wonder what
comparable territory was left for me to explore: that territory turned out
not to be “out there,” but within our heads.

I was especially pleased, therefore, when Paul MacLean, who had
been my colleague at Yale, dedicated to me the translation into English he
had inspired, of a book written by Ramón y Cajal, one of the eminent
pioneers to describe the shape of brain cells. The dedication read: “To Karl
Pribram, Magellan of the Brain.”

The inner world within which targets and intentions are pursued is
referred to as “motivation.” Sometimes the term refers simply to the fact
that we are alive and eagerly navigating our world. A more important use
of the term “motivation,” which is developed in the next chapter, is akin to
its use in music where “motif” describes a pattern that forms the theme
that unites the entire enterprise.

In describing the brain organizations that form our world within, I
will follow the theme, the “motif” that permeates this book: I compare
contexts that are shaped with those that are patterned. Essentially, the
“world within” is “shaped” by its chemistry; brain processing provides its
“patterns.”

Shaping the World Within
Organic chemistry is a chemistry of shapes. Bonds, shaped by

valences (possible combinations), connect parts of molecules whose form
determines their function. Large molecules, such as proteins, can change



configuration with the displacement of an atom, such as hydrogen, to alter
the protein’s function.

As a prelude to tackling the brain processes that deal with pleasure
and pain, I will describe a most interesting proposal regarding shape in
“membranes,” the substrate on which the patterns of pain and pleasure
operate. As in the case of perception and action, we are faced with a
surface and a deep process: but for the world within it is the chemistry that
furnishes the deep process and the patterns dealing with feelings and
choice that form the surface structure.

46. Changes in the conformation of an amino acid under different contexts provided by H20.
Languages of the Brain, 1971

A Membrane-Shaping Process
Even water changes its properties by changing its form. A simple but

revolutionary example of the importance of shape in chemical processing
comes from a suggestion as to how water, when its H2O molecules
become aligned, becomes superconductive. My colleagues and I, as well
as others, such as I. Marshall in his 1989 article “Consciousness and Bose-
Einstein Condensates,” have proposed that the membranes of cells such as
those of the stem cells lining the cavities of the brain can organize the
configuration, the shape, of water. The membranes are made up of



phospholipids. The lipid, fatty part of the molecule is water repellant and
lies at the center of the membrane, giving it structure; the phosphorus part
forms the outer part of the membrane, both on the inside and outside, and
attracts water. The phospholipid molecules line up in parallel to compose
the linings of the membrane. Gordon Shepherd, professor of physiology at
Yale University, has described the water caught in the interstices of the
aligned phosphorus part of the membrane as being “caught in a swamp.”
My colleagues, anesthesiologist Mari Jibu, physicist Kunio Yasue, the
Canadian mathematician Scott Hagen and I ventured to suggest that the
water in the swamp becomes “ordered;” that is, the water molecules line
up much as they do in a meniscus at a surface. Ordered water forms a
“superliquid”: superliquids have superconducting properties. Such
properties are not constrained by the resistance produced by ordinary
materials.

In our work, we speculated that these membrane processes might
serve as substrates for learning and remembering. As such, they may do
this, in the first instance, by ordering the membranes involved in the most
basic components of learning and remembering: the processing of pain
and pleasure. These are the membranes of stem cells lining the central
cavities of the brain that actually provide a most effective substrate for
experiencing pain and pleasure.

47. Phospholipid membrane of dendrites (From Neurobiology by Gordon M. Sheperd)

Core-Brain Pattern Sensors
In describing the context within which we navigate our world, scale is

an important consideration. Overall, at a large scale, as long as we are



alive, as animals we strive to move. Movement has consequences. Some
consequences make us move more, some make us move less. Movement
itself uses energy, and one of the ways we can sense the depletion of
energy is that we feel hungry. When we are hungry, we move more; having
eaten, we move less.

We describe changes in the depletion and restoration of our energy in
terms of the body’s metabolism. Tracking metabolism requires sensors,
sensitive to various metabolic patterns. Some rather exciting experiences
introduced me to these sensors; they line the most interior parts of the
brain.

In the early 1940s, as his resident in neurosurgery, I was assisting
Paul Bucy and his mentor, Percival Bailey (who had worked for a decade
and a half with Harvey Cushing, the first neurosurgeon in America) at the
Neuro-psychiatric Institute of the University of Illinois at Chicago to
perform a delicate surgical procedure. Under local anesthesia, we had
opened the back of the our patient’s skull to gain access to the auditory
nerve with the purpose of curing her Ménière’s disease, a persistent
ringing of the ear accompanied by occasional vertigo (spinning dizziness).
Cutting into the auditory nerve reduces and often entirely abolishes the
intolerable symptoms. The brain stem (an extension of the spinal cord that
enters into the skull) and part of the cerebellum were exposed to view.
During surgery it is customary to keep the exposed brain wet by squirting
it with a dilute salt solution. The concentration of salt is the same as that
in the body fluids, including the cerebrospinal fluid that bathes the brain
and spinal cord.

On this occasion, every time we squirted the liquid onto the brain the
patient, who, was awake, would complain. The brain itself is insensitive;
only on the rare occasions when it becomes necessary to pull on blood
vessels does the patient feel uncomfortable. So we were surprised that the
patient had felt anything at all. We thought perhaps there was an aberrant
blood vessel in the way, but there wasn’t. Squirt. Again the patient
complained. Another squirt and the patient felt nauseated: we certainly did
not want her to retch, which would put pressure on the exposed brain and
might dislocate parts of it.

The squirt was clearly the cause of her discomfort. We asked our
scrub nurse whether she was certain that the liquid she had given us to
squirt was the correct one. She assured us that it was: she had personally



found doubly distilled pure sterile water for the surgery. “DISTILLED
WATER! How could you?” Unless the correct amount of salt is in the
solution, distilled water is an irritant to delicate tissue such as the stem-
cell membranes lining the ventricles. Recall that these membranes are
dependent for their function on the ordering of their “water,” which is
imbedded in the appropriate saline solution.

The offending distilled water was quickly replaced, and the surgery
proceeded without any further hitch and with excellent postoperative
results.

Bucy, Bailey and I sat down after the operation and discussed what
had happened. The brain is insensitive; the water might have caused a bit
of local swelling if we had persisted in bathing the brain with it but only if
we had removed a fine layer of covering tissue, the pia. Inadvertently, we
had discovered that the inside lining of the brain is sensitive. So often in
science discoveries are made serendipitously, by chance. Actually, I have
always felt that the real purpose of an experimental laboratory is not so
much to “test conjectures or hypotheses” but to furnish the opportunity to
make unexpected and unintended observations.



48.

Bailey recalled that he had recently seen a patient who had been shot
in the head. The bullet was lodged in one of his ventricles (“little
stomachs”) filled with cerebrospinal fluid. The patient’s complaint:
Sometimes he would wake in the morning in a foul mood. At other times
he would awaken cheerful. He had learned that he could change his mood
by positioning his head: If when he first arose, he hung his head face down
over the side of the bed, he would become cheerful in a few minutes; when
he tilted his head back, his black mood would return. Sure enough, X rays
showed that by tilting his head forward or backward, the patient could
change the location of the bullet, which would “drift” inside the ventricle.

The question was whether or not to try to remove the bullet. The
answer was no, because the surgery would require cutting into the



ventricle to get the bullet out. Even minute temporary bleeding into the
ventricle is extremely dangerous, usually followed by a long period of
unconsciousness and often death. Bleeding elsewhere in the brain, once
stopped, causes little if any injury. The brain tissue repairs itself much as
would such an injury in the skin.

Bailey remarked that the symptoms shown by these two patients
indicated that our ventricles, which are lined with stem cells with their
undifferentiated membranes, could well be sensitive. The nervous system
of the embryo develops from the same layer of cells as does our skin, the
surface part of the layer folding in to become the innermost layer of a
tube. It should not, therefore, have surprised us that this innermost part of
the brain might have sensitivities similar to those of the skin.

With the patient’s permission, we tested this idea in a subsequent
surgical procedure in which one of the ventricles of the brain lay open and
therefore did not have to be surgically entered. When we put slight local
pressure on the exposed ventricle, the patient felt it as an ache in the back
of his head; when we squirted small amounts of liquid, a little warmer or
colder than the normal body temperature, onto the lining of the ventricle,
the patient also experienced a headache. (Pressure and temperature are the
two basic sensitivities of our skin.) We then proceeded with the intended
surgical procedure, cutting the pain tract in the brain stem—a cut that the
patient momentarily experienced as a sharp pain in the side of his head—
with excellent postoperative results. (Actually the patient’s head jumped
up when he experienced the pain, a jump that fortunately made Bucy cut in
just the right place. But we decided that next time the actual cut would be
done under general anesthesia.)

Percival Bailey had classified brain tumors on the basis of their
development from this innermost layer of cells lining the ventricles, the
“ependyma,” stem cells that have the potential of forming all sorts of
tissue. Bailey had spent a whole year with me and another resident,
peering down a microscope that was outfitted with side-extension tubes so
we could simultaneously see what he was seeing. He would tell us
fascinating stories about his work in Spain studying with Hortega del Rio
and how Ramón y Cajal, Hortega’s chief, was annoyed with Hortega for
studying stem cells instead of nerve cells. In Chapter 2 of my 1971 book
Languages of the Brain, and in Chapter 9 of this book, I summarized what



I had learned—bringing to bear what Bailey had taught us on how our
brain can be modified to store experiences as memories.

49.

The Core-Brain
At this juncture, a brief exploration of functional brain anatomy will

help. The names of the divisions of the nervous system were established
over the years prior to World War II. During these years, the nervous
system was studied by dividing it horizontally, as one might study a
skyscraper according to the floor where the inhabitants work. The
resulting scheme carried forward the basic segmental “earthworm” plan of
the rest of the body (in many instances, in my opinion, inappropriately as
the cranial nerves are not organized according to the segmental pattern of
the rest of the body).



The tube we call the spinal cord enters our skull through the foramen
magnum (Latin for the “big hole.”) Within the skull, the brain stem
becomes the hindbrain. As we move forward, the hindbrain becomes the
midbrain. Still further forward (which is “up” in upright humans) is the
forebrain. The forebrain has a part called the thalamus (Latin for
“chamber”), which is the way station of the spinal cord tracts to the brain
cortex (Latin, “bark,” as on trees.) The most forward part of the brain stem
is made up of the basal ganglia. During the development of the embryo,
cells from the basal ganglia migrate away from the brainstem to form the
brain cortex.

A New View of the Brain
After World War II, techniques became available to study the nervous

system from inside out. Most skyscrapers have a core that carries pipes for
water and heat, a set of elevators, and an outer shell where people work. At
UCLA Horace (Tid) Magoun and Don Lindsley studied the midbrain by
electrically destroying its core. In Montreal, Wilder Penfield and Herbert
Jasper studied the core of the thalamus by electrically stimulating it.
Concurrently, at Yale, in John Fulton’s laboratory, using neurosurgical
techniques, I was able to reach the inner aspects of the cortex, exploring
them by electrical and chemical stimulation, and by making surgical
removals.

50. The segmental pattern of the human body



51. The segmental pattern of the spinal cord and peripheral nerves

These new methods, pursued on three fronts, produced new insights
into what we came to call the core-brain. From the midbrain forward there
is actually a continuous band of cells that had earlier been divided into
separate structures. In the midbrain, the gray matter (brain cells) around
the central canal connecting the ventricles, is called the “periaqueductal
gray” because, at that point, the canal inside the brain stem forms an
aqueduct that leads from the brain stem ventricles (that we found to be so
sensitive) to the ventricles of the forebrain (where the bullet had lodged in
Bailey’s patient.) Shortly, we’ll see how important this gray matter is to
our understanding of the way we process pain. The periaqueductal gray
extends seamlessly forward into the hypothalamic region (which is the
under part of the thalamus, its upper part being called the dorsal
thalamus.) The gray matter of the hypothalamic region, in turn, seamlessly
extends forward into the septal region where the brain’s thermostat is
housed.

The sensors that control the variety of behaviors that regulate
metabolism are located in this band of cells bordering the ventricles. The
gray matter that makes up the band regulates and makes possible the
modification of a variety of behaviors such as drinking, eating, motor and
sexual activity.

Control is homeostatic; that is, the behavior is turned on and turned
off according to the sensitivity of the sensors. This sensitivity is set
genetically but can be modified to some extent by our experience.



52. (From Brain and Perception, 1991)

53. The median forebrain bundle (afferent and efferent) highlighting the concentric pattern of the
brain. 52. (From Brain and Perception, 1991)



The thermostats that control the temperature of our buildings work in
this fashion. A sensor in the thermostat usually consists of two adjacent
metal strips or wires that expand and touch, closing an electrical circuit as
the room temperature increases. When the temperature in the room drops,
the metal shrinks, opening the electrical circuit. Opening and closing the
circuit can turn on an air-conditioning unit or a furnace. The homeostats in
the core-brain turn on, and turn off, behaviors such as drinking and eating,
which are controlled by their respective sensors. Roughly, drinking is
controlled by the amount of salt in the blood and eating is controlled by
the amount of sugar in the blood. More on these topics shortly.

Ordinarily the loci of control over these behaviors are attributed to
processing “centers.” There is separation between these loci of control, but
within the controlling region there is considerable distribution of function.
Here are some examples:

The cells located around the central canal in the hindbrain control
breathing, heart rate and blood pressure. When I arrived at Yale one of the
graduate students wanted to know whether the “respiratory center” on one
side of the hindbrain controlled breathing on the same side or the opposite
side of the body. Under general anesthesia, he made a cut into the
“respiratory center” of a rat’s brain, but nothing happened to the animal’s
breathing. Perhaps the cut was inadequate. A week later he made another
cut; deeper. Again nothing happened. After a week, yet another cut, and
still no change in breathing. He then tried cutting into the breathing
“center “ on the other side of the brain stem. By this time he was surprised
that the animal had survived so much surgery; his cuts led a zigzag path
through the length of the hind-brain. Conclusion: control over breathing is
distributed over a region among cells that do not form a “center.”

By using a variety of chemical tagging and electrical stimulation
techniques, brain scientists have found that such distribution of function is
characteristic of the entire band of gray matter surrounding the ventricles.
In the hypothalamic region of rats, stimulation of one point would result in
wiggling of its tongue. Stimulation of an adjacent point led to swallowing,
the next adjacent point to penile erection, and the next to swallowing
again. The cells in the hypothalamic region that regulate the amount we
eat are strung out almost to the midbrain, and those that are involved in
regulating our basal temperature extend forward into the septal region. To
talk of “centers” in the brain within which all cells are engaged in the



same process is therefore misleading. Always there are patches of cells
and their branches that are primarily devoted to one function intermingled
to some extent with those devoted to another function. Thus, there are
systems devoted to vision within which there are cells that are tuned to
parts of the auditory spectrum. With respect to the regulation of internal
body functions, processes such as chewing and swallowing often
intermingle with cells that regulate ovulation and erections. Such an
arrangement has the virtue of providing flexibility when new coalitions
among functions are called for.

Odd Bedfellows
We return now to the sensitivities shared by the tissue of the spinal

canal and ventricles with those of the skin. The sensitivities of the skin can
be classified into two sorts: touch and pressure make up one category,
while pain and temperature make up the other. These two categories of
sensitivities are clearly separated in our spinal cord. The nerves that
convey the sensations of touch and pressure run up to our brain through
the back part of the cord; those that convey pain and temperature run
through the side of the cord. This arrangement allows surgeons to cut into
the side of the cord in those patients who are experiencing intractable pain,
in order to sever the patients’ “pain and temperature” fibers without
disturbing their sensation of touch. There is no way to isolate pain from
temperature fibers, however. Thus, cutting into the spinal cord and
severing the pain/ temperature nerves eliminates both pain and
temperature sensibility. As noted above, the human body, its vertebrae and
nervous system, is formed in segments, like those of an earthworm, thus
sensibility is lost only in the segments below the cut but not above it. This
segmentation becomes manifest when shingles develop: the itch and pain
are usually restricted to one body segment.



54. Example of core-brain receptor sites in rat brain: striped areas indicate uptake of labelled
estrogen (female sex hormone) molecules. Lateral structure showing uptake is amygdala. (From

Stumpf, 1970)

Pain and temperature—what odd bedfellows! The story of how I
came to understand how these fellows became bedded together follows in
the remainder of this chapter. To begin, brain scientists were puzzled for a
long time as to how we sense pain—because we possess no receptors that
are specific to sensing pain. For touch, pressure, cold and hot there are
specialized nerve endings that act as receptors—but even for these
sensations the specialized receptors are unnecessary: we can feel these
categories of sensation in parts of the body where such specialized
receptors are absent. John Paul Nafe, an eminent professor of physiology
working at Florida State University during the mid-20th century,
suggested, on good evidence, that the nerve endings in the skin could be
stimulated in two ways: the first is produced by depressing the skin, hence
stretching the skin’s nerve net laterally, which gives rise to the sense of
touch or pressure, depending on the magnitude of the stretch. A second
gradient from depth to surface—established by contraction and dilation of
the skin’s blood vessels—provides the input for sensing temperature. Nafe



proposed that when the depth-to-surface gradient is activated from
outward to inward, as when radiation comes from a heat source such as the
sun, we feel warm; conversely, as we radiate out to our environment,
activating the gradient from inward to outward, we feel cool.

But at extremes the distinction between sensing pain and sensing
temperature disappears: with exposure to extreme hot or cold
environmental situations, feelings of pain and even pleasure merge.

For example: I had an experience shared by many Arctic and
Antarctic explorers. I was trekking with my companion to my cottage in
deep drifts of snow, having had to park my car about a mile away from
home. I get asthma in cold weather, and I was wheezing away. I soon felt
tired and decided to rest for a moment, so I lay down in the snow. What a
relief! I soon felt nice and warm and cozy despite an external temperature
well below zero, and would have dropped off to sleep if my companion
had not noticed that I had stopped. She kicked me and got me up and
going.

At the extremes, our experience of warmth and cold become
indistinguishable. Possibly this effect is related to the control of pain by
our endorphins (endogenous morphine-like substances), the same
chemicals that account for “second wind” during extended exertion. I’ll
have more to say on this topic shortly.

The Search for “Pain in the Brain”
Where, within the brain, do the spinal cord nerve tracts that transmit

our sensations of pain and temperature end up? One tract ends in the
parietal lobe of the brain, the same place where the nerves that transmit
our sensations of touch and pressure end up. Knowing this, in a few cases
surgeons removed the cortex of the parietal lobes in an attempt to rid
patients of intractable pain. This failed to work. On the other hand, when
the most forward parts of the frontal lobes were removed or cut into, as in
the lobotomy procedure, the intrusive persistence of a patient’s pain, the
suffering, always disappeared.

During the late 1940s, I came to the conclusion that persistent
sensations of pain/temperature must reach the frontal lobes by way of a
tract separate from the one that reached the parietal lobe. I therefore
undertook a series of studies (the last of which was published in 1976) to



discover, not only the pathways by which pain sensations reached the
frontal cortex of our brain, but how that cortex controlled our experience
of pain. An immediate difficulty to be faced was the fact that I did not use
painful stimulation in my animal experiments. I resolved this by assuming
that, if I studied temperature, pain’s odd bedfellow, I would be a long way
toward unlocking the secret of frontal lobe control of pain.

Pain is not a single sensation, even at the skin. There is “fast” pain
and “slow” pain. Fast pain is discrete as when we feel a pinprick. We can
tell where we were pricked and when the prick occurred. By contrast, slow
pain is hard to locate in either place or time. It takes us longer to feel the
slow pain, thus the name “slow.” The difference between fast and slow
pain is due to a difference in the size of the nerve fibers that transmit the
sensation from the skin to the spinal cord. The nerve fibers that transmit
fast pain are large; the nerve fibers that transmit slow pain are very small.
Interestingly, the small nerve fibers also mediate temperature sensations,
fibers that become even more intertwined with pain fibers in the spinal
cord.

The difference between the two types of pain is classically
demonstrated in patients who have a form of syphilis called tabes dorsalis,
which can become manifest long after the initial infection. These patients
have lost their fast pain but retained their slow pain because their large
nerve fibers have been damaged where they enter the spinal cord.

The distinction between fast and slow pain is maintained in the brain
stem and the brain: the fast pain tracts end in the parietal lobe of the brain,
which left unsettled the issue as to whether slow pain reaches the frontal
lobe. A clue as to where these slow pain-related systems might be located
is that in the brain stem, surrounding the connection between the
ventricles, lies a system (the periaqueductal gray) that is sensitive to
morphine-like substances, the endorphins, which suppress the experience
of pain.

My research therefore had to concentrate on finding evidence that
forebrain systems might be involved in the experiencing of pain—and to
use the relation between pain and temperature as the tool for such a search.

I initially trained monkeys that had been used in a variety of other
studies to home in on the parts of the brain that might be involved in
sensing temperature. I taught the monkeys to choose the colder of two test
tubes to receive a peanut. I placed the test tubes either in buckets of ice or



of hot water, randomly varying the cold and the hot between trials. I found
that monkeys with damage to the bottom of the frontal lobe and the medial
surface of the temporal lobe were impaired in making the choice.
However, the failure to choose was not equally severe in all the monkeys
and making the choice took a variable period of time.

With the help of a postdoctoral student and others with technical
expertise in the laboratory—in the acknowledgments in the publication we
noted that since the experiment had taken over five years to complete, the
whole laboratory had had a hand in its execution at one time or another—
we set out to repeat my earlier experiments with better control over our
stimuli: a metal plate was cooled or heated between trials in a random
order. The monkeys were trained to press one or the other of two panels:
one if the plate felt cold, the other if the plate felt warm. As a control, an
identical visual task was set up in which the monkeys were shown a “+” or
a “#” and had to press one panel when they saw a + and the other panel
when they saw the #. We then placed electrodes on the surface of the
parietal lobe and on the bottom of the frontal cortex and the medial part of
the temporal lobe—the areas that had given promise of being involved in
the earlier study.

The experiment gave excellent results: when we disrupted brain
function by stimulating the frontal and temporal cortex and the relevant
pathways to these parts of the brain, the monkeys completely failed to
make the temperature choice but performed perfectly on the visual one.

They again performed well on the temperature choice when the
stimulation was turned off. Nor did stimulation of the parietal lobe have
an effect on the performance of the monkeys, a result that supported the
findings of other studies that had shown that only very fine differences in
temperature—not gross differences as used in our study were affected by
surgical resection of the parietal cortex of monkeys. We concluded that
perhaps slow pain, as indicated by its odd bedfellow temperature, reached
the frontal cortex and not the parietal cortex of the brain.

While I was attaining some insight into the brain systems entailed in
controlling the persistence of intractable “slow pain”—the pain that is
alleviated by frontal lobe surgery—postdoctoral fellows in Don Hebb’s
department at McGill University were discovering what constitutes the
essential brain process involved in our experience of pleasure.



Pleasure: Self-Stimulating the Brain
In the 1950s, Peter Milner and James Olds, working in Don Hebb’s

laboratory at McGill in Montreal, Canada, stumbled on an exciting
experimental result. They wanted to study whether they could influence
the rate of learning by electrical stimulation of a part of the brain stem.
The rat was to learn to push a hinged platform placed on the floor of its
cage whenever the brain stimulus was turned on. Within a few minutes the
experimenters were surprised to see that the rat was pushing the platform
without the experimenters turning on the electrical stimulation to its brain.
A short circuit had occurred that hooked up pushing the platform with the
electrical stimulation. As Milner and Olds were searching for the short
circuit, they noticed that the rat was pushing the platform more and more
often. Serendipitously, they had discovered that the rat would push a
platform in order to electrically stimulate its brain. The brain stimulation
was “rewarding,” and perhaps felt “pleasurable,” to the animal.

When Milner and Olds looked at the brain of the rat to see where they
had implanted their electrode, they found that it was much farther forward
than they had planned. Instead of being in the brain stem, the electrode had
been placed in a tract that courses along the sides of the centrally placed
ventricle of the brain. A few more experiments showed this tract and the
surrounding tissue to be the locations from which electrical self-
stimulation of the brain could be obtained.

Some months after Milner and Olds made their discovery, Jim Olds,
his wife, and I were having lunch together in Montreal. I took the
opportunity to suggest that it would be important to map all of the brain
sites from where self-stimulation of the brain could be obtained. I was
eager to know whether these sites might include sites in the olfactory (by
now called the limbic) system of the brain. Jim, who had obtained his
degree in sociology from Harvard and was delving into brain science for
the first time, said that he was not up to such a tedious task. His wife
Nicki, however, shared my view that the mapping would be important. She
had a doctorate in philosophy and in the history of science and she was
eager to get into the laboratory. With Jim’s and my encouragement, she
volunteered to do the experiments.



55. Places in a rat’s brain where self-stimulation is elicited. (From Brain and Perception, 1991)

The results of her mapping showed that sites of self-stimulation were
located not only in the tract adjacent to the core ventricle gray matter but
also in what is the rat’s olfactory brain. Further, the amount of effort the
rat would expend (how fast and for how long the rat would push the panel)
on self-stimulating was a function of the three systems that I had recently
outlined in a 1954 paper, “Functions of the ‘Olfactory’ Brain.”

The discovery of self-stimulation by Olds and Milner was celebrated
as locating the “pleasure centers” in the brain. When a psychiatrist in New
Orleans electrically stimulated these brain sites in humans, the patient
stated that he or she was experiencing a pleasant feeling. One patient
repeatedly professed deep love for the psychiatrist who had turned on the
stimulus—but the feeling was present only during the stimulation!

By this time I had arrived at Yale, in John Fulton’s Department of
Physiology, where another set of experiments was under way in which an



experimenter turned on —and left on—an electrical stimulus in the same
brain locations as in the Olds and Milner experiments. The rats would turn
off this ongoing electrical stimulus by pushing a panel, but the electrical
stimulation would turn itself on again after a brief pause. In this
experiment, the rat would dutifully keep turning off the stimulation. This
was the mirror image of the turn-on self-stimulation effect. The location
of these turn-off effects was the same as those that produced the turn-on
effect.

I suggested to our graduate student who was doing the experiment
that the stimulation be adjusted so that, in the turn-on experiment, the
stimulus stays on after being turned on, to see if the rat would turn it off. It
did. And when the electrical stimulation remained off, the rat turned it on
again. Jokingly we called the graduate student’s thesis “Sex in the Brain!”

On the basis of this result, we can interpret the self-stimulation
experiments as “biasing,” that is, modifying the context within which the
naturally occurring pleasure is processed. Nicki Olds showed that this is
an excellent explanation as she explored the interaction between self-
stimulation with food and water intake and sexual behavior. She showed
that “pleasure” is ordinarily self-limiting, although, under certain
conditions, it can become “addictive.” Jim Olds pointed out that the
danger of potential addiction is why there are so many social taboos that
aim to constrain the pursuit of pleasure. (I’ll have more to say about the
conditions that lead to addiction in Chapter 19.)

Ordinarily, pleasure has an appetitive phase that ends in satiety. The
finding of how the brain process works to produce an appetitive and a
satiety phase has important clinical implications. Pleasure is self-limiting
unless you trick the process, as in bulimia, by short-circuiting it: the
bulimic person artificially empties his stomach so that the signals that
usually signify satiety, such as a full stomach and absorption of fats, are
disrupted. Is the bulimic’s short-circuiting process a parallel to that which
occurs in self-stimulation of the brain? Or, as in anorexia, can culture play
the role of “keeping the stimulation on” in the role of a person’s self image
so that the person’s brain is always set to experience only a “turn-off”
mode. We need to find noninvasive ways by which we can change the
settings in the brains of the persons with these eating disorders.

Evidence that further associates pleasure with pain came from
experiments that showed pain to be a process in which the experience of



pleasure can be the antecedent of the experience of pain. The experimental
findings that led to this conclusion are taken up shortly.

To summarize what we have covered so far in this chapter: Our world
within is formed by stimulation of a variety of sensors lining, and adjacent
to, the central canal of the nervous system, a canal in which the
cerebrospinal fluid circulates. In warm-blooded animals, including
humans, the sensors are linked to one another through the regulation of
behaviors such as drinking, eating and motor activity that regulate our
basal temperature: thus the explanation for the odd pairing of pain and
temperature in our spinal cord and brain is that the regulation of
temperature is the basis of “how” we come to experience pleasure.

The following are examples of what to me were exciting discoveries
of the sensitivities and their regulation of pleasure and pain by these
“core-brain” sensors.

Brain and the Regulation of Thirst
I remember that while I was in college, I asked my father what makes

us thirsty, other than dryness of the mouth and tongue? How do we know
immediately just how much to drink, how many swallows to take to
replenish our thirst—for instance, after a tennis match?

The discovery of how the brain regulates thirst is one of those
delightful sagas that occasionally intrude among the more disciplined
scientific enterprises. There is a disorder known as diabetes insipidus
during which the person urinates huge amounts—and, as a consequence, is
continually thirsty. This disorder is caused by an imbalance in the
secretion of a pituitary hormone. The question arises: What ordinarily
controls the secretion of this hormone?

In the late 1880s, my father had done his medical school thesis on the
control of the pituitary gland by cells in the hypothalamic region. When I
first found this out, I was surprised that the hypothalamic control over the
pituitary secretions was already known. But the specific manner of how
this control operated was not known even in the mid-1940s. After World
War II, Pier Anderson and his colleagues at the Technical University of
Stockholm in Sweden devised a simple experiment that revealed the
“how” of the hypothalamic control of the pituitary gland in the production
of thirst. Using a goat, they placed a small tube into the ventricle just



above the pituitary gland. In this location, a network of blood vessels
connects the hypothalamic region with the pituitary gland. Anderson took
a pinch of table salt and poured it down the tube. The goat rushed to a
nearby fountain and drank and drank and drank and would have burst its
belly had not Anderson been prepared to put another tube into its stomach.
Further research showed that the amount of drinking was directly
proportional to the concentration of salt in the ventricle.

I had to see this for myself, so on a visit to Stockholm I made an
appointment with Anderson. There was the fountain in the center of a
square surrounded by stables. I made friends with one of the goats who
had a tube in his tummy. A minute amount of salt was inserted into
another small tube in the goat’s head. The goat and I trotted off together to
the fountain and the goat drank and drank, with the water pouring out of
the tube in his stomach. Many carefully done experiments showed the
“how” of the thirst process and how the exact quantity of water drunk is
determined by the proportion of salt in the sensors lining the ventricle
surrounding the tissue above the pituitary. Here was a simple
demonstration of the sensitivity and the power of control that the core-
brain sensors exert over our behavior—and, I believe, over our feelings, if
my reading of the goat’s urgency in getting to the fountain is correct.

Masochism: A “Thirst” for Pain
Another major event in my understanding of brain function came

with the discovery of the endorphins. Surprise: pain turned out to be
processed in much the same way as pleasure. As I previously noted, we
have no specific receptors for pain. But pain can be produced by excessive
stimulation, of whatever sort, even by a very bright light or by an overly
loud sound. This is true also of excessive stimulation of the tracts in our
spinal cord and brain stem that relay signals from our body and face. Pain
is coordinate with excessive stimulation when it overwhelms our
ordinarily ordered sensory processes.

No one has found a “center” for pain in the brain. However, during
the 1970s, experiments with rats showed that when the cells of the
periaqueductal gray matter are electrically stimulated, pain produced by a
peripheral stimulus, such as pinching the skin, is turned off. At about the
same time, several laboratories reported that morphine acts selectively on



the sensors in the aqueduct adjacent to the periaqueductal gray. Next,
experimenters showed that a hormone called endorphin, whose effects
were almost identical to those of morphine, is secreted by our pituitary
gland and that the amount of secretion is sensed and regulated by cells in
the hypothalamic region and in the periaqueductal gray. These endorphins
account for protection against the pain produced by pinching the skin, as
in the experiments during electrical stimulation of the periaqueductal gray.

Further research demonstrated that the stimulation and the
endorphins influenced the part of the spinal cord where sensory input from
the pinching becomes organized. The theory, and its confirmation that
predicted this result, is known as the “gating theory,” popularized by
Patrick Wall, professor at MIT and later at the University of London, and
by Ronald Melzack, professor at McGill University in Montreal. The
theory states that our ordinary sensations are patterned and that, when
excessive stimulation overrides the patterned sensory input, pain is
experienced.

The results of this research demonstrate that pain is controlled
homeostatically, exactly as thirst, hunger and temperature are controlled!
(In fact, excitations of the brain that result in the secretion of endorphins
also produce goose bumps and the sensations of chills and thrills.) When
we are tired, our endorphins are low and small injuries can seem to hurt a
lot. When we get our second wind during running, our endorphins kick in.
During the 1960s and 70s, I told my students that it is stupid to buy drugs
on the street when you can generate your own homemade supply by
exercising.

An incident highlights the novelty of these discoveries about
endorphins and the homeostatic regulation of pain. Science can provide
answers to puzzles that often stump philosophers. This may hardly seem to
be news to most of us—but needs to be mentioned because a number of
philosophers and even some scientists have stated that the mind/matter
relationship is one that only philosophers can resolve—that no matter how
many experiments we do, such experiments, or the theories we base on
them, will be irrelevant to the issue of the relationship between mind and
brain. On one occasion, after my colleague Sir John Eccles had given a
splendid talk, a noted philosopher got up and made remarks to this effect.
Eccles and I were both furious. It was lunchtime, and Eccles took me by



the hand as we marched out and said, “Karl, we simply must do our own
philosophy.” Which we then did.

Shortly after the discovery of endorphins, I attended a meeting on the
topic of philosophy in medicine at the medical school of the University of
Connecticut. One philosopher delivered a very long paper in which he
pointed out how unsolvable the topic of pain is: “If one defines pain
behaviorally, in terms of escape or avoidance, what about masochism?”
For over an hour he harped on that same theme over and over. I was the
discussant of the paper and based my critique on the recent discoveries of
the endorphins: One of my students had just completed a study involving
sado-masochism, demonstrating that, at least in her sample groups, there
was never any real pain experienced. Rather the experience was an almost-
pain, more like an itch. The participants in her groups were so well attuned
to one another that they never went beyond that threshold to a stimulation
that produced actual pain. True, this is not the stereotype one gets from
some of the stories written by the Marquis de Sade. However, other, more
contemporary tales, as for instance Story of O, do resemble the careful
analysis that my student came up with.

When I start to scratch a mosquito bite, histamine has already been
secreted at my local nerve endings at the site of the bite. The histamine
affects the local small blood vessels to produce an itch, almost like a
slight burning sensation. I scratch until it hurts, then I stop scratching. I’m
thirsty: I drink until I feel full. I eat until I feel sated. The parallel between
the appetitive-consummatory cycle in homeostatically controlled
experiences and those operating in our experiencing of pain is evident.
Scientific experiment, leavened by philosophical reasoning, provided an
answer that philosophy alone could never have attained.

The world within is governed by homeostatic patterns. These patterns
are not static but change with circumstance; thus, Waddington coined the
term “homeorhetic,” which has found favor. Whether homeo-static or
homeorhetic, the essence of the patterns is a regular recurrence of an
appetitive-satiety, a “go” and “stop,” cycle. Both pleasure and pain partake
of this pattern.

Much excitement, which I shared, was generated during the 1950s as
these insights about homeostasis and homeorhesis were gained. As I
described in Chapter 5, the principles that were initially formed with
regard to the world within were shown to be equally valid for sensory



processing. All senses were shown to be controlled by feedback from the
brain—with the exception of primate vision, which I set my laboratory to
discover over the next decades. As noted earlier, a result of our excitement
was that George Miller, Eugene Galanter and I wrote a book published in
1960, Plans and the Structure of Behavior, which related our insights to
experimental psychology and to how computer programs are organized.

Go- and stop-homeorhetic cycles are regulated by higher-order
modulations, anticipated in Plans and the topic of the next chapter.

In Summary
1. The brain is lined with a layer of stem cells that is sensitive to the

same stimuli as is the skin from which they are derived in the
embryo: pressure and temperature—and, in addition, to changes in
the salinity of the cerebrospinal fluid bathing them.

2. Surrounding the stem-cell layer are control systems of brain cells
that, when electrically excited, turn on and turn off behavior. These
systems have been interpreted to provide comfort (pleasure) and
discomfort (pain) to the organism so stimulated, and this
interpretation has been confirmed by verbal reports from humans
stimulated in this fashion.

3. A stimulus is felt as painful when any pattern of stimulation is
overwhelming and disorganized.

4. Endogenous chemicals, endorphins, act as protectors against pain.
When activated, these chemicals produce thrills and chills.

5. A stimulus is felt as pleasurable when (in warm-blooded animals) it
activates the metabolic processes anchored in the regulation of
temperature. Finding temperature regulation to be the root of pleasure
accounts for the previously un- understood intimate intermingling of
the pain and temperature tracts in the spinal cord.



Chapter 10
The Frontolimbic Forebrain: Initial Forays

Wherein I describe my initial discoveries that defined the limbic systems
of the brain and the relation of the prefrontal cortex to those systems.

Some strange commotion
 Is in his brain: he bites his lip and starts:

 Stops on a sudden, looks upon the ground,
 Then lays his finger on his temple; straight,

 Springs out into fast gait; then, stops again,
 Strikes his breast hard; and anon, he casts

 His eye against the moon: in most strange postures
 We have seen him set himself.

—Henry VIII, Act III, Scene 2.
 Quoted by Charles Darwin, The Expression of the Emotions in Man and

Animals, 1872



The Yale Years
1948 was a critical year for me. I took and passed the exams that

certified me as a member of the American Board of Neurological Surgery
and was appointed assistant professor in the Departments of Physiology
and Psychology at Yale University. I thus achieved my long-sought
entrance into Academe through the offices of John Fulton, the head of the
Department of Physiology.

My relationship with Fulton began during the late 1930s while I was a
student at the University of Chicago. The first edition of Fulton’s classic
text on the Physiology of the Nervous System was published and made
available the weekend before my final exam in a course in physiology
given by Ralph Gerard. I was mesmerized by Fulton’s text: it laid out what
was then known, so clearly and thoroughly, that I could not put the book
down. On Monday, Gerard posed a single question for the final: “Discuss
the organization of the nervous system.” I filled blue book after blue book,
basing my answer on Fulton’s text, until the time allotted was up. Gerard
told me that it was the best answer he had ever received on any
examination. Fortunately, he had not yet seen the Fulton text.

My next encounter with Fulton was in 1943, while I was an intern-
resident in neurosurgery with Paul Bucy. We often discussed the scientific
aspects of the surgical operations we were engaged in, and Bucy told me
that if I ever wanted to do research, Fulton’s department at Yale would be
the place to go.

Therefore, while in practice and working in Karl Lashley’s laboratory
in Jacksonville, Florida, I expressed my desire to get to Yale (which owned
the laboratory). Lashley wrote letters of recommendation to the dean—and
received no answer. (Lashley at the same time was trying to get Nico
Tinbergen, who later received the Nobel Prize, placed at Harvard, with
similar negative results.) And I was asking Bucy and Percival Bailey at the
Illinois Institute of Neurology and Psychiatry to write letters of
recommendation. Bailey’s reply was succinct as was his wont: “Karl, if
you are thinking of obtaining a job like mine, your chances are slim as I
intend to hang onto the one I have.” (He changed his mind a decade or so
later when Warren McCulloch moved permanently to MIT, and I was
offered his research position at the Illinois Institute. But by that time my
laboratory was already well established at Stanford.)



In 1948, I took the opportunity to visit Fulton personally. Fred
Mettler at Columbia University had invited me to New York to discuss his
frontal lobe topectomy project. I had had a ward of patients assigned to me
upon whom to perform frontal lobotomies. As a well-trained surgeon, I
held to the edict: “Primum non nocere”—“First do no harm.” I was
administering behavioral tests to the patients, and they were improving,
given the personal attention involved. (More on this in Chapter 16.) I was
therefore interested in Mettler’s project, and he was graciously inviting me
to see what was going on.

I decided that while in New York I would visit Fulton in nearby New
Haven and had made an appointment to meet him in the Yale Medical
School Library. I drove up to Yale, approached the hallowed halls and felt
like genuflecting as I entered the rotunda leading to the library. Fulton
greeted me warmly, and we talked about his experiments with the
chimpanzees Becky and Lucy upon whom he had performed frontal
lobectomies. I told him of my results at the Yerkes Laboratory and that
they did not corroborate his findings. He fetched two large volumes of
protocols, gave them to me to read, and said he’d meet me for lunch.

What I found is described in detail in Chapter 16. It agreed with what
I had found but totally disagreed with the interpretation that had been
presented to the scientific community. I met Fulton for lunch. He asked if I
had found what I wanted—my answer was yes—but he asked nothing
about the particulars of what I had found. I remember little else of the
meal after he next asked, “Would I like to come work with him?” !!!!

A few months later, I received a telegram stating that he had received
the funding from the Veterans Administration he had been waiting for, and
could I start in two weeks? I finished what I could and started at Yale the
day before Fulton left for London. The rest has become, as they say,
history.

Electrical and Chemical Stimulation of the Limbic
Forebrain

In the previous chapter, I detailed the form that brain processes take
in generating our feelings of pain and pleasure. Closely related are the
brain processes that deal with the autonomic nervous system that regulates
our visceral and endocrine functions. In this chapter, I describe my



discoveries—while I was at Yale University in John Fulton’s Department
of Physiology—of the brain systems that deal with those visceral body
functions and their presumed relationship to emotional and motivational
behavior.

A persistent theme in describing how our body participates in the
generation of emotions and motivations was formulated by William
James. James suggested that when we perceive an emotionally exciting
sensory event, the brain sends a message to our body, which responds to
the stimulus by evoking different bodily (especially visceral) reactions.
The emotion is due to our sensing these body reactions. His formulation is
known as the “James-Lange theory.” Lange had proposed that the body’s
blood vessels provide the signals that we interpret as an emotion. James
suggested that not only blood vessels but also viscera, such as the gut and
heart, generate these signals. Occasionally, James also mentions the
muscles as originating such signals, and more recently Antonio Damasio,
professor of neurology at the University of Southern California, in a 1994
book, Descartes’ Error, called attention to the variety of signals arising in
the body that contribute to our feelings. He calls these signals “somatic
markers.”

One of the reasons I was attracted to Fulton’s laboratory at Yale was
that the laboratory had built an electrical stimulation device with which
brain stimulations could produce changes in blood pressure, heart and
respiratory rate, and other visceral changes. I had tried unsuccessfully to
produce such changes with an old alternating-current device called a
“Harvard inductorium.” Penfield told me that he had had some success in
changing heart rate using the inductorium to stimulate the cortex on the
upper, medial surface of the temporal lobe, but that he could not reproduce
this result very often. The Yale apparatus put out “square-wave” pulses,
and the duration of the pulses could be controlled. When that duration was
adjusted to a millisecond, the visceral changes produced by cortical
stimulation were invariably produced.

The studies accomplished at Yale using this square-wave stimulator
had shown that visceral responses could be obtained by stimulations of the
cingulate gyrus and the cortex at the base of the frontal lobe. I devised a
method that exposed not only these areas of the cortex but that of the
“insular” cortex buried in the lateral fissure between the frontal and
temporal lobes and the adjacent cortex of the front end of the temporal



lobe surrounding the amygdala as well. Using these techniques, assisted by
a graduate student, Birger Kaada, and a postdoctoral neurosurgical student,
Jerome Epstein, I was able to map a continuous region of cortex extending
from the temporal lobe to the cingulate cortex. Electrical stimulation of
this cortex produced changes in blood pressure, heart, and respiratory rate.
We recorded these changes by means of a long needle hooked up to a
rotating set of drums over which we stretched paper we had “smoked”
using a smoking candle or gas burner. After obtaining a record, we
shellacked the paper and waited until the next morning for it to dry. We
called the region from which we obtained our responses the “mediobasal
motor cortex” to distinguish it from the classical motor cortex on the
lateral surface of our brain. Today, a more memorable name would be “the
limbic motor cortex.”

56. The Limbic Motor Cortex

I next turned to using chemical stimulation of the brain. During my
residency in neurosurgery, I had assisted Percival Bailey, Warren
McCulloch and Gerhard von Bonin at the Neuropsychiatric Institute of the
University of Illinois at Chicago in their chemical stimulations of the
monkey and chimpanzee brain, stimulations they called “strychnine
neuronography.” Neuronography outlined brain systems that were
intimately interconnected, clearly separating these from the surrounding
regions. But Bailey, McCulloch and von Bonin mapped only the lateral
surface of the brain. Using my newly devised techniques to provide access



to the medial aspects of the brain, I therefore went to work, assisted by
Paul MacLean, who had joined my project in Fulton’s department at Yale,
to map these hitherto inaccessible regions of the brain.

Neuronography demonstrated the division of the limbic forebrain into
the two systems: the amygdala system and the hippocampal system, each
with intimate connections to a part of the frontal cortex. We were able to
clearly demarcate limbic from non-limbic regions; but, as in my
behavioral studies, the “limbic” cortex included neocortical additions in
the primate, additions that were not present in the cat.

57. This figure, depicting the lateral and inferomedial aspects of a monkey’s brain shows
diagrammatically the distribution of regions and segments referred to in the text. Dotted stippling

denotes frontotemporal; rosroventral striations denote medial occipitotemporal; vertical
striations denote medial parieto-occipital; horizontal straitions denote medial frontoparietal;

dorsorostral straiations denote medial frontal.

The other important result of our neuronographic studies was that,
though there were major inputs to the hippocampus from the adjacent
isocortex, there were no such outputs from the hippocampus to adjacent
non-limbic cortex. The output from the hippocampus is to the amygdala
and accumbens, and through the subcortical connections of the Papez
circuit. MacLean, always adept at coining names for the results of our
experiments, suggested that this one-directional flow of signals from
isocortex to the limbic cortex indicated a “schizophysiology” in that the
“limbic systems,” though open to influence from the rest of the cortex,
worked to some extent independently from the isocortex—much as James
Papez had suggested. Papez was most pleased with our results when
MacLean and I recounted them on a visit to his laboratory.

Emotions and the Limbic Brain
How then are the limbic parts of the brain involved in organizing our

emotions? My initial inquiries indicated that there was some considerable



substance to the views formulated by William James and James Papez, but
at the same time, I was at a loss when I tried to answer how these systems
worked in organizing our emotions. An answer had to come from carefully
analyzing the composition of what constitutes our emotions and
motivations, and relating the constituents to particular brain systems.
When I wrote my book Brain and Perception, I found that I needed to
consider the entire brain, not just the sensory input systems, to determine
how our brain organizes our perceptions.

The same situation applies in studying emotions. For instance, we can
describe the changes in behavior following removal or electrical
stimulation of the amygdala, but when we attach names to the emotions
that accompany those changes in behavior we need to use the parts of the
brain that are involved in producing names, parts of the brain considerably
removed from the limbic systems. Thus, to attribute the taming of a
monkey to a loss of “fear” may or may not be correct, as we will see in the
next chapter. A realistic view of the relationship between emotion and
motivation to the organization of brain systems, including those called
limbic, can be attained only when all the facets of the behavioral and brain
systems are considered.

The Yale-Hartford Laboratories
In 1951, three years after Fulton asked me to come to Yale, he was

asked to head the medical school’s library, which he had founded and
nourished, and to give up the Department of Physiology. Though I could
continue to teach at Yale, most of the physiology department’s laboratory
space was now assigned to biochemistry. I therefore moved the bulk of my
laboratory to a new research building at a nearby mental hospital, the
Hartford Retreat, renamed the Institute of Living. I needed financial
support to make the move.

David Rioch, psychiatrist and neuroscientist, as well as a dear friend,
had just become director of the Army’s Research Laboratories at the
Walter Reed Hospital in Washington, DC, and was, at that time, bringing
together a group of superb investigators to shed light on the complexities
of the brain/emotion/motivation relationship. When we started we knew
practically nothing about the limbic forebrain, certainly nothing about the
relation of the prefrontal cortex to the limbic systems and what these parts



of the brain might have to do with emotion and motivation. Rioch had
worked on hypothalamic systems that, at the time, were considered the
“head ganglion of the autonomic nervous system,” which by definition
excluded cortical contributions to visceral and emotional processing. It
was my work with Bucy, supported by Fulton, that had overturned that
earlier conception.

The immediate question Rioch was addressing was set by troops who
could hardly make it to their battle station in the Korean war, but when, in
an experimental setting, were told that orders had been changed and to
reverse their course to go for R and R in Japan, would cheer and sing and
march double time. What chemistry or brain processes accompanied such
a change in emotion and motivation?

It was Rioch who funded my initial grant applications to pursue my
research. I asked Rioch if the Army might provide some $2,000 for
automated behavioral testing equipment. Rioch replied that this would be
impossible. I must ask for at least $20,000 since it would cost $2,000 just
to do the paper work in processing my grant application. I asked for, and
received the $20,000. For almost a decade, this money, plus additional
funding, supported a promising and productive group of graduate students,
not only from Yale but also from Harvard, Stanford, the University of
California at Berkeley, and McGill University in Canada. The laboratory at
the Institute became a Mecca for students intensely interested in the
relationship between brain and behavior.

Functions of the Olfactory Brain
In 1952, with the help of my then Yale graduate student Larry Kruger,

I published a paper titled “Functions of the Olfactory Brain.” We did not,
at the time, use the term “limbic systems” for this constellation of
structures—that formulation was to be achieved shortly. I received over
2,000 reprint requests for that paper.

We were able to confirm the division of the “olfac-tory brain”
(beyond the olfactory sensory structures per se) that had been attained
with chemical neuronography into two distinct segments on the basis of
the results of electrical stimulations: 1) those parts that receive a direct
input from olfactory structures (which lie just behind the nose and receive
an input from the receptors in the nose) and 2) a set of structures that



receives input from the earlier segment. The first segment is composed of
a system that centers on the amygdala, which has a cortical component;
the second segment is composed of the circuit that includes the hippo-
campal and cingulate cortexes.

In our paper, Kruger and I noted that in primates, including humans, a
band of new six-layered neo-cortex immediately surrounds the allocortex
in the neighborhood of the amygdala and the cortical systems adjacent to
it. In our classification we were following the lead of Filomonov, a
Russian neuro-anatomist whom I visited in Moscow to learn firsthand why
he had included these newer components in what he called “juxt-
allocortex” (juxt, “next to”). Juxt-allocortex, more recently renamed “peri-
allocortex” is new cortex, appearing for the first time in primates. As a
result, the use of the name “neocortex” to distinguish its functions from
those of the allocortex is inappropriate.

This detail is important because the popular idea that our brains are
made up of old cortex that organizes more primitive behaviors such as
emotions and motivations—as opposed to new cortex that serves cognitive
functions— doesn’t hold up. Thus, many of our social institutions that are
based on this emotions-versus-cognitions dichotomy badly need
revamping. I’ll have much more to say about this in Chapter 17.

My behavioral, electrical and chemical stimulation experiments
undertaken at Yale corroborated the anatomical considerations that had led
to the inclusion of juxtallo-cortical formations as intimate parts of the
allocortex adjacent to it.

The second segment of the olfactory brain was the hippocampal-
cingulate system. This system had been singled out in the 1920s and 1930s
by the eminent Viennese neurologist Constantin von Economo and by
neuro-anato-mist James W. Papez in Columbus, Ohio, as generating our
emotional feelings and their expression. Their reasoning was based purely
on anatomical connections: Fibers from the hippocampus connect to the
septal and hypothalamic region, which in turn connects to the thalamus,
whose fibers reach the cingulate cortex which then connects back into the
hippocampus. This circuit is now named after Papez. The Papez circuit
was thought to be involved in emotions because our emotions often seem
to go round and round like a self- sustaining circuit. The behavioral idea is
a good one: during the 1970s, I used to give lectures on “The Hang-up
Theory of Emotions”—that emotions are like getting stuck in a loop. But



providing evidence for the anatomy-based belief that the Papez circuit is
involved in generating emotions has proved to be considerably more
elusive.

My research on monkeys, which I had started at the Yerkes
Laboratory of Primate Biology in Florida and was continuing at Yale,
showed that the amygdala, not the Papez circuit, is involved in taming and
changes in sexual behavior that follow a temporal lobe removal. Nor did
hippocampal removals in humans produce any effects on experiencing or
expressing their feelings. Furthermore, removals of the hippocampus have
devastating effects on a specific kind of memory, effects that I’ll take up
below and in Chapter 15.

However, in totally different sets of experiments the hippocampus
has been shown to be intimately involved in processing stress by
regulating the pituitary-adrenal hormonal system, a finding that needs to
be incorporated in any attempt to understand the role of the limbic
systems in organizing emotions.

Anatomical Fundamentals
A brief review of the brain anatomy relevant to the processes

organized by the frontolimbic forebrain can be helpful at this point:
We can visualize the central nervous system as a long tube which is

fabricated during our embryonic development from the same layer of cells
that elsewhere makes up our skin. This tube is lined with stem cells and
within the tube flows the cerebrospinal fluid. The lower part of the tube
makes up our spinal cord. Where the tube enters our skull through a big
hole—in Latin, foramen magnum— the tube becomes our brain stem. The
hind part of the brain stem is called the “hindbrain”; the mid part of the
brain stem is the “midbrain”; and the forward part is the “fore-brain.” The
most forward part of the forebrain is made up of the “basal ganglia.”

The basal ganglia can be divided into three components: An upper, a
middle, and a limbic (from the Latin, limbus, “border”). The upper
component is made up of the caudate (Latin, “tailed”) nucleus and the
putamen (Latin, “pillow”). The middle component is the globus pallidus
(“pale glob”). The limbic component is made up of the nucleus accumbens
(Latin, “lying down”) and the amygdala (Greek, “the almond”).



The brain cortex makes up two hemispheres that have developed in
the embryo by a migration of cells from the basal ganglia. Six layers of
cells migrate from the upper basal ganglia to make up most of the cortex
technically called the “iso” (Latin for “uniform”) cortex. A more common
name for this expanse of cortex is “neocortex” (new cortex), but as I
described earlier in this chapter, this more common name is misleading in
some of its usages. The remainder of the cortex is called the “allo” (Latin
for “other”) cortex. The allocortex is formed when cells from the
amygdala and other limbic basal ganglia migrate. These migrations
sometimes do not occur at all; but for the most part, they make up a three-
layered cortex. Sometimes, however, there is a doubling of three layers as
in the cingulate (Latin, “belt”) cortex.

Mistaken Identities
The limbic basal ganglia and their allo-cortical extensions are called

“limbic” because they are located at the medial limbus (Latin, “border”)
of the hemispheres of the forebrain. When I started my research in the late
1940s, this term was used mainly for the cingulate cortex lying on the
upper part of the inner surface of the brain’s hemispheres. But also, Paul
Broca, the noted 19th-century neurologist, had discerned that the
allocortex formed a ring around the inside border of the brain’s
hemispheres. However, until the 1950s, the allocortex was generally
referred to by neuro-anatomists as the “olfactory” or “smell” brain
because it receives a direct input from the nose.

Today, what was then called the “olfactory brain” is called the “limbic
system” and “everyone knows” that our emotions are organized by our
limbic brain. But what constitutes what neuroscientists call the limbic
system varies to some considerable extent, and the relationship of the
various parts of the limbic system to emotion is, at times, considerably
distorted. Thus the popular view has built-in difficulties. The amygdala is
only one structure among those loosely thought of as the limbic forebrain.
In fact, originally the amygdala was not included at all in the limbic
circuitry. Rather, two sources converged to be baptized by Paul MacLean
as the “limbic systems.” As noted above, one source was Paul Broca who
had discerned a rim of cortex around the internal edge of the cerebral
hemispheres to be different from the rest of the cerebral mantle. This



cortex appeared paler and was later shown to have only three layers rather
than six. He called this rim of cortex ”le grande lobe limbique.”

58. Upper and limbic basal ganglia

59. Cortical projection from the basal ganglia

The other source, described by J. W. Papez, consisted of the
hippocampus, its downstream connections through the septal region to the
mammillary bodies of the hypothalamus, thence to the anterior nuclei of
the thalamus which is the origin of a projection to the cortex of the
cingulate gyrus, which had been labeled the “limbic” cortex. The cingu-
late cortex, in turn, feeds back into the hippocampus. Papez reasoned that
emotions tend to go round and round and this anatomical circuit does just
that.



MacLean’s limbic system encompassed Broca’s cortex and Papez
circuitry, and originally did not include the amygdala. It was not until
MacLean joined my project in John Fulton’s department at Yale that it
became imperative on the basis of my results to include the amygdala in
any circuitry that presumably organizes our emotions. This inclusion
totally changed what had hitherto been thought of as the limbic forebrain.

The Prefrontal Cortex
In addition to experimentally defining the limbic systems, my

research established the essential relationship of the anterior frontal
cortex, usually called the prefrontal cortex, to those limbic systems. The
finding, noted earlier, that a mediobasal limbic motor cortex surrounds the
prefrontal cortex suggested that such a relationship existed. However, the
definitive study came from my analysis of the thalamo-cortical input to
the prefrontal cortex. The thalamus, the halfway house of sensory input to
the cortex, is a three-dimensional structure. The brain cortex is essentially
a two-dimensional sheet of cells and their connecting fibers. Anatomical
projections that connect thalamus and cortex must therefore “lose,”
enfold, one dimension. This means that one thalamic dimension
disappears in its projection to cortex. For most of the forebrain, the
dimension “lost” is what in the thalamus is a medial to lateral dimension;
front-to-back and up-and-down dimensions are maintained in the
projection so that the thalamic organization and the cortical terminus of
the thalamic input remain essentially unchanged.

Not so for the thalamic projections to the limbic and prefrontal
cortex. The thalamic dimension “lost” in the projection is the front-to-
back dimension. For instance, a long file of cells reaching from the front
to the back of the thalamus projects to a point on the tip of the prefrontal
cortex in primates.

This result defines an essential difference between the frontolimbic
forebrain and the posterior convexity of the brain. As the next chapters
will show, this difference is as important for processing the brain/behavior
relationship as that between the right and left hemispheres of the brain.

In Summary



1. The idea that the neocortex is responsible for organizing our
cognitive reasoning, while the older cortical formations are solely
responsible for generating our baser emotions and motivations, does
not hold up. In primates, including us, the neocortex adjacent to the
older cortex is very much involved in organizing the visceral and
endocrine processes that presumably underlie (or at least accompany)
our emotions and motivations. This finding calls into question the
presumed primitive nature of our emotions and motivations.

2. The olfactory systems of our brain, now called the “limbic systems,”
are composed of two distinct systems. One system is based on the
amygdala, a basal ganglion, the other on the hippocampus.

3. The basal ganglia of the brain can be divided into three
constellations: an upper, a mid and a limbic. This chapter reviewed
evidence that the limbic basal ganglia—the amygdala, accumbens
and related structures—are involved in organizing visceral and
endocrine responses presumably critical to emotional experience and
behavior.

4. But direct evidence of the involvement of limbic basal ganglia in
emotions had, as yet, not been obtained. The next chapters describe
evidence that leads to the conclusion that the limbic basal ganglia are
involved in emotions and that the upper basal ganglia—the caudate
nucleus and the putamen—are involved in organizing motivational
behavior.

5. The hippocampus, an intimate constituent of the Papez circuit, plays
an important role in the chemical regulation of the pituitary/adrenal/
hormonal response to stress. Electrical and chemical stimulations
showed that there is a large input from other parts of our cortex to the
hippo-campus but that its output goes mostly to other limbic system
structures.

6. However, the effects of damage to the Papez hippocampal circuit are
much more drastic on certain aspects of memory than on emotion or
motivation. This poses the question of how the response to stress, and
therefore the Papez circuit, relates to memory and, in turn, how
memory relates to emotion and motivation.

By 1958, when I moved to Stanford University, I had achieved a good
grasp of the neuroanatomy and electrophysiology of what came to be



known as the limbic forebrain and had written several well-received
papers reviewing my findings. However, the relationships of the different
limbic structures to behavior remained rudimentary. The research that I
undertook to begin to understand these relationships is reviewed in the
next chapter.



Chapter 11
The Four Fs

Wherein the experimental analysis of the brain systems involved in
fighting, fleeing, feeding and sex, the “Four Fs,” are recounted.

First I shall do some experiments before I proceed further,
because my intention is to cite experience first and then with
reasoning show why such experience is bound to operate in such
a way. And this is the true rule by which those who speculate
about the effects of nature must proceed.

—Leonardo da Vinci, c. 1513 from Fritjof Capra, “The Science of
Leonardo”



The brain processes that organize our feelings do not “feel like” those
feelings. In early stages of science, we expect that our brains and bodies,
as well as our physical universe, resemble what we experience every day.
Thus we are pleased when we are able to correlate a specific aspect of an
experience with a specific brain system. But when we examine the patterns
that make up our experience and those that make up that brain system, the
match between patterns is not evident. Transformations occur between the
patterns that initiate our experiences and those that process the
experiences. The brain processes that organize our feelings are not as we
“picture” them.

The television process provides a good example of the difference
between what we experience and the processes that make the experiences
possible. At the beginning of the process is a scene and at the end is a
picture of that scene. But that is not all there is to television (tele is Greek
for “far” or “far off”). We need to consider the steps from the broadcasting
studio, where a scene is transformed into a code that can be transmitted, to
the decoding transformation at the receiver; and then the transformation
that activates a screen, in order to understand how the picture on the
screen is generated so that it corresponds to the scene in the studio. For the
brain/behavior/experience relationship it is the same: we need to specify
the transformations that make the relationship possible.

In the mid-20th century, the zeitgeist in experimental psychology was
behaviorist: any terminology that referred to our subjective experiences—
terms such as “fear,” “joy,” “hunger”—was taboo. At the time I too,
heartily approved of these restrictions in terminology and decided to
employ only “operational” descriptions to characterize my results of the
experimental analysis of the taming and the other effects in monkeys
following the removal of the amygdala.

In the 1920s and 1930s, Walter Cannon had described the outcome of
his hypothalamic stimulations as resulting in “fight and flight” responses.
For symptoms that followed removal of the amygdala—the taming, the
increase in sexual mounting, and repetitiously putting things in their
mouths—I added two responses to Cannon’s “fight and flight” to make up
“Four Fs”: Fighting, Fleeing, Feeding and Sex.

This characterization immediately became popular and still is. In the
mid-1950s, when I gave a series of lectures on this topic at the University
of Michigan, my initial audience was a mere 100 students and faculty. By



the end of the week, we had to move the lectures to an auditorium seating
1,000, and all the seats were filled.

For the 1960 Annual Reviews of Psychology, I wrote up the essence of
what had been discovered in my laboratory during the 1950s at Yale
University regarding these Four F’s. A few skirmishes with the editors
ensued: the four-letter-word revolution at Berkeley had not as yet
occurred. (Of course, for the uptight, the fourth F could stand for
fornication.) Fortunately for me, the Annual Reviews were published in
Palo Alto, near my Stanford laboratory, and by personally supervising the
final draft just prior to the typesetting of the paper, I was able to have the
last word.

While at Yale, I had aimed to seriously quantify the behaviors that
made up the Four Fs. For instance, I wanted to know what caused those
monkeys whose amygdalas had been removed to put everything into their
mouths. Could it be that they had lost their sense of taste and therefore
were sampling everything? These monkeys would eat hot dogs, whereas
normal monkeys, who are not fond of meat, might take a bite and stop
eating. In order to test the monkeys’ ability to taste, Muriel Bagshaw, a
Yale medical student, and I set up a series of drinking flasks, each with a
greater dilution of quinine (aka tonic) water and let the monkeys choose
which flasks they would drink from. Using this technique, we showed
something that at the time had not been discovered: the part of the brain
involved in discriminating taste was located in the cortex buried within the
lateral fissure of the anterior part of the temporal lobe near, but not in, the
amygdala.

In another set of experiments, we used rats. John Bro-beck, a
colleague of mine at Yale who later became head of the Department of
Physiology at Johns Hopkins University, was an expert in making
electrolytic lesions (a fancy term for burning a hole) in the hypothalamic
region—which is connected to the amygdala by way of two tracts—lesions
which resulted in the rats eating and eating and eating. Bro-beck was
looking at changes in the rats’ metabolism that were related to the
excessive eating behavior. He was the perfect collaborator for me to study
the possible metabolic “causes” of changes in eating behavior following
the effects of the temporal lobe removals on eating. We were joined by a
postdoctoral fellow from India, Bal Anand, who many years later would
become the head of the All India Institute of Medical Sciences.



At Yale, we three set up our experiment. The results were a disaster
mitigated by an unexpected silver lining. First, I tried to remove the rats’
amygdala, the part of the medial temporal lobe that I’d been removing in
monkeys. Rats have very small brains—no temporal lobes, and their
amygdalas are spread out and therefore difficult to remove without
damaging other brain structures. Surgery didn’t work, so Brobeck went to
work with his electrolytic lesions. Now the rats refused to eat at all!—just
the opposite of what I’d found in my earlier tests with monkeys.

For the behavioral part of our study, pre-operatively I set up drinking
flasks for the rats, just as Bagshaw and I had done for monkeys: The rats
loved the quinine (tonic) water and several of them drank themselves sick.
I also provided the rats with a choice between their daily ration and one
that looked like (and had the consistency of) their usual food but was made
up of axle grease and sawdust. The rats loved the new “food” and made
themselves sick gorging on it. Once they had become sick, whether from
quinine or axle grease, they never touched either again. Thus they would
make a poor control group for this experiment. Furthermore, the removal
of the amygdala might make the rats deathly ill because they would not
stop eating or drinking, not because they had lost their sense of taste. Just
making rats sick was not our intention. These tests were obviously not
going to work for our experiment.

This set of preliminaries to undertaking our experiments had taken
well over a year, and Anand soon would have to return to India. There
were no results from all our work. I suggested that Anand do an
anatomical study before returning to India, to see how many of Brobeck’s
electrolytic lesions had hit their target: the amygdala. Brobeck was not
enthusiastic: He’d been doing electrolytic lesions in the hypothalamic
region for years, first at Northwestern University and then at Yale, and had
almost always produced the expected results: rats that overate. But why
not section the brains of the rats anyway, to locate the sites of the
electrolytically produced lesions? That way Anand would learn the
technique and would have something to show for his time at Yale.

Surprises
The anatomy was done on both the overeating rats and the ones that

had stopped eating. Surprise #1: Hardly any of the lesions that had



produced the overeating were in the expected location (the ventromedial
nucleus) in the hypothalamic region. They were in the vicinity, but not
“in,” the nucleus. This confirmed, for me, a long-held view that a nucleus
isn’t a “center,” a local place, at all: the cells that make up the “nucleus”
are spread over a fairly long distance up and down the hypothalamic
region. This was later ascertained by a stain that specializes in picking out
the cells of this nucleus.

Surprise #2: The electrodes that Brobeck had aimed at the amygdala
had gone far afield—the lesions made with these electrodes were located
halfway between those that had been aimed at the hypothalamic region and
those that had been aimed at the amygdala. Thus, Anand and Brobeck had
obtained an important result: they had discovered a new region, a region
that influenced eating behavior, which they dubbed the “far-lateral”
hypothalamic region. The results were published in the Journal of
Neurophysiology with an introductory note acknowledging my role in the
experiment, and they were quoted for many years to come.

But the story does not end here. Shortly after the publication of this
paper, Philip Teitelbaum, then a young doctoral student at Johns Hopkins,
became intrigued with the “stop eating” effect and got his rats to eat and
drink enough to tide them over the initial period after surgery. He did this
by giving the rats sugar water and chocolate candy. At a meeting, he
reported the results as having been produced by lesions in the far-lateral
nucleus of the hypothalamus and I stood up, noted his excellent work, and
went on to point out that the locus of the electrolytic lesions in the Anand
and Brobeck experiments (for which I’d supervised the anatomy) and
presumably in Teitelbaum’s, were located in white matter consisting of
nerve fibers, not of cells, and that there really was no such “far-lateral
hypothalamic nucleus.” At that time we didn’t know where those nerve
fibers originated or ended. Teitelbaum has told me on several occasions
since that he was totally shaken by my comment.

He persisted nonetheless, and we know now, through his efforts and
those of others, that these fiber tracts connect places in the brain stem with
the basal ganglia; that they are an important part of the dopamine system
which, when the system becomes diseased, accounts for Parkinsonism.

Anand established the meaning of these results. He went back to India
and pursued the research he had started at Yale using electrical recordings
made from the regions he had studied there. In his India experiments, he



found that as a rat ate, the electrical activity of its medial region
progressively increased while that of its lateral region progressively
decreased. During abstinence, the opposite occurs: the rat’s lateral region
becomes progressively more active while the medial region becomes
progressively more quiescent. His experiments demonstrated that there is
a seesaw, reciprocal relationship: The medial region serves a satiety
process; the lateral region serves an appetitive process. This reciprocal
relationship applies to drinking as well as to eating.

Hit or Run?
The experimental results of our studies of “fighting” and “fleeing”

behavior were equally rewarding. At Yale we set up several colonies of
young male monkeys and selected one of these colonies in which the
monkeys clearly formed a dominance hierarchy. We had developed
quantitative measures of dominance based on procedures used by Rains
Wallace, a friend of mine who was chief of the research institute
representing the insurance companies in Hartford, Connecticut, where I
lived at the time. Wallace had found that, in order to quantify social
behavior that is almost always determined by a variety of factors (in the
insurance case, factors involved in injury or death)— one must choose an
“anchor.” The anchor chosen must be the most obvious or the most easily
measurable behavior. Then other behaviors that might influence the factor
we are studying (in our case, dominance) can be rated with respect to the
behavior we have chosen as an anchor. As an anchor in our experiment
with the dominance hierarchy of monkeys, I chose the number of peanuts
each monkey picked up when the peanuts were dropped into their cage one
by one. We also used the order in which each monkey grabbed the peanut,
followed by other “subsidiary” measures such as facial grimace, body
posture of threat, and actual displacement of one monkey by another. We
filmed all of this so that we could study the films frame by frame.

I removed the amygdalas (of both hemispheres of his brain) of the
most dominant monkey in the colony. As expected, he gradually fell to the
bottom of the hierarchy, but it took about 48 hours for his fall to be
completed. Other monkeys were challenging him, and he seemed not to
know how to meet these challenges.



60. A: dominance hierarchy of a colony of eight preadolescent male rhesus monkeys before any
surgical intervention. B: same as A after bilateral amygdalectomy had been performed on Dave.

Note his drop to the bottom of the hierarchy. (From Pribran, 1962)

Next, I operated on the monkey who had replaced the first as the most
dominant. The result regarding the change in the order of dominance was
essentially the same. To be certain that our results would be generally
applicable, I then operated on the monkey that had originally been third in
the hierarchy. Surprise! He became more aggressive and maintained his
dominance. Of course, my colleagues accused me of sloppy surgery. We
had to wait a few years (while the monkeys were used in other
experiments) before an autopsy showed that my removal of the amygdala



was as complete in the third monkey as it had been in the first two
monkeys I had operated upon.

When my Yale collaborators, Enger Rosvold, Alan Mirsky and I were
later reviewing our films of the monkeys’ interactions after their surgery,
we found that in the case of the third monkey no challenges to his
dominance had occurred. The next monkey in line—monkey #4—was a
peacenik, happy to get his share of food and water without troubling the
other monkeys.

Brain Surgery and Social Context
Over and over, my experience with removals of the frontal and limbic

parts of the brain had demonstrated, as in this instance, that such
vulnerability to social context occurs during the immediate postoperative
period. Dramatic changes in behavior can be produced immediately
postoperatively by social input. Lawrence Weiskrantz—at that time a
Harvard graduate student in psychology, doing his dissertation
experiments in my laboratory—and I informally tested this observation:
After surgery, I approached the monkey in a docile fashion, always
keeping my head below his level, and withdrawing when he approached.
The monkey became his old belligerent rhesus self and even more so—
grimacing and attacking an approaching figure or hand. By contrast, using
another monkey who had had his amygdala removed in the same fashion,
Weiskrantz behaved firmly and assertively. His monkey became docile,
and we all had him eating out of our hand.



61. C: same as A and B, except that both Dave and Zeke have received bilatral
amygadalectomies. D: final social hierarchy after Dave, Zeke, and Riva have all had bilateral
amygdalectomies. Note that Riva fails to fall in the hierarchy. Minimal differences in extent of

locus of the resections do not correlate with differences in the behavioral results. The disparity
has been shown in subsequent experiments to be due to Herby’s nonagressive “personality” in

the second position of the hierarchy. (From Pribram, 1962.)

Our Yale dominance experiment has been quoted over the years in
many social psychology texts. The significance of the results became
especially important when the Assistant District Attorney of the State of
California was heard to suggest that removal of the amygdala might be a
treatment of choice for the hardened violent criminals populating our jails.
I called him to alert him to our results—that the criminals might become
even more vicious, unless the postoperative social situation was very
carefully established, and that we didn’t really know how to do this.
Luckily, the matter was quickly dropped like the hot potato it was. It was



the mid-1960s and I was concerned that someone might recommend brain
surgery for the “revolutionary” students on our campuses.

Brain surgery does not remove “centers” for aggression, fear, sex,
etc. Ordinarily brain processes enable certain behaviors and, once
engaged, help to organize and reorganize those behaviors.

There is a difference however when our brain becomes injured and
“scars” form, for instance when scars produce temporal lobe epilepsy:
some patients occasionally have uncontrollable episodes during which
they become violent, even to the extent of committing rape or murder.
Once the seizure is over, the patient returns to being a docile and
productive person. If the patient recalls the period of his seizure—though
most do not—or when evidence of how he behaved is shown to him, he is
terribly upset and remorseful, and will often request surgical relief if the
seizures have become repetitive.

The Fourth F
As we were studying at Yale the effects of amygdalectomy on

dominance and on eating and drinking similar experiments were being
performed at Johns Hopkins University in the Department of Physiology
and in the Army Research Laboratories at Walter Reed Hospital in
Washington, DC. In addition, both of these research groups were studying
the effects of amygdalectomy on sexual behavior in cats. At Hopkins,
there was no evidence of change in sexual behavior after the surgical
removal. By contrast, at the Army Laboratories, as in Klüver’s original
report of his and Bucy’s experiments on monkeys in Chicago,
“hypersexuality” followed the removal of the amygdala in cats.

I paid a visit to my friends in the Department of Physiology at
Hopkins and looked at the results of their surgical removals, which were
identical in extent to mine. I looked at their animals—cats in this case
rather than monkeys. The cats were housed in the dark basement of the
laboratory in cages with solid metal walls. Anyone planning to handle the
cats was warned to wear thick gloves. When the cats were looking out
through the front bars of the cage, the experimenters would blow into their
faces to test for taming: the cats hissed. Attempts to grab the cats resulted
in much snarling, clawing and biting.



I then traveled 30 miles from Baltimore to Washington to visit my
friends at the Walter Reed Medical Research Center, where they were also
removing the amygdalas from the brains of cats. The brain surgery had
been well carried out and appeared, in every respect, identical to what I
had seen at Hopkins. But the situation in which these cats were housed was
totally different. These cats were kept in a large compound shared with
other cats as well as with other animals—including porcupines! The
animals were playing, wrestling, eating and drinking. My host, who was in
charge of the experiments, approached one of the cats who cuddled to him.
To my consternation, he then tossed the cat into a corner of the compound,
where the cat cowered for a while before rejoining the other animals in
their activities. “See how tame these cats are? The experimenters at
Hopkins must have missed the amygdala.” By now, the cats had resumed
their mounting behavior, including a very patient and forgiving porcupine
who kept its quills thoroughly sheathed while a “hypersexed” tabby tried
to climb on top of it.

I wrote up this story of the Baltimore and Washington cats for the
Annual Reviews of Psychology, again emphasizing that it is the social
setting and not the extent or exact placement of the brain removal that is
responsible for the differing outcomes we find after the surgery.

I was able to add another important facet to the story: experiments
that were done at the University of California at Los Angeles on a set of
“Hollywood cats.” This time, the experimenters had studied normal cats.
They were placed in a large cage that took up most of a room. There were
objects in the cage, including a fence at the rear of the cage. The behavior
of the cats was monitored all day and all night. The UCLA experimenters
found what most of us are already aware of: that cats go prowling at night,
yowling, and, (what we didn’t know) mounting anything and everything
around! After the brain surgery this nocturnal behavior took place during
the day as well. Testosterone levels were up a bit. However, in our
experiments with monkeys at Yale, we had shown that while testosterone
elevations amplify sexual behavior such as mounting once it has been
initiated, these elevated levels do not lead to the initiation and therefore
the frequency of occurrence of sexual behavior. Today we have the same
result in humans from Viagra.

The Hollywood cats thus showed that the removal of the amygdalas
influenced the territory, the boundaries of the terrain within which sexual



behavior was taking place. Although an increase in sexuality due to an
increase in hormone level might have played a role in amplifying sexual
activity, the increase in that behavior might also have resulted in the
modest increase in sexual hormone secretion. By far, the more obvious
result was a change in the situation—day vs. night—in which the cats were
doing It: For ordinary cats, night is the familiar time to practice the fourth
F. For the cats who had been operated upon, every situation was equally
familiar and/or unfamiliar. It is the territorial pattern, the boundary which
forms the context within which the behavior occurs, that has been altered
by the surgery, not the chemistry that fuels the behavior.

Basic Instincts
In light of all these results, how were we to characterize the Four Fs?

On one of my several visits with Konrad Lorenz, I asked him whether the
old term “instinct” might be an appropriate label for the Four Fs. The term
had been abandoned in favor of “species-specific behavior” because if
behavior is species specific, its genetic origins can be established. I had
noted during my lectures at Yale that if one uses the term instinct for
species-specific behavior, human language is an instinct. This usage of the
word is totally different from its earlier meaning. Nevertheless, Steven
Pinker, professor of linguistics at MIT, has recently published a most
successful book entitled The Language Instinct.

My suggestion to Lorenz was that we might use the term “instinct”
for species-shared behaviors. What makes the studies of birds and bees by
ethologists like Lorenz so interesting is that we also see some of these
basic patterns of behavior in ourselves. Falling in love at first sight has
some similarities to imprinting in geese, the observation that Lorenz is
famous for. Sir Patrick Bateson of Cambridge University, who, earlier in
his career, was one of my postdoctoral students at Stanford, has shown
how ordinary perception and imprinting follow similar basic
developmental sequences.

Lorenz agreed that my suggestion was a good one, but felt that it
might be a lost cause in the face of the behaviorist zeitgeist. His view was
also expressed by Frank Beach in a presidential address to the American
Psychological Association in a paper, “The Descent of Instinct.”



Thus, the Four Fs, though they have something to do with emotional
(and motivational) feeling, can be described behaviorally as basic instincts
or dispositions. These dispositions appear to be a form of a hedonic-
(pleasure and pain) based memory that, as we shall shortly see, directs
attention in the process of organizing emotions and motivations. As an
alternative to “basic instincts,” the concept “dispositions” captures the
essence of our experimental findings regarding the Four Fs. Webster’s
dictionary notes that dispositions are tendencies, aptitudes and
propensities. Motivations and emotions are dispositions.

The brain processes that form these hedonically based dispositions
are the focus of the next chapters.



Chapter 12
Freud’s Project

Wherein I take a detour into the last decade of the 19th century to find
important insights into emotional and motivational coping processes.

One evening last week while I was hard at work, tormented with
just that amount of pain that seems to be the best state to make
my brain function, the barriers were suddenly lifted, the veil was
drawn aside, and I had a clear vision from the details of the
neuroses to the conditions that make consciousness possible.
Everything seemed to connect up, the whole worked well
together, and one had the impression that the thing was now
really a machine and would soon go by itself.

—Sigmund Freud, letter to Wilhelm Fliess, October 20, 1895



As I was moving from Yale to Stanford in the late 1950s, I had just
disproved Köhler’s DC theory of the brain as responsible for perception,
and I had arrived at an impasse regarding the “how” of the Four Fs. I had
put up a successful behaviorist front: One of my Yale colleagues, Danny
Friedman, asked me why I’d moved to Stanford. He truly felt that I’d
come down a step or two in Academe by moving there. He remarked, “You
are a legend in your own time.“ If that was true, it was probably because I
gave up a lucrative career as a neurosurgeon for the chance to do research
and to teach. My initial salaries in research were one tenth of what I was
able to earn in brain surgery. But I was accomplishing what I wanted
through my research: I had been accepted in academic psychology and had
received a tenured academic position, held jointly in the Department of
Psychology and in the Medical School at Stanford.

At that time, “The Farm,” as Stanford is tenderly called, really was
that. What is now Silicon Valley, a place of “concrete” technology, was
then Apricot Valley (pronounced “aypricot”) with miles of trees in every
direction. Stanford Medical School’s reputation was nonexistent: that’s
why a whole new faculty, including me, was put in place when the school
was moved southward from San Francisco to the Stanford campus in Palo
Alto. Our first concern at the Medical School: How will we ever fill the
beds at the hospital? A heliport was established to bring in patients from
afar and advertisements were sent out to alert central and northern
California residents that we were “open for business.” How different it
was then from when, 30 years later, I would become emeritus and leave
California to return to the East Coast.

I had succeeded Lashley as head of the Yerkes Laboratory of Primate
Biology in Florida and Robert Yerkes had become my friend during the
negotiations for my appointment. Still, I had encountered insurmountable
problems in trying to obtain university appointments for prospective
research personnel at the Laboratory because of its distance from any
university. I had been equally unsuccessful in moving the laboratory to a
friendly university campus where teaching appointments could be
obtained.

I felt that research institutions, unless very large (such as NIH or the
Max Planck), become isolated from a broader scope of inquiry that a
university can provide, and thus, over time, become inbred and fail to
continue to perform creatively. I had tried to convince Harvard to accept a



plan I had developed to move the Laboratory to a warehouse in
Cambridge, since Harvard had a stake in the Laboratory; Lashley’s
professorship was at Harvard. Harvard faculty had nominated me for a
tenure position three times in three different departments (psychology,
neurology and even social relations), but the faculty recommendations
were not approved by the administration.

All effort in these directions was suddenly halted when Yale sold the
Laboratory to Emory University (for a dollar) without consulting the board
of directors. This was in keeping with the administrative direction Yale
was taking: the Department of Physiology had already shifted its focus
from the brain sciences to biochemistry, and the president had mandated
loosening Yale’s ties (that included remanding tenured professorships) to
the Laboratory and other institutions that were mainly supported by
outside funding.

The time was ripe to “Go West, young man” where other luminaries
in psychology—Ernest (Jack) Hilgard, dean of Graduate Studies, and
Robert Sears, the head of the Department of Psychology, at Stanford—had
been preparing the way.

Hello, Freud
Despite my so-called legendary fame and the administrative

disappointments at Yale and Harvard, the issue that bothered me most, as I
was moving to Stanford, was that I still really didn’t understand how the
brain worked. In discussions with Jerry Bruner, whose house my family
and I rented for that summer while I taught at Harvard, he recommended
that I read Chapter 7 of Freud’s Interpretation of Dreams. I also read the
recently published biography of Freud by Ernest Jones, who mentioned
that someone knowledgeable about the brain should take a look at Freud’s
hitherto neglected Project for a Scientific Psychology, the English
translation of which had been published only a few years earlier in 1954. I
read the Project and was surprised by two important insights that Freud
had proposed:

1. Motivation is the “prospective aspect of memory.” At that time,
psychologists were focused on internal ”drives” as the main source of
motivation, a conception my laboratory experiments and those of
others had already found wanting; and



2. Brain circuitry involves action potentials, resistance, and local graded
activity. My theoretical work had emphasized electrical field
potentials that were characteristic of the brain’s fine- fibered,
dendritic, deep structure—and here was Freud, a half century earlier,
already aware of the importance of this aspect of brain processing.

I immediately set to work on a paper, calling Freud’s Project a
“Rosetta stone” for understanding psychoanalysis. My paper emphasized
the memory-motive structure and local graded activity, the field potentials
in the brain which was translated into English as “cathexis.” I found
fascinating insights into how a late-19th-century neurologist could view
brain function. Freud had struggled with the problem of form in terms of
quantity vs. quality. He showed that an adrenaline-like quantitative
chemical buildup resulted in anxiety—”unpleasure” in the translation.
Anxiety, Freud indicated, was controlled by way of the memory-motive
circuitry in the basal (ganglia of the) forebrain. Qualities, Freud wrote,
composed consciousness enabled by processes operating at the brain’s
surface, its cortex. The cortical processes were not made up of circuits but
of “patterns of periodicity,” of energy transmitted from our sensory
receptors.

I had been suggesting that an essential part of cortical processing was
holographic-like—that is, based on interference patterns formed by
frequencies of waves. Frequencies are the reciprocals of Freud’s patterns
of periodicity. I was stunned and elated. It is helpful to find that one’s
scientific views are recurrently confirmed under a variety of experimental
conditions.

The publication of my paper, entitled “The Neuro-psychology of
Sigmund Freud” in the book Experimental Foundations of Clinical
Psychology, would lead to a ten-year study of the Project in a partnership
with Merton Gill, the eminent psychoanalytic scholar. Our book, Freud’s
“Project” Re-assessed, was the fruit of this effort. It was great fun: the
last sentence in the book reads “—and our views (about the virtue of the
future place of psychoanalysis as a natural science) are irreconcilable.”

.

The “Project” Reassessed



Gill was at Berkeley and I at Stanford. When he first read my paper,
he congratulated me on having finally become a scholar, not just a bench
scientist. He noted, however that I’d made 27 errors in my not-so-very-
long paper. I was upset and went to work to correct the errors. After a
month, I wrote a reply to Merton, saying that I was grateful for his critique
and that he was correct in 7 of his criticisms—but the other 20 were his
errors, not mine. After a few more exchanges like this, I decided to
telephone him. I suggested that since we were just across San Francisco
Bay from each other we might meet, which we did—alternating between
Berkeley and Stanford every Friday afternoon for the next two years. We
scrupulously studied both the English and German texts and compared
them. German was my native language, and Merton’s knowledge of
Freud’s later work was critical: we needed to see how many of the early
ideas contained in the Project had later been abandoned or changed.
Surprisingly few.

After two years of work, Merton and I had a stack of notes taller than
either of us. What to do? I said, let’s write a book. I quickly disabused
Merton of writing voluminous footnotes, as was then the fashion in
psychoanalysis. From George Miller I had inherited the motto: “If it’s
important, put it into the text. If not, leave it out.” Merton and I each wrote
chapters—and after a first draft we started critiquing each other’s chapters
and produced another set of notes. I used them to write another draft. More
critique from Gill. I wrote up a third draft.

Silence. After two weeks I called Gill: He hadn’t read the draft. He
was sick of the Project, and never wanted to hear of it again. I could do
anything I wanted with our work. The only stipulation was that I would
never mention his name in connection with it. He could not give me any
reasons for his change of heart; all he could say was that he was tired of
the work. I was dismayed, but I did as he asked in the two more papers that
I wrote, saying only that I was deeply indebted to M. G., who didn’t want
me to mention him.

Resurrection
A decade passed, and I was asked to give the keynote address at the

meeting of the International Psychoanalytic Society in Hawaii. I felt
deeply honored and accepted the invitation. Next, I was asked who might



introduce me. Merton Gill seemed to be the perfect person. I called Gill,
now in New York. He told me that he had moved from Berkeley shortly
after we had last talked and had been depressed and hadn’t given a talk in
ten years, but that he felt honored that I was asking him to introduce me!
Gill, the curmudgeon, feeling honored? The world was turning topsy-turvy.
He asked that I send him some of my recent brain physiology studies so
he’d be up to date. I replied that no, I wasn’t going to present those. What
I wanted to do was to present the findings in our book. Did he still have a
copy of the last draft? Search. “Here it is in my files,” he said. “Have you
read it?” I asked him. “No.” “Will you?” “Yes. Call me next week.” I did,
on Tuesday. Gill was ebullient: “Who wrote this?” I said I did. “Brilliant.
Wonderful.” I called back a few days later to be sure I’d actually been
talking to Merton Gill. I had. All was well.

We met in Hawaii. My talk was well received, and Merton and I
decided to resume our long-delayed joint effort. I felt that we needed to re-
do the first chapter, which was about the death instinct—a horrible way to
begin a book. Merton thought that if we could solve what Freud meant by
primary and secondary processes, we would make a major contribution to
psychoanalysis and to psychology. He polished the final chapter, and I
went to work on the “processes” and found, much to my surprise, that they
were similar to the laws of thermodynamics that were being formulated by
the eminent physicist Ludwig Boltzmann in Vienna at the same time that
Freud was actively formulating his ideas summarized in the Project.

Freud’s Ideas
During our work on the Project many more rewarding insights

emerged about Freud’s ideas. Gill was especially pleased to find that Freud
had already addressed what was to become the “superego” in his later
works. Freud described the “origin of all moral motives” to a baby’s
interaction with a caretaking person who helped the infant relieve the
tensions produced by his internally generated drives. The brain
representations of the actions of the caretaking person are therefore as
primitive as those initiated by the drives. It is the function of the
developing ego to bring the caretaking person into contact with the infant’s
drives. Gill had written a paper to that effect even before he had read the
Project, and was therefore pleased to see this idea stated so clearly. The



“superego” is therefore a process during which society, in the role of a
caretaking person, tries to meet and help satisfy drive-induced needs, not
to suppress them. This is contrary to the ordinary interpretation given
today which holds that the ‘superego’ acts to suppress the ‘id.’

Another gem was Freud’s description of reality testing: An input is
sensed by a person. The input becomes unconsciously—that is,
automatically—processed in our memory-motive structure. A comparison
between the results of the memory-motive processing and that input is
then made consciously before action is undertaken. This “double loop of
attention,” as Freud called it, is necessary because action undertaken
without such double-checking is apt to be faulty.

62. Stylized representation of the “machine” or “model” of psychological processes presented in
the Project. (From Pribram and Gill Freud’s “Project” Re-assessed)

In the 1880s, Helmholtz had already described such a parallel process
for voluntarily moving the eyes. Freud greatly admired Helmholtz’s work
and was overtly trying, in the Project, to model a “scientific psychology”
according to principles laid down by Helmholtz and Mach. In the absence
of voluntary reality testing, action and thought, which for Freud was
implicit action, are merely wishful. Freud defined neuroses as composed
of overriding wishes. One can see, given the rich flavor of the Project,
why I became so excited by it.



For almost a half-century I’ve taught a course on “the brain in the
context of interpersonal relations,” and I always begin with Freud. The
students are as surprised and excited, as I have been, to realize that
“cognitive neuroscience,” as it is called today, has common threads going
back to the beginning of the 19th century—threads that formed coherent
tapestries toward the end of that century in the writings of Sigmund Freud
and William James.

Within a year of our decision to complete our book, Freud’s
“Project” Re-assessed, Gill and I were ready for its publication, which
immediately received a few nice reviews from neurologists and
psychiatrists, and quite a few really bad ones from psychoanalysts. Basic
Books disappointed us in its distribution, and my presentations to various
psychoanalytic institutes were greeted with little enthusiasm except for
those in the Washington-Baltimore Institute and the William Allison
White Institute in New York, where discussions were lively and informed.

Now, over thirty years after the 1976 publication of our book,
beginning on the 100th anniversary the Project in 1995, both have become
reactivated: Psychoanalysis is returning to its neurobiological roots. There
are now active neuro-psychoanalytic groups in Ghent, Belgium, and
Vienna, Austria, and at the University of Michigan at Ann Arbor—and
there is also a flourishing Neuropsychoanalytic institute in New York.

Freud called his Project, which was about the brain, as well as his
later socio-cultural work, a “metapsychology.” The metapsychology was
not to be confused with his clinical theory, which was derived from his
work with his patients. Most psychoanalysts work within Freud’s clinical
theory— but that is another story, some of which is addressed in Chapter
19. The current endeavors in neuro-psychoanalysis are bringing a new
perspective to psychoanalysis and psychiatry. As Gill and I stated in our
introduction to Freud’s “Project” Re-assessed, Freud already had
formulated a cognitive clinical psychology in the 1890s, long before the
current swing of academic and practicing psychologists to cognitivism.
But even more interesting to me is the fact that the Project is a
comprehensive, well-formulated essay into cognitive neuroscience a
century before the current explosion of this field of inquiry. Reading Freud
and working with Merton Gill helped me, a half century ago, to reach a
new perspective in my thinking about what the brain does and how it does
it.



For a few years, in the mid-1960s, I would present lectures on the
insights and contents of Freud’s Project as my own—only to acknowledge
to an unbelieving audience during the discussion of the presentation, that
what I had told them was vintage Freud, 1895.



Choice



Chapter 13
Novelty: The Capturing of Attention

Wherein I recount a set of surprising results of experiments performed in
the 20th century that changed our view of how motivations and emotions
are formed.

When I have inserted knowledge into my mysteries or thrillers I
have found that my readers think that I’m showing off and that
the passage(s) interrupt the plot.

—Chair, ThrillerFest Conference, New York, June 2007



While the Freud “project” was going forward, most of my effort went
into designing and carrying out experiments that would answer the
question: What brain process, what disposition(s), did the diverse
behaviors making up the Four Fs have in common?

As I was about to move to Stanford, tackling the search for a common
brain process that underlies the Four Fs frontally seemed to have reached a
dead end. I therefore decided to do something that is unusual in the
scientific community (and certainly a course of action that would never be
awarded a research grant): I decided deliberately to outflank the problem
by testing monkeys in some fashion that could not be readily subsumed
under the heading of the Four Fs. What would be the effects, if any, of
removal of the amygdala in such a situation?

Transfer of Training
I had recently heard a lecture on “transfer of training.” The procedure

consisted in first training an animal or child to choose the larger of two
circles and then presenting two rectangles to see whether the animal (or
human) will again choose the larger of the two without any further
training. I used monkeys that had been operated on some three years
earlier—their hair had grown back, and they were indistinguishable from
the monkeys who had not been operated upon. I then trained all the
monkeys, those who had been operated on and those who had not, to
choose the paler of two gray panels set in a background of a third shade of
gray. Once the monkeys were thoroughly trained, I began to substitute,
approximately every fifth trial, two new panels with shades of gray that
were substantially different from the gray of the original panels. A number
of the monkeys chose the lighter shade of gray of the two new panels.
Others chose randomly between the lighter or darker of the two new
panels, as if they had not learned anything.

When I checked the surgical records of the monkeys I had tested, I
was delighted: All the monkeys who chose randomly had been operated on;
their amygdalas had been removed. And all the monkeys who had
transferred their training—what they had learned—to the new task were
the control animals who had never been operated upon. Here was the
performance of a task that was affected by amygdalectomy and that could
by no stretch of imagination be labeled as one of the Four Fs.



I had finished this experiment at Yale just before moving to Stanford.
It was time to reconcile this result with those obtained on the Four Fs.
Muriel Bagshaw who, as a student, had assisted me with the “taste”
studies, had received her MD degree at Yale after having a baby and had
relocated to the Pediatric Department at Stanford just at the time I moved
my laboratory to the new Stanford Medical Center. We resumed our
collaboration with enthusiasm and immediately replicated the transfer of
training experiment, using circles, rectangles and other shapes. Once
again, the monkeys who had their amygdalas removed were deficient on
tasks that demanded transfer of training, while their memory for tasks that
did not involve transfer of training had remained intact. I surmised that
what might underlie the deficits obtained in the Four F experiments is that
the animals that had had their amygdalas removed failed to transfer their
pre-operative experience to the postoperative situation and failed to
accumulate the results of new experiences that accrued daily.

Growing Up
I was still not completely over my behaviorist period. Though I was

“growing up,” as Lashley had predicted that I someday would, and had
done so to a considerable extent while Miller, Galanter and I were writing
Plans and the Structure of Behavior, I still felt that we were buying into
representations and Plans being stored and reactivated “in the brain”—and
I was at a loss as to how such a process might work. I had disposed of
Köhler’s isomorphism and had not yet adopted Lashley’s interference
patterns. But for my conjecture that the disposition underlying the Four Fs
has to do with transfer of training, the issue was straightforward: Does the
deficit in transfer of training indicate that processes occuring in the limbic
basal ganglia allow specific experiences to became represented in the
brain? Lashley (who firmly supported “representations”) and I had once
argued this point while we were strolling down the boardwalk in Atlantic
City. I can still “see” the herringbone pattern of the boards under our feet
every time I think of representations—suggesting that perhaps he was
right.

As I was worrying the issue of representations in the early 1960s,
while I was putting together my laboratory at Stanford, Alexander Luria
and Eugene Sokolov, his pupil, came from Moscow for a weeklong visit.



We three had just attended a conference in Princeton where Sokolov had
presented his results, definitively demonstrating that repeated sensory
inputs produce a representation in the brain—a neuronal model of the
input, as he called it. Sokolov had exposed his human subjects to regular
repetitions of tones or lights. Each subject had demonstrated an “orienting
reaction” to the stimulation, an orienting reaction which Sokolov had
measured by changes in the subject’s skin conduction, heart rate, and the
EEG. As Sokolov repeated the stimulation, the subject’s orienting reaction
to that stimulus gradually waned—that is, the subject’s response
habituated. Sokolov then omitted one in the series of stimuli. Leaving out
the “expected” stimulus provoked an orienting reaction to the absence of a
stimulus that was as large as the reaction the initial stimulus had
produced! The reaction to the absence of a stimulus meant that the person
now reacted to any change in the whole pattern of stimulation, a pattern
that had become a part of his memory. Sokolov’s evidence that we
construct a specific representation of the pattern of our experience was
incontrovertible. We orient to a change in the familiar!!!

But at the same time, my reservations regarding “representations”
were confirmed. At the moment of response to the omitted stimulus, the
response was not determined by the environmental stimulus but by the
memory, the neuronal model, of prior stimulation. As we shall see in the
next few chapters, there are whole brain systems devoted to storing the
results of “nonstimulation.” Walter Freeman, professor of neuroscience at
the University of California, Berkeley, has emphasized the fact that our
perceptions are as much determined by our memories as by the
environmental situations we confront. Freeman had to come to this
conclusion because, in his experiments, his animals did not show the same
brain patterns when confronted by the same environmental patterns. As
William James had put it, our experience is like a flowing river, never
quite the same.

Freeman and I are good friends, and I have tremendous respect for his
work and have been influenced by it. But for decades we discussed his
findings, and I was always troubled by the changed brain patterns that
appeared to the same stimulus in his experiments. Finally, Walter
articulated clearly that it is the memory, the knowledge attained, that is as
much if not more important to forming a precept as is the stimulating



event itself. Sokolov’s “leaving out” an expected stimulus is a simple
demonstration of this finding.

Our situation is like that of the Copernicans—we grope for ways to
articulate the results of our experiments, explanations that at one level we
already “know.” With respect to the results of the experiments on
familiarization, and their relevance to emotion, these insights into what
constitutes a so-called “representation” turned out to be even more
significant than what constitutes a “representation” with regard to
perception.

At Princeton, Sokolov had read his paper in somewhat of a monotone
but in reasonably good English. By contrast, Luria had made his
presentation—on the scan paths that eye movements describe when
looking at a picture—with a flourish. Sokolov seemed unimpressed,
twirling his eye glasses as Luria talked. I assumed that Sokolov had heard
Luria many times, had “habituated” and was therefore bored. Or was he
really just monitoring Luria for the KGB? We were, after all, at the height
of the Cold War.

Arriving at Stanford after several stops at universities on the way,
Luria appropriated my secretary and asked her to do a myriad of tasks for
him. Sokolov seemed extremely impatient, in his black suit and balding
head, confirming my suspicion that he might be KGB. Suddenly, Luria
said he was tired and needed a nap. As we were showing him to his room,
Sokolov whispered to me, “I must see you. It is urgent.” Of course, I
understood: he must be KGB and needed to convey to me some
restrictions or warnings about Luria’s rather notorious behavior.
(Unbridled in his everyday transactions, he had been “rescued” several
times by his wife, a chemist in good standing in the Soviet Union.)

Once Luria was tucked away for his nap, Sokolov heaved a sigh of
relief and exclaimed, “That man is driving me mad. We have stopped
exclusively at camera stores between Princeton and here. I need a shirt. I
saw a shopping mall as we arrived. Could you please, please take me
there?” So much for the KGB!

Our own State Department, which had vetted and cleared the Luria-
Sokolov trip to the U.S. was a bit more serious about having me monitor
them while under my custody. I was not to take Luria more than 50 miles
beyond Stanford; not to Berkeley and not even to San Francisco! “Et
cetera, et cetera, et cetera,” as the King of Siam said in The King and I.



Government monitoring aside, we managed to have a wonderful
week. Between planning experiments, we went on several (short) trips. I
took Luria and Sokolov to Monterey to see Cannery Row; both of them
had read Steinbeck. We went to a lovely little café with a garden in back,
situated on a slight slope. Luria, who was facing the garden, got up and in
his usual brisk manner dashed off to see it, failing to notice the outline he
had left in the sliding glass door he’d crashed through! His impact was so
sudden that only where he hit was the glass shattered and scattered: the
rest of the door was intact. I settled with the cafe-owner— for $30. Luria
was surprised that anything had happened: there was not a mark on him.

Despite government “injunctions,” we did go to San Francisco:
Sokolov had read all the Damon Runyan and Dashiell Hammett stories and
had to see all the sites he remembered—that is, between Luria’s camera
store stops. We had an appointment for Luria and Sokolov to speak at
Berkeley and were a bit late for their talks—but all went smoothly. I never
heard from either the State Department or the KGB about our various
visits.

Orienting and Familiarization
Muriel Bagshaw and I set out to use Sokolov’s indices of the

orienting reaction in monkeys. Another student and I had first confirmed
Sokolov’s conclusions on humans. In conducting these tests, however, we
had found that the placement of the heart and blood pressure monitor was
critical—somewhat different results were obtained with even slightly
different placements of the monitor. There is much art in experimentation
—that is why I try to see for myself an experimental procedure in
someone else’s laboratory, or to repeat the experiments in my own
laboratory, if the results of an experiment are critical to my thinking.

Over the next two decades, Bagshaw and I explored the effects of
removal of the amygdala in monkeys on the orienting reaction and its
subsequent habituation. In the process, we also found that classical
Pavlovian conditioning is disrupted by removal of the amygdala. Our
results showed that in the conditions of our experiments, both habituation
and conditioning are dependent on activating the viscera (hollow organs,
such as the gut, heart and lungs) of the body and the autonomic nervous



system that controls those viscera. No gut-involvement, no habituation;
that is, no familiarization.

63. Afferent and Efferent Connections of the Amygdala

Ordinarily, a novel (or reinforcing) event produces a visceral reaction
largely by way of the autonomic nervous system: a galvanic skin response
due to a slight increase in sweating; a brief increase in heart rate; and a
change in respiratory rate. These responses “habituate,” indicating that the
stimulus has become familiar: with repetition of the stimulus, the
responses attain a low amplitude of waxing and waning. What surprised us
is that after amygdalectomy the novel stimuli failed to habituate; that is,



the novel failed to become familiar. No representation, no “neuronal
model” had been formed.

These new observations fitted well with those I had obtained at Yale
in the late 1940s, when I showed that electrical stimulation of the
amygdala and the related limbic motor cortex results in changes in our
heart and breathing rate, blood pressure and in gut contractions. Since
then, James McGaugh of the University of California at Irvine has shown
that there is a direct influence of amygdala stimulation on the secretion of
adrenaline by the adrenal gland.

But just as in the case of the classical motor cortex, the behavioral
function of the limbic motor cortex is not to regulate the
visceral/autonomic functions per se, but rather to develop attractors; that
is, to process targets. In the case of the classical motor cortex, these
targets serve as attractors that organize an achievement in the world we
navigate, an intention-in–action. In the case of the limbic motor cortex,
the targets serve to organize an appraisal, an assessment of the relevance
to the organism of a novel input.

Such assessments of relevance are usually explored under the term
“attention.” Actually, the experimental analysis of the effects of removals
of the amygdala (or their stimulation) resulted, for the most part, in
changes in attention, changes that had then to be related to the processing
of emotion. The next sections of this chapter summarize and reorganize, in
terms of what we finally achieved, what seemed to us to be a very long
experimental route we had to traverse between attention and emotion.

Something New
In 1975, Diane McGuinness, then a graduate student at the University

of London, and I published a review of decades of experiments, performed
around the world, that aimed to show differences in bodily responses to
various emotions. McGuinness and I noted, in our review, that all the
physiological measures that scientists had used to characterize differences
in emotional responses in a subject ended in discovering differences in the
subject’s manner of attending. Donald Lindsley and Horace (Tid) Magoun,
working at the University of California at Los Angeles (UCLA), had
proposed an “activation theory of emotion” based on differing amounts of
“attention” being mobilized in a situation. Thus, the results of the



experiments McGuinness and I had reviewed, including those that
Bagshaw and I had performed, were in some respects relevant to the
processing of emotions. But exactly how was, before our review, far from
clear.

Mc Guinness and I recalled that William James had divided attention
into primary and secondary. This division was later characterized as
assessing “what is it?” (primary) vs. assessing “what to do?” (secondary.)
Because assessing “what is it?” interrupts our ongoing behavior, we
described it as a “stop” process. Because assessing “what to do” initiates a
course of action or inaction we may take, we described it as a “go”
process. The experiments in my laboratory had demonstrated that the
amygdala and related limbic brain systems deal with “stop,” while the
upper basal ganglia deal with “go” processes.

James’s division of attentional processes into primary and secondary
paralleled his characterization of emotions as stopping at the skin, while
motivations deal with getting into practical relations with the
environment. One popular idea derived from this distinction is that some
people respond to frustration more viscerally and others more muscularly:
this difference is called “anger-in” and “anger-out.” In our culture, women
are more prone to respond with anger-in while men are more prone to
anger-out.

These divisions also parallel those made by Freud: primary processes
fail to include reality testing (which depends on a double loop of
attentional processing); reality testing defines secondary processes during
which we seek to get into practical relations with the world we navigate. It
is important to note that, for both James and Freud, assessing “what to
do?” as well as assessing “what is it?” deal with attention. Thus, assessing
“what to do?” motivation, is a disposition, an attitude, not an action.
Interestingly, both in French and German “behavior” is called
comportment and verhaltung—how one holds oneself. In this sense,
attending to “what is it?” also becomes an attitude.

I received an award from the Society of Biological Psychiatry for
showing that the upper basal ganglia (cau-date and putamen) organize and
assess, the sensory input involved in “what to do?”—that these basal
ganglia control more than the skeletal muscle systems of the body, as is
ordinarily believed. The award was given for my demonstration that
electrical stimulations of the basal ganglia alter receptive field properties



in the visual system in both the thalamus and the cortex. In discussing my
presentation of these results, Fred Mettler, professor of anatomy and
neurology at Columbia University, had this to say at a conference there,
during which he gave the introductory and final papers:

I don’t think we should be deluded by the rather casual way
that Karl presented this material. This is an elegant display of
experimental techniques in an area which is extremely difficult
to handle. I congratulate you, Karl. What he has shown you
throws into relief the importance of two systems concerning
which we have so far had only a few hints during the
presentations of the two days. He has opened the way to a new
symposium, dealing with the interconnections of the [basal
ganglia] with the cortex on the one hand, and the [basal
ganglia] with the thalamus, on the other. . . . The difference he
has shown you . . . [is] what we may call the associational
handling of sensory input. Without the [basal ganglia] the
animal is quite unable to relate itself to its environment at a
satisfactory level of self-maintenance. Without its cortex it is
unable to relate itself accurately to its environment but it can
still do it. The cat, maligned feline though it may be, is able to
get along reasonably well without much cortex but if you add a
sizable [basal ganglia] deficit to this, the animal looks at you
with vacuous eyes and, in uncomprehending manner, will walk
out of a third floor window with complete unconcern.

Though these experiments were performed in the 1960s and 1970s,
they have not as yet triggered an “orienting reaction” in the habitués of
established neuroscience.

Emotions as Hang-ups
Thus, the Four Fs are expressions of our basic dispositions to eat, to

fight, to flee, to engage in sex. The experiments I’ve just described have
shown that the behavioral expressions of these dispositions are rooted in
the basal ganglia. The upper basal ganglia are involved in starting and
maintaining these behaviors, while the limbic basal ganglia, especially the
amygdala in its relation to the hypothalamic region, regulate satiety:



stopping eating and drinking, and placing social and territorial constraints
on fighting and fleeing, and on sex. This raises a key question: What has
“stopping” to do with emotion?

The answer to this question came from electrical stimulations of the
medial hypothalamic region and of the amygdala in rats, cats, monkeys
and humans. When the stimulation of these brain structures is mild, the
animal or human becomes briefly alert. In humans, drowsiness or boredom
are briefly interrupted—only to be quickly reinstated. Moderate
stimulation results in greater arousal, an “opening up,” with evidence of
interest in the animal’s surroundings. (In humans such stimulation results
in a smile and even in flirting.) Medium-strength stimulation results in
withdrawal and petulance. Strong stimulation produces panic in humans
and what has been called “sham rage” in animals: the subject of the
stimulation lashes out at whatever target is available. Alerting, interest,
approach, withdrawal, panic and emotional outburst are on a continuum
that depends on the intensity of the stimulation!

These results led me to talk about emotions as “hang-ups” because
alerting, interest, withdrawal, panic and outbursts do not immediately
involve practical relations with an animal’s or a person’s environment.
The coping with pain and pleasure is performed “within the skin.” As
noted, William James defined emotion as stopping at the skin. When
expressed, emotions don’t get into practical relations with anything or
anyone unless there is another person to “read” and to become influenced
by the expression.

When we are no longer “hung up,” we again become disposed to
getting into practical relations with our environment; that is, we are
motivated and our upper basal ganglia become involved.

Conditioned Avoidance
Popular as well as scientific articles have proclaimed that the

amygdala and hippocampus are “the centers in the brain for fear.” Such
pronouncements make some of us who are teaching feel exasperated. Of
course there is a descriptive base for the popular assertions: Some
scientists are interpreting the function of a particular brain system on the
results of one experimental test or a group of related tests. When they find
evidence, such as a freezing posture and increase of defecation in rats,



they assume that the rat is “afraid.” Working with primates makes such
assumptions more difficult to come by.

My monkeys, and even dogs, figured out what my experiments that
aimed to test for “fear” were all about. In such experiments, a signal such
as turning on a light indicated to the animal that it needs to jump out of its
cage into an adjacent one to avoid a mildly unpleasant electrical current
that would be applied to the bottom of the cage 4 seconds hence. Animals
learn such a conditioned avoidance quickly. Monkeys showed me that they
had become conditioned by jumping—and then by repeatedly reaching
over and touching the electrified cage they had just jumped out of—that
they knew what was going on. Knowing and emotionally fearing are not
the same.

The dogs came to me with a Harvard graduate student who was to
study the effects of removal of the amygdala in a “traumatic avoidance”
experiment. In this experiment the dogs were to learn to avoid a noxious
degree of electric current that was turned on some seconds after a gate
dividing two compartments was lifted. In an initial experiment I lifted the
gate without turning on any electric current—and the dogs jumped
immediately to the other compartment. Was the “grass greener on the
other side of the fence”? The student and I then substituted a flashing light
for the “fence” as a signal that the dogs were to jump over a low barrier to
reach the other compartment. The dogs learned to do this substitute for a
”traumatic” conditioning procedure in fewer than ten trials. (Of course, as
was my policy for the laboratory, I never turned on any traumatic electric
current whatever, at any time.) Dogs like to please and they quickly
figured out what we wanted. In these experiments, both monkeys and dogs
appeared pleased to have a chance to play and seemed cognitively
challenged rather than afraid.

A Story
How to choose the course of the “what is it?” and “what to do?” is

beautifully illustrated by a story Robert Sternberg loves to tell, based on
his dedication to Yale University, where he teaches and does research.

Two students, one from Yale and the other from Harvard, are hiking
alone in the woods in Yellowstone Park when they come upon a bear. Both
students are interested in bears, but have also heard that bears can be



dangerous. The Harvard student panics. What to do? Climb a tree? Bears
can climb trees. Run? Could the bear outrun us? Hide? The bear can see
us. Then he noticed that his friend, the Yalie, had taken out his running
shoes from his backpack, had sat down on a fallen log, and was lacing
them up. “Why are you doing that? the Harvard student asked. “Don’t you
realize that you’ll never outrun that bear whether you have track shoes on
or no?” “I don’t have to outrun him,” replied the ‘cortical’ Yalie. “I only
have to outrun you.”

The bear was a novel experience for the students. Their visceral and
autonomic systems most likely responded to the novelty. The intensity of
the response depended on what they knew of bears in that habitat. The
feelings of interest initiated by the sight of the bear were due not only to
the visceral responses that were elicited; quickly the students’ “what to
do?” processes became involved. Further, at least one of the students
entertains a “cortical” exercise, an episode in morbid humor, in attitude—
or so the story goes. I can see them laughing, to the puzzlement of the
bear, who wanders away from the scene.

What Comes First
William James had suggested that the emotional components of

attention follow our awareness of “what is it?” However, recent anatomical
and physiological evidence has demonstrated that some sensory input
(visual, tactile and auditory) actually reaches our amygdala before it
reaches our brain’s cortex. When this input is sensed, signals are sent, via
the autonomic part of the brain’s motor systems, to the viscera and adrenal
gland. Thus visceral and endocrine activity is inaugurated on a “fast track”
before cortical processing (the perception of the event) occurs.

Experiments have repeatedly shown that an event can have an effect
when we do not recognize the content of the experience. The experiments
consist of showing one hundred pictures to a person and then briefly
showing five hundred pictures of which one hundred are the ones
previously shown. People have no difficulty in identifying the one hundred
as having been previously experienced—although they can’t remember the
content of these pictures. To experience a specific emotion, such as fear,
we must be afraid of something, but the previously shown pictures evoked
no recognition and therefore only a feeling of familiarity.



My colleagues and I had shown, in the experiments described above,
that removal of the amygdala did not abolish the visceral and endocrine
responses when the animals were exposed to a novel pattern. The
amygdala were only a part of the limbic motor cortex that could convey
the novel input. But much to our surprise, these responses persisted as the
pattern was repeated: the responses did not habituate.

Habituation, familiarization was dependent on the presence of an
intact amygdala able to process the visceral and endocrine responses. The
amygdala were involved in processing the effects of body responses so
that the novel pattern became familiar, a part of the subject’s memory, a
“neuronal model” of the experience.

In the absence of habituation, animals are physiologically aroused by
anything and everything as if it were “novel”—the response activating
visceral and endocrine reactions. Novelty, the unfamiliar, triggers a
visceral/endocrine response. Triggering of visceral and endocrine
responses by novelty brings together the activation and visceral theories
of emotion but in a most unexpected manner. The rest of this chapter
describes this unexpected relationship between visceral activation and
emotion.

What Is Novelty?
There is a great deal of confusion regarding the perception of novelty.

In scientific circles, as well as in art and literature, much of this confusion
stems from confounding novelty with something that scientists and
engineers call “information.” During a communication, we want to be sure
of what is being communicated. I often don’t quite get what another
person is saying, especially on a telephone, so I ask that the message be
repeated, which it is, perhaps in a slightly different form. My uncertainty
is then reduced. In 1949, C. E. Shannon and W. Weaver, working at the
Bell Telephone Laboratories, were able to measure the amount of
reduction of “unsureness,” the amount of uncertainty produced during a
communication. This measure of the reduction in uncertainty became
known as a measure of information. (In 1954, Gabor related this measure
to his wavelet measure, the quantum of information, which describes the
minimum uncertainty that can be attained during a communication.)



The experiencing of novelty is something rather different. During the
early 1970s, G. Smets, at the University of Leuven, performed a definitive
experiment that demonstrated the difference between information (in
Shannon’s sense) and novelty. Smets presented human subjects with a
panel upon which he flashed displays of a variety of characters equated for
difficulty in being told apart. The displays differed either in the number of
items or in the arrangement of the items. The subjects had merely to note
when a change in the display occurred. Smets measured the visceral
responses of the human subjects much as we had done in our monkey
studies. Changing the number of items in the display—demanding a
change in the amount of information to be processed, in Shannon’s terms
— produced hardly any change in visceral responses. By contrast,
changing the arrangement of the items into novel configurations (without
changing the number or the items themselves) evoked pronounced visceral
reactions. Novelty is sensed as a change in pattern, a change in
configuration, a change from the familiar.

In presenting the results of Smets’s experiments, this past year one of
my students at Georgetown complained that she understood that novelty is
different from information: But how? Her classmates agreed that they
would like to know specifically what characterizes “novelty.” I stewed on
the problem for three weeks and on the morning of class, as I was waking
up, the answer became obvious:

Novelty actually increases the amount of uncertainty. This increase is
the opposite of the reduction of uncertainty that characterizes Shannon
information. Novelty, not information, is the food of the novelist as
indicated in the quotation that introduces this chapter.

Normally, we quickly habituate to a novel occur-rence when it is
repeated. We form a representation that we use as a basis for responding or
not responding, and how we respond. This representation of what has
become familiar constitutes an episode, which is punctuated by a “what is
it?”, an orienting reaction to an event that initiates another episode until
another orienting reaction stops that episode and begins a novel one. We
can then, together with other episodes, weave together the complex that
forms these episodes into a story, a narrative. Unless a particular episode
becomes a part of our “personal narrative,” we fail to remember it.



Constraints
How then are episodes formed from the stream of our experience?

The basic finding—that our stream of consciousness, the amount of
attention we can ”pay,” is constrained—comes from knowing that the
“what is it?” stops ongoing experience and behavior. Monkeys with their
amygdala removed did not stop eating or drinking or fleeing or fighting or
mounting in circumstances where and when the normal animals do. After I
removed their amygdala, the boundaries, the where and when, that define
a sequence of experience or behavior before a change occurs, were gone.
In the normal animal, as well as in us, those boundaries define an
experienced episode.

I removed the amygdala on young puppies, and we got the same
result I had obtained in monkeys. The puppies who had their amygdala
removed did not rush to eat, as did their intact control littermates, but once
they began eating they went on and on, long after their littermates had
become satiated and had left the food tray. When we tested monkeys who
had had their amygdalas removed to see whether they ate more because
they were “hungrier” than their unoperated controls (or their preoperative
selves), we found that the monkeys who had had their amygdalas removed
were actually less eager to eat at any moment (as measured by the amount
eaten over a given time period or by their immediate reaction to the
presentation of larger food portions), but once they began to eat, they
would persist for much longer before quitting the eatery.

When we are in the grip of an intense feeling, such as love, anger,
achievement or depression, it feels as if it will never end, and it pervades
all that we are experiencing. But after a while, feelings change; a stop
process has set a bound, a constraint, on the feeling. Satiety constrains
appetites; territoriality constrains sexuality; cognition constrains fleeing
or fighting.

A constraint, however, does not necessarily mean simply a
diminution of intensity. Constraints (Latin: “with-tightening”) may, under
specifiable circumstances, augment and/or prolong the intensity of a
feeling. It’s like squeezing a tube of toothpaste: the greater the squeeze the
more toothpaste comes out. Obsessive-compulsive experiences and
behaviors, for instance, provide examples of constraints that increase
intensity.



Constraints are provided for us by the operation of a hierarchical set
of our neural systems, of which the amygdala and other basal ganglia are
in an intermediary position. Beneath the basal ganglia lie the brain stem
structures that enable appetitive and consummatory processes and those
that lead to self-stimulation: the homeorhetic turning on and off of
pleasures and pains. At the top of the hierarchy of brain structures that
enable feelings and expressions of emotion and motivation is the cortex of
the frontal lobes.

Labeling our feelings as fear, anger, hunger, thirst, love or lust
depends on our ability to identify and appraise this environmental “what”
or “who,” and such identification depends on secondarily involving some
of our other brain systems, which include the brain cortex.

The chapters hereafter deal with the importance of the cortex in
enabling the fine grain of emotional and motivational feelings and
expressions that permit us to weave such episodes into a complex,
discerning narrative.

The Middle Basal Ganglion
I have said nothing about the middle basal ganglion, the globus

pallidus, because little is known. Clinically, malfunctions of the middle
basal ganglion have been held responsible for Tourette’s syndrome, the
occurrence of tics that are sudden outbursts of uncontrollable movements
or speech acts, often exclamations of obscenities. Fortunately, these
embarrassing intrusions become less frequent as the person grows older.
On the basis of this clinical evidence, one can venture the conjecture that
the middle basal ganglion when functioning normally mediates, at a
primitive level, the relation between an emotional hang-up and an attempt
at a motivated “getting into practical relation with the environment.”

Such mediation is provided at a higher level, that is, with more
cortical participation, by hippocampal processing. The experiments in my
laboratory as well as those of others, described in Chapter 15, show that
the activity of the hippocampus accomplishes efficient mediation between
emotions and motivations by processing the episode, the context, the
familiar within which attended behavior is occurring.

Epilepsy



Before the advent of EEGs, the diagnosis of temporal lobe epilepsy
was difficult to make. I had a patient who had repeatedly been violent—
indeed, he was wanted by the police. He turned himself in to the hospital
and notified the police of his whereabouts. I debated with myself as to
whether he did indeed have temporal lobe epilepsy or whether he was
using the hospital as a way to diminish the possible severity of his
punishment. The answer came late that night. The patient died in status
epilepticus during a major seizure.

Fortunately, most patients with temporal lobe epilepsy do not become
violent. On another occasion, decades later, I was teaching at Napa State
Hospital in California every Friday afternoon. After my lecture, one of the
students accompanied me to my car, and I wished her a happy weekend.
She was sure to have one, she said: she was looking forward to a party the
students and staff were having that evening. The following Friday, after
my class, this young lady and several others were again accompanying me
to my car. I asked her how she had enjoyed the prior weekend’s party. She
answered that unfortunately she’d missed it—she had had a headache and
must have fallen asleep. The other girls immediately contradicted her:
they said she’d been at the party all evening—had seemed a bit out of it,
possibly the result of too much to drink. Such lapses in memory are
characteristic of temporal lobe epilepsy. The young lady was diagnosed
and medicated for her problem, which eliminated the seizures.

An oft-quoted set of experiments performed in the early 1960s by S.
Schachter and J. E. Singer at Columbia University showed that
experienced feelings can be divided into two separate categories: the
feelings themselves and the labels we place on those feelings. These
experiments were done with students who were given an exam. In one
situation, many of the students (who had been planted by the
experimenter) said how easy the questions were, turned in their exams
early and also pointed out that this was an experiment and would not count
on the students’ records.

Another group of students was placed in the same situation, but these
planted students moaned and groaned, one tore up the test and stomped out
of the room, another feigned tears. Monitors said that the exam was given
because the university had found out it had admitted too many students
and needed to weed some out.



When asked after the exam how each group felt about it, and what
their attitude was toward the experimenters, the expected answers were
forthcoming: the “happy” group thought the exercise was a breeze; the
“unhappy” group felt it was a travesty to be exposed to such a situation.

As part of the experiment, some of the students had been injected
with saline solution and other students with adrenaline (as noted, the
adrenal gland is controlled by the amygdala.) Analysis of the result
showed that the adrenaline-injected students had markedly more intense
reactions—both pleasant and “painful”—than did the saline group.

The label we give to a feeling that has been engendered is a function
of the situation in which it is engendered; its intensity is a function of our
body’s physiology, in the case of this experiment, a function of chemistry,
the injection of adrenaline that influenced the amygdala/ adrenal system.

Understanding What I Found
The results of the experiments described in this chapter show that the

“what is it?” response is just that. It is a capturing of attention, a stopping
of an episode of ongoing processes, an alerting. Stimuli that stop ongoing
behavior produce an increase in uncertainty, thus arousal, interest,
approach, avoidance and panic. The “what is it?” reaction occurs when an
experienced event is novel. Repetition of the event leads to
familiarization. Familiarization establishes a representation of the
experienced event.

Familiarization fails to occur in the absence of visceral responses.
Visceral and endocrine responses are necessary to the formation of a
representation of the event, a memory. Emotional feelings result from the
activation of a memory, not directly from a visceral input, as suggested in
earlier theories. We do not experience an emotion when we have a
stomach-ache unless it conjures up memories of possible unpleasant visits
to doctors or even surgery. Visceral and endocrine activation follows a
mismatch between what is familiar and the novel input. If the mismatch to
what is familiar to us is great, we will experience the mismatch as
disturbing, even as panic. These reactions occur along a single dimension
of intensity: electrical stimulation of the amygdala in humans and animals
produces alerting, interest, engagement, retreat and panic. Emotions result



from the processing of familiarity. Visceral reactions are essential in
gaining familiarity, a form of memory.

Visceral/endocrine inputs, ordinarily called “drive stimuli,” play a
central role in organizing memory, the retrospective aspects of emotions
and motivations. However, the impact of these “drive” stimuli on the
formation of the memory is shared by attention to the environmental
context, Freud’s caretaking person, within which the emotional and
motivational feelings are occurring.

Having completed several drafts of this chapter, it occurred to me
that the results of my experiments have shown that emotions serve as
attractors; emotions are the prospective aspect of a hedonic
(pleasure/pain) memory process just as motives are in Freud’s
memory/motive theory. The action-based memory/motive process
involves the upper basal ganglia (the caudate and the putamen). Freud did
not know about the functions of the limbic basal ganglia—in fact, neither
did I, nor did my teachers when I was in medical school and during my
residencies. The familiarization/ emotional process parallels the
readiness/motivational process and involves the limbic basal ganglia (the
amygdala and accumbens). Emotion is the attractor—the prospective,
attentional, intentional, or thoughtful aspect of processing familiarity.
Typically, as the saying goes when plucking the petals of a daisy: we
attempt to assess whether he loves me; he loves me not—or will she;
won’t she. It is the conscious or unconscious emotional possible prospect
(maybe next time it’ll work) that accounts for “hang-ups” and for
repeatedly returning into less than satisfactory relationships.

Labeling our feelings as fear, anger, hunger, thirst, love or lust
depends on our ability to identify and appraise this environmental “what”
or “who,” and such identification depends on secondarily involving some
of our other brain systems, which include the brain cortex.

In Summary
1. Motivations get us into practical relations with our environment.

They are the proactive aspect of an action-based memory processes
mediated by the upper basal ganglia: the caudate and the putamen.

2. Emotions stop at the skin. Emotions are hang-ups. They stop ongoing
behavior. Emotions are based on attending to novelty which increases



our uncertainty. Thus, we alert to an unexpected occurrence. We may
become interested in such an occurrence and become “stuck” with
this interest. More often, we become bored, habituated. Habituation,
familiarization occurs as the result of a change in hedonic memory
processes initiated in the limbic basal ganglia. Familiarization
depends on visceral activation. Emotions include the prospective
aspects of this change in memory.

3. Electrical stimulation of the amygdala of animals and people has
produced, in ascending order of the amount of stimulation (applied
randomly), reactions that range from a) momentary to prolonged
interest; b) to approaching conspecifics as in sexual mounting or
flirting; c) to retreating from the environment; and d) to outbursts of
panic which may lead to attack, labeled “sham rage.”

4. In short, although processing the content of an episode (the familiar)
is highly specific, the response engendered varies along an intensive
dimension reaching from interest to panic and rage.



Chapter 14
Values

Wherein I chart the brain processes that organize the manner in which we
make choices.

Motives like to push us from the past; values try to draw us into
the future.

—George A. Miller, Psychology: The Science of Mental Life, 1962

The empirical relations that determine the value of a piece of
currency depend, in part, on the utility of that piece of currency
for that particular individual. . . . Two interrelated classes of
variables have been abstracted by economists to determine
utility: demand and expectation; two similar classes have been
delineated from the experiments reported here—each of the
classes related to a distinct neural mechanism. [In addition] a
still different neural mechanism has been delineated whereby
preferences among values can be discriminated.

—Karl H. Pribram, “On the Neurology of Values,” Proceedings of the 15th
International Congress of Psychology, 1957



Preferences and Utilities
My claim in the previous chapter was that the amygdala and related

systems are involved in familiarization, a form of memory, the prospective
aspects of which form the basis of the intensities of our feelings and their
expression. This claim raises the issue as to how familiarization is based
on assessing and evaluating a situation in the world we navigate. An
unexpected experimental result that we obtained in my laboratory
addressed this issue. The experiment was a simple one, aiming to answer
the question: Would a monkey who put everything in his mouth (and ate
what was chewable) choose the edible foods before choosing the inedible
objects when both were presented together on a tray? Before surgery, the
monkeys had chosen mainly the edible objects; each monkey had shown a
stable order in what had become familiar, that is, in what he preferred.
Much to my surprise, after removal of the amygdala, this order of familiar
preferences remained undisturbed—but the monkeys would now go on to
take non-food objects and try to chew on them until all the objects were
gone from the tray. Their “stop” process had become impaired.

64. Object testing board

In 1957, I presented the results of these and related experiments at the
16th International Congress of Psychology in Bonn, Germany. My text was
set in terms of economic theory—especially as developed by the thesis of
“expected utility” that had been proposed in 1953 by the mathematicians
John von Neumann and O. Morgenstern. I had become interested in how
we measure processes such as changes in temperature, and von Neumann
and Morgenstern had an excellent exposition of the issue. Also in their
book they talked about zero-sum games, a topic which became popular
during the Cold War with the Soviet Union. It was during these literary
explorations that my experimental results on preferences occurred, and



Von Neumann and Morgen-stern’s depiction of the composition of utilities
caught my attention. Wolfgang Köhler, friend and noted psychologist,
came up to me after my initial presentation of these results and said,
“Karl, you’ve inaugurated a totally new era in experimental psychology!”

I subsequently presented my findings at business schools and
departments of economics from Berkeley to Geneva. During the 1960s and
1970s at Stanford, I often had more students from the Business School in
my classes than from the psychology and biology departments. But despite
this enthusiastic reception from the economic community, the
inauguration of a new era in neuro-economics would have to wait a half-
century for fulfillment.

Desirability and Duration
In my presentations to psychology, biology and economics students,

and in publications thereafter, I suggested that the amygdala deals with the
“desirability” of what is being chosen, while the frontal cortex is involved
in “computing” the likelihood that the desired choice is available.
Desirability is an intensive dimension that determines how much effort we
are willing to expend in order to satisfy the desire. Desirability thus
determines demand. Both my experiments with monkeys and my
observations of humans revealed that the intensity of the desire for food,
as measured by the amount of effort the person or animal would expend, is
drastically lowered by amygdalectomy. The animals and patients ate more
not because they were momentarily “hungrier” but because they failed to
stop eating: their stop (satiety) mechanism had become impaired.

In the critical experiments demonstrating brain involvement in
estimating the likelihood of achieving a desired outcome, I performed a
“fixed interval experiment,” so-called because it uses a ”fixed interval
schedule” for presentation of a peanut to a monkey. I scheduled a waiting
period of two minutes between delivering each peanut to a place within the
monkey’s reach. During such a waiting period, animals and humans
usually start out with very few responses (in this case, pressing a lever).
The number of responses produced over a particular interval of time—
that is, the rate of responding—increases gradually during the two minutes
until just before the expected time of delivery of the peanut, when the
number of responses reaches a maximum.



65. Effect of removal of frontal cortex

66. Hunger deprivation has no effect.

In running this experiment, I noticed that the signal on my desk that
indicated when the peanut was being delivered was coming slightly earlier
as time went on. When I investigated, I saw my monkey’s hand reach out
of the hole in his cage through which the peanuts were delivered, just
seconds before the delivery period was up. The hand cleverly flicked the
delivery apparatus (which was modeled after a refreshment bar delivery
machine) into a premature delivery. The monkey timed his flicking so that
the early delivery was hardly noticeable! I wondered how long this ruse



had been going on and how many of my monkeys had been playing this
game. Fortunately, the problem was easy to fix—a narrow tunnel was
inserted between the hole in the cage and the peanut delivery machine.

The amount of studying done during a semester follows the same
pattern, which is called a “scallop”: at the beginning of each semester,
students are apt to take their course work rather lightly, but as the semester
goes forward, so does the amount of studying. In terms of economic
theory, I interpreted the scallop to indicate that the estimation of the
probability of being rewarded—the worry that we’ll miss the boat by not
studying, or miss the moment of the delivery of the peanut—“grows”
exponentially over the interval.

Lowering or raising the amount of food available to, and eaten by, a
monkey during the course of the experiment changes the rate at which he
responds—that is the number of responses he makes during the two-
minute interval between the delivery of each peanut—but does not change
the exponential shape of the scallop: another one of those unexpected
results. After removal of the frontal cortex, the rate of responding does not
change, but the scallop disappears. Two of my four operated monkeys
timed the expected delivery of the peanut so accurately that they needed
only one response (the others made fewer than five) to obtain a peanut; the
rest of the time they waited patiently or fiddled with the cage. This result
demonstrated that, contrary to what most neuroscientists believe, clock
timing is not disturbed by removals of the frontal cortex. Rather, it is a
sense of duration: how long, and how structured an experienced interval
seems.

67. Peanut dispensor for monkey experiment



In a classical (Pavlovian) conditioning situation, timing was so
accurate in these monkeys that if we changed the interval between the
conditional (signal) and unconditional (rewarding) stimulus, the animal
failed to adjust to the new situation. The monkeys’ estimation of duration
had been fixed so narrowly that they missed opportunities that were just
beyond the point of their fixation. As in the case of responding in the
fixed-interval experiment, their behavior had become fixated, as is shown
in human obsessive-compulsive disorders. The Greeks distinguished clock
time, kronos, from a sense of duration, kairos. Our brains also make this
distinction.

Preferences
But the most surprising finding in this series of experiments was that

the monkeys who had their amygdala removed showed no disruption of the
order in which they chose the items presented to them. Their preferences
were not disrupted. At the time when I was running my experiments,
experimental psychologists were discovering a surprising fact: preferences
were shown to depend on perceived differences arranged hierarchically.

In another set of experiments, my colleagues and I were showing that
the removal of the temporal lobe cortex biased (led) the monkeys toward
taking risks, whereas removal of the hippocampal system biased monkeys
toward caution. Preferences were shown to be arranged hierarchically
according to perceived risk: I prefer a Honda to a Cadillac; I prefer meat
to vegetables; and vegetables to potatoes. Thus, I was able to relate my
results on maintaining preferences after amygdalectomy to the monkeys’
intact ability to perceive differences. The amygdala are not involved with
preferring meat to potatoes, as my experiments showed.

By contrast, the intensive character of “desirability” is not arranged
hierarchically, as was shown by Duncan Luce of the University of
California at Irvine in his experiments on expectations. For instance, in his
discussions with me, he noted that desirability, and therefore demand,
changes dramatically, not according to some hierarchy, with changes in the
weather (packing for a trip to a location that has a different climate is
terribly difficult.) Desirability changes dramatically with familiarity (what
is exciting initially has become boring) and with recent happenings as, for
instance, when air travel became momentarily less desirable after the



attack on the World Trade Center in New York— airlines earnings fell
precipitously. It is practically impossible to rank (hierarchically)
desirability and to try to do so can sometimes get one in trouble: Never,
never attempt to charm your lover by ranking the desirability of his or her
characteristics against someone else’s. We either desire something or we
don’t, and that desire can change abruptly. Damage to the amygdala
dramatically alters a person’s or monkey’s desire for food or any of the
other Four Fs, not the ranking of preferences.

“Concilience”—What It Means
Recently a comprehensive review paper published in Science has

heralded a new approach to neuroscience: neuroeconomics. “Economics,
psychology and neuroscience are converging today into a single, general
theory of human behavior. This is the emerging field of neuroeconomics in
which concilience, the accordance of two or more inductions drawn from
different groups of phenomena, seems to be operating.” (Glimcher and
Rustichini, 2004)

The review presents evidence that when paradoxical choices are made
(those choices that are unpredicted from the objective probabilities, the
risks, for attaining success), certain parts of the brain become selectively
activated (as recorded with fMRIs). These studies showed that, indeed,
preferences, risks, involve the posterior cortex—a welcome substantiation
of my earlier conjecture. But the review, though mentioning von Neumann
and Morgenstern’s theorem, fails to distinguish clearly between
preferences and utilities, the important distinction that resulted directly
and unexpectedly from my brain experiments.

The recent studies reviewed in Science confirmed many earlier ones
that had shown that our frontal cortex becomes activated when the
situation in which our choices are made is ambiguous. This finding helps
explain the results, described earlier, that I had obtained in my fixed-
interval and conditioning experiments: The frontal cortex allows us a
flexibility, a flexibility that may permit us to “pick up an unexpected
bargain” which would have been missed if our shopping (responding) had
been inordinately “fixed” to what and where we had set our expectations.

Another recent confirmation of the way brain research can support
von Neumann and Morgenstern’s utility theory came from psychologist



Peter Shigzal’s experiments using brain stimulation as an effective reward.
He summarized his findings in a paper entitled “On the Neural
Computation of Utility” in a book on Well-Being: The Foundations of
Hedonic Psychology. Shigzal discerns three brain systems: “Perceptual
processing determines the identity, location and physical properties of the
goal object, [its order in a hierarchical system of preferences]; whereas . . .
evaluative processing is used to assess what the goal is worth. A third
processor . . . is concerned with when or how often the goal object is
available.”

The results of Shigzal’s experiments are in accord with the earlier
ones obtained in my laboratory and are another welcome confirmation.
Contrary to practices in many other disciplines, the scientist working at
the interface between brain, behavior and experience is most always
relieved to find that his or her results hold up in other laboratories using
other and newer techniques. Each experiment is so laborious to carry out
and takes so many years to accomplish that one can never be sure of
finding a “truth” that will hold up during his lifetime.

These recent explorations in neuroeconomics owe much to the work
of psychologist Danny Kahneman of Princeton University whose extensive
analyses of how choices are made received a Nobel Prize in economics in
2003. Kahneman set his explorations of how we make choices within the
context of economic theory, explicitly contributing to our understanding of
how our evaluations enable us to make choices, especially in ambiguous
situations.

Valuation
The important distinction I have made between preference and utility

is highlighted by experiments performed by psychologists Douglas
Lawrence and Leon Festinger in the early 1960s at Stanford. In these
experiments, animals were taught to perform a task and then, once it had
been learned, to continue to perform it. For example, a male rat must learn
to choose between two alleys of a V-shaped maze. At the end of one of
these alleys, which had been painted with horizontal stripes, is a female rat
in estrus. The alleys with the female at the end are switched more or less
randomly so that the placement of the female changes, but the striped
alley always leads to her. The rat quickly catches on; he has developed a



preference for the stripes because they always lead him to the female he
desires.

Once the rat knows where to find the female, does he stop? No. He
increases his speed of running the maze since he needs no more
“information” (in scientific jargon, reducing his uncertainty) in order to
make his choice of paths to reach her. But we can set a numerical “value”
on running speed, on how fast the male runs down the alley. This value
measures the level, the setpoint, of the male’s desire for the female.

Rewards (and deterrents) thus display two properties: they can
provide information, that is, reduce uncertainty for the organism, or they
can bias choices, place a value on their preferences.

The experiments performed at Stanford by Festinger and Lawrence
showed that the “laws” that govern our learning and those that govern our
performance are different and are often mirror images of one another. The
more often, and the more consistently, a reward is provided, the more
rapid the learning. During performance, such consistency in reward risks
catastrophe: when the reward ceases, so does the behavior, sometimes to
the point that all behavior ceases and the animal starves. In human
communities, such a situation arises during excommunication: a
dependable source of gratification is suddenly withdrawn, sometimes
leading to severe and incurable depression.

A slower and more inconsistent schedule of reward results in slower
learning, but it assures that our behavior will endure during the
performance phase of our experience. This is due to the buildup of the
expectation that the current lack of reward may be temporary.

The Reversal of Means and Ends
George Mace of the University of London pointed out that the

“mirror image” effects shown between a learning situation and the one
that exists during performance are examples of a more general means-ends
reversal in patterns of experience. Reversals occur between 1) the
acquisition of a pattern of behavior as, for instance, during learning, where
our behavior is experienced as means to an end, and 2) the reversal of the
pattern during performance, where our behavior becomes experienced as
an end in itself. His example suggests that such means-ends reversals are
ordinarily engendered by affluence:



What happens when a man, or for that matter an animal,
has no need to work for a living? . . . the simplest case is that of
the domesticated cat—a paradigm of affluent living more
extreme than that of the horse or cow. All basic needs of a
domesticated cat are provided for almost before they are
expressed. It is protected against danger and inclement weather.
Its food is there before it is hungry or thirsty. What then does it
do? How does it pass its time?

We might expect that having taken its food in a perfunctory
way it would curl up on its cushion and sleep until faint internal
stimulation gave some information of the need for another
perfunctory meal. But no, it does not just sleep. It prowls the
garden and woods killing young birds and mice. It enjoys life in
its own way. The fact that life can be enjoyed, and is most
enjoyed in the state of affluence, draws attention to the dramatic
change that occurs in the working of the organic machinery at a
certain stage. This is the reversal of the means-ends relation. In
the state of nature the cat must kill to live. In the state of
affluence it lives to kill. This happens to men. When men have no
need to work for a living there are broadly only two things left to
them to do. They can play and they can cultivate the arts.

In play the activity is often directed to attaining a pointless
objective in a difficult way, as when a golfer, using curious
instruments, guides a small ball into a not much larger hole from
remote distances and in the face of obstructions deliberately
designed to make the operation as difficult as may be. This
involves the reversal of the means-ends relation. The ‘end’—
getting the ball into the hole—is set up as a means to a new end,
the real end: the enjoyment of difficult activity for its own sake.

Addiction: A Tyranny of the Future
For years, learning theory was at center stage in the field of

experimental psychology. Performance had long been a neglected
stepchild. In my 1971 book Languages of the Brain, I called for a theory of
performance and named it, tongue in cheek, “addictionance” theory



because the process of addiction follows the very same rules as those that
are in effect during performance.

Gambling hustlers and drug dealers know how to get someone
addicted: the person is first readily provided with the substance which
then, inconsistently, becomes harder and harder to obtain, either because
opportunities become scarcer or because the price of the substance rises.
At the addiction center in Louisville, Kentucky, an attempt was made to
reverse this consequence by allowing a steady supply of substance to be
readily available, and then to cut down the amount in regular, controlled
stages rather than inconsistently. The technique was successful provided
no other, more fundamental problems, such as incipient mental illness,
had to be faced. For a while in England, addictive drugs could be had on
prescription. While these laws were in effect, the criminal aspects that are
produced by current law enforcement programs in the United States did
not occur. Earlier, in the 1930s, we had a similar experience when Franklin
Roosevelt ended prohibition of alcohol. The negative result of
enforcement seems to be a hard lesson for governments to learn; the
consequences of substance abuse overshadow a reasoned approach to it.

There are, of course, other factors that lead to addiction—factors such
as genetic predisposition and cultural setting. But we do have a
demonstration today of a situation in which addiction is avoided: when
physicians prescribe morphine for long-lasting pain, the patients are
readily weaned when the source of the pain is removed.

In Summary
My surprising experimental results obtained in exploring the effects

of amygdalectomy on expectation led to their interpretation in terms of
economic theory. I had framed the results of experiments on the amygdala
on changes in emotion and attention as indicating that emotions were
based on familiarity, a memory process, and that the prospective aspects of
familiarity controlled attention, and thus led to expectations. In turn,
expectations were shown to be composed of three processes that were
formed by three separate brain systems: preference, organized
hierarchically, formed by the posterior systems of the brain; estimations of
the probability of reward by the prefrontal systems of the brain; and
desirability, an intensive dimension, formed by the amygdala (and related



systems). This intensive dimension is non-hierarchical and ordinarily is
bounded—a stop process—by the context within which it occurs. The
composition of that context is the theme of the next chapter.



Chapter 15
Attitude

Wherein the several brain processes that help organize our feelings are
brought into focus.

“A wise teacher once quoted Aristotle: ‘The law is reason free
from passion.’ Well, no offense to Aristotle, but in my three years
at Harvard I have come to find that passion is a key ingredient to
the study and practice of law and life.

“It is with passion, courage of conviction, and strong sense of
self that we take our next steps into the world, remembering that
first impressions are not always correct. You must always have
faith in people, and most importantly, you must always have faith
in yourself.”

— Elle’s commencement address in the movie Legally Blonde



Efficiency: Remembering What Isn’t
When I first began to remove the amygdala, I did so on one side at a

time. Later, I was able to remove both amygdala in one operation. While
the initial single-sided removal took from four to six hours, my most
recent removals of both amygdala took forty minutes—twenty minutes per
side. I have often stood in wonder when a skilled artisan such as a
watchmaker, pianist or sculptor performs a task with an ease and rapidity
that is almost unbelievable.

My experience shows that efficiency in performance develops by
leaving out unnecessary movements. This chapter reviews the evidence
that the Papez Circuit, the hippocampal-cingulate system, has a great deal
to do with processing our experience and behavior efficiently, and how
efficiency is accomplished.

One of the Stanford graduate students in my class referred to the
hippocampus as “the black hole of the neurosciences.” It is certainly the
case that more research is being done on the hippocampus than on any
other part of the brain (except perhaps the visual cortex) without any
consensus emerging as to what the function of the hippo-campus actually
is. Part of the reason for the plethora of research is that hippocampal tissue
lends itself readily to growth in a Petri dish: such neural tissue cultures are
much easier to work with, chemically and electrically, than is tissue in
situ. These studies clarify the function of the tissue that composes the
hippocampus but does not discern its role in the brain’s physiology or in
the organism’s behavior.

Another reason for the difficulty in defining the function of the
hippocampus is that there is a marked difference between the
hippocampus of a rat, the animal with which much of the research has
been accomplished, and the hippocampus of a primate. In the primate, the
upper part of the hippocampus has shriveled to a cord of tissue. The part of
the hippocampus that is studied in rats is adjacent to, and receives fibers
from, the part of the isocortex that receives an input from muscles and
skin. Thus, removal of the hippocampus in rats changes their way of
navigating the shape of their space.

What remains of the hippocampus in primates is mainly the lower
part. This part is adjacent to and receives most of its input from, the visual
and auditory isocortex. Studies of the relationship of the hippocampus to



behavior have therefore focused on the changes in their patterns of
behavior beyond those used in navigating the shape of their space.

Just as in the case of research on the amygdala, almost all research on
damage to the hippocampus in primates, including humans, has devolved
on the finding of a very specific type of memory loss: the memory of what
is happening to the patient personally fails to become familiar. Again it is
memory, and familiarity, not the processing of information, or for that
matter of emotion, that is involved.

Fortunately, I have found a way to work through the welter of data
that are pouring into the “black hole” of hippocampal research: I can begin
with the results of research that my colleagues and I performed in my
laboratory. The most important of these was based on the observation that
when we navigate our world, what surrounds the target of our navigating is
as important as the target itself. When I steer my way through an opening
in a wall (a door), I am not aware of the wall—unless there is an
earthquake. I am familiar with the way rooms are constructed and, under
ordinary circumstances, I need not heed, need not attend the familiar. The
example of the earthquake shows, however, that when circumstances
rearrange the walls, the “representation” of walls, the memory, the
familiar has been there all along. The experiments we performed in my
laboratory showed that the hippocampus is critically involved in “storing”
that familiar representation.



68. Afferents to Hippocampus

69. Efferents from Hippocampus

However, the processing of what is familiar is different for the
hippocampal system than for the amygdala system. A series of
experiments in my laboratory demonstrated that while the amygdala is
processing what is novel during habituation, the hippocampus is
processing the context within which habituation is happening: the
hippocampus is processing what is already familiar. In a learning task,



when a monkey is choosing between a “plus” and a “zero” and choosing
the “plus” is always rewarded irrespective of where it is placed, the
amygdala is processing choosing the “plus,” the hippocampus is
processing not- choosing the “minus,” the non-rewarded cue. This was a
surprising result for our group until we realized that in order to walk
through a door we must process the walls so as not to bump into them. Our
experience with familiar walls has become, for the moment, ir-relevant, no
longer “elevated” in attention. We no longer are interested in walls unless
an earthquake occurs, in which instance our experience of the walls
promptly dis-habituates and re-elevates into attention. The memory of
walls and of the minus cue does not disappear during habituation and
learning.

In another experiment, we recorded the electrical activity occurring
in the hippocampus of monkeys trained to perform a “go no-go” task: on
every other trial they were rewarded with a peanut for opening a box. On
the intervening trials they had to refrain from opening the box. As
expected, the electrical activity of the hippocampus was very different
when recorded during the two responses.

We next trained the monkeys on a “go-right go-left” task in which
each trial was rewarded: the monkeys had to choose between two boxes,
one marked with a plus sign and the other with a square. The box with the
square always had a peanut in it. The monkeys quickly learned the task
and got themselves a peanut on every trial. Our surprise came when we
analyzed the recordings of the electrical activity from the hippocampus:
the electrical activity was identical to that obtained when the monkeys had
had to refrain from opening the box in the previous task!!! When choosing
between a plus and a square, the choice could be based on either a positive
or a negative choice: “The plus always signifies a peanut” or “The square
always signifies no peanut.”

Actually, we showed that in ordinary circumstances, the monkeys
attend, over the course of the experiment, both the “always a peanut” and
“always no peanut” boxes when performing these tasks. But the brains of
our monkeys sorted out which box is being attended, the amygdala
responds to choose “always a peanut;” the hippocampus to choose “always
no peanut.”

In several other experiments we showed that the hippocampus is
important in attending these “always no peanut” aspects of a task. For



instance, the number of such “always no peanut” boxes on any given trial
is important to monkeys whose hippocampus is intact, but not to monkeys
whose hippocampus has been removed.

A slightly different but related effect of hippocampal removal was
shown in a task in which the “always a peanut” was shifted from a panel
with a plus sign to a panel with the square sign. The monkeys who had had
their hippocampus removed learned that the shift had occurred as quickly
as did the monkeys with their hippocampus intact—but once they had
attained a 50% level of obtaining a peanut, they just continued at that level
of performance. It seemed to me that these monkeys acted as if they were
satisfied with being rewarded half the time without worrying or thinking.
They were no longer motivated to do better. After a few weeks at this level
of performance, all of a sudden the monkeys “snapped out of it” and went
on to do as well as did the monkeys with their hippocampus intact. The
slopes of achieving 50% and 90% were normal. But during the long period
when they were at the 50% level, they seemed not to “want to exert the
extra effort” to do better.

My interpretation was that at the 50% level of reward the new build-
up of “not-choosing” had not proceeded sufficiently to counter the context
of “not-choosing” from the pre-reversal stage of the experiment to kick
the monkey into a more productive motivational phase. Support for this
interpretation came from the finding that over progressive reversals the
mid-plateau of 50% performance became progressively shorter.

Ordinarily, when we walk through doors, we do so efficiently; we
don’t need to expend any effort scanning the walls for an exit. Come an
earthquake, and we immediately make that effort by “paying” attention in
order to find the safest possible place to shelter.

Experiments performed with normal rats have shown that, given
several places where they might find food, they will quickly learn which
places they have already found (and emptied of) food. They do not return
to those now “always no food” locations. By contrast, rats who have had
their hippocampus removed do return to such locations—over and over
and over. In my laboratory, we obtained the same results with monkeys
using different non-spatial patterns of “reward” and “no reward:” the
monkeys who have had limbic system removals return—over and over and
over—to the “no reward.”



Much of the experimentation on the effects of hippocampal removals
on behavior has focused on the spatial location of the unrewarded cues that
animals continue to return to after surgery. As noted earlier, this is in large
part due to the use of rats in most of the experiments and that the dorsal
hippocampus is the site of the resections. The dorsal hippocampus is
adjacent to, and connects primarily with, the somatic sensory and motor
cortexes. In primates, the dorsal hippocampus has become vestigial (now
it is a sliver of tissue called the “induseum griseum”). In primates the site
of resection is the ventral hippocampus, as in my experiments. The ventral
hippocampus is adjacent to, and connects primarily with, the visual and
auditory cortex. Thus we can expect the function of the ventral
hippocampus to be more “cognitive” than that of the dorsal hippocampus.

70. Returns to No-Reward Cues: Control Subjects



For decades, the results of these experiments on non-spatial cognitive
deficits were ignored, but this is now changing: for instance, a study
performed in David Gaffan’s laboratory published in The Journal of
Neuroscience (Wilson, Charles; Buckley, and Gaffan, November 2007)
showed a similar impairment in learning randomly changing object
discriminations.

A recent (1999) book by A. D. Redish, Beyond the Cognitive Map:
From Place Cells to Episodic Memory, affords a comprehensive, critical
and detailed review of the navigational functions of the hippocampus. The
book comes to the conclusion that the hippocampus is important to our
ability “to reset an internal coordinate system” and that this process is
critical to both navigation and episodic memory. To me the book is a most
worthwhile endeavor as it sets out an attractor network theory of how
context resetting can occur. From my vantage, however, the context is not
just a place code, as Redish and most of the scientific community continue
to suggest; rather, the context is constructed by attending to any context
that is “what isn’t,” what at the moment is not the focus of navigating our
world.



71. Returns to No-Reward Cues: After Brain Surgery

Holographic Aspects of Hippocampal Processing
In order for the animals with an intact hippocampus to “not respond” to
where, or in which trials, there was “always no reward,” a representation
of prior experience has to be addressed. The nature of this representation
has been well worked out: it is distributed. The cellular architecture of the
hippocampus contains several webs rich in fine fibers that can serve as a
holographic-like process. Some quotations from the 1970s of the work of
John O’Keefe of the University of London, one of the most influential
scientists working on the hippocampus, highlight the holographic nature of
the hippocampal representation:

Attempts to gain an idea of the way in which an
environment is represented in the hippocampus strongly suggest



the absence of any topographic isomorphism between the map
and the environment. For example, cells recorded next to each
other . . . are as likely to have their major fields in different
parts of the environment as in neighboring parts. Furthermore, it
appears that a small cluster of neighboring . . . cells would map,
albeit crudely, the entire environment. This observation . . .
suggests that each environment is represented many times over
in the hippocampus, in a manner similar to a holographic plate.
In both representation systems the effect of increasing the area
of the storage which is activated is to increase the definition of
the representation.

A second major similarity between the way information can
be stored on a holographic plate and the way environments can
be represented in the hippocampus is that the same hippocampal
cell can participate in the representation of several
environments . . . . There was no systematic relationship amongst
the fields of the same neuron in the different environments. One
can conclude that each hippocampal place cell can enter into the
representation of a large number of environments, and
conversely, that the representation of any given environment is
dependent on the activity of a reasonably large group of
neurons.

The third major similarity between the holo-graphic
recording technique and the construction of environmental maps
in the hippocampus is the use of interference patterns between
sinusoidal waves to determine the pattern of activity in the
recording substrate . . . . Pioneering work by Petsche, Stumpf
and their colleagues (in Vienna) showed that the functions of
[hippocampal cells] was to translate the amount of activity
ascending from various brainstem nuclei into a frequency
moduled code. [Hippocampal cells] fire in bursts, with a bursts
frequency which varies from 4 to 12 Hertz. Increases in the
strength of brainstem stimulation produced increases in the
frequency of the bursts but not necessarily in the number of
spikes within each burst.



These important experimental results resolved any remaining
reservations I had had in accepting the idea that there are
“representations” of our experience “stored” in the brain. However, the
representation in the brain has no resemblance to the experiences we
encounter in navigating our world. Rather, the representations of our
experience become to some extent distributed everywhere and every-when
much as the patterns that become distributed in the holographic process.
As in perception, the representations in the hippocampus must be
transformed to be meaningful and of use in navigating our world.
Transformations are carried out in the following manner:

Hippocampal Processing Mediates Novelty and
Readiness

There are two kinds of electrical rhythms that can be recorded from
the hippocampus. (This is true of primates— where the distinction isn’t
obvious—as well as of non-primate species: in my laboratory we used
computer analysis to tease the two rhythms apart.) One type of rhythm
occurs when the animal alerts to a novel stimulus; the other occurs when
the animal explores its environment. Most likely, when we alert to novelty
(the “what is it?” experience) we encode that event into a holographic
representation. When we then proceed to the “what to do?” phase, we
engage the hippocampal rhythms that are activated when we explore our
environment. In short, the “stop” process creates a holographic-like
representation; in turn, the “go” process transforms that representation
into a readiness to engage in practical space-time relations in navigating
our world.

The transformations that take place by virtue of hippo-campal
function are, therefore, parallel to those that are performed in the
perception/action systems. Hippocampal activity performs these
transformations by bringing together and modulating the activities of the
limbic basal ganglia “stop” processes with those of the upper basal
ganglia “go” systems to assure the most efficient use of these processes.

Stress



Efficient processing of our experience depends on balanced metabolic
processing. Stress has received bad press in this regard. True, excessively
prolonged stress can be deleterious to our health and well-being. But
without any stress we would probably eat and sleep most of our lives away.
The hippocampus regulates the pituitary gland’s secretion of steroids, the
hormones secreted by the outer part, the cortex, of the adrenal gland. The
adrenal gland is composed of this outer cortex and an inner core, which
secretes adrenalin, also known as “epinephrine.” This core secretion is
regulated by the amygdala. Thus the functional anatomy of the brain
reflects the functional anatomy of the adrenal gland.

Adrenalin mobilizes our stop processes, the “what is it?” attentional
process. The amount of adrenalin secreted depends on what we experience:
if being stopped in our tracks is frustrating, the intensity of our
experience, abetted by adrenalin, increases.

When we activate our “go” processes, the processes that get us into
practical relations with our environment, we are apt, in our society, to push
ourselves to exhaustion. But more normally, hippocampal activity
efficiently controls the adrenal steroids (which are closely related to the
sex hormones) and ordinarily protects us from exhaustion. As every
athlete knows, and those of us who have been administered steroids
medically have experienced, they make us feel great. (Of interest in this
context is the fact that the steroids and the endorphins are “cut” from
adjacent portions of a very long protein molecule.)

The effect of hippocampal processing on motivation thus provides a
context of specific patterns that are currently “irrelevant” but at the same
time allows the motivation to be expressed along an intensive dimension:
from feeling fine, feeling great, feeling stressed, feeling overwhelmed to
feeling exhausted. Thus the hippocampus works for the expression of
motivations much as does the amygdala for the expression of emotions.

Bringing It All Together
In order to describe how brain processes are involved in organizing

our emotions and motivations, we need to distinguish the names we use to
talk about these processes. To begin with, in our common usage of the
English language, we fail consistently to differentiate between emotions
and feelings. I use the term “feelings” to describe both emotions and



motivations, making an important distinction that goes back to William
James. I have adopted James’s definition: Emotions stop at the skin;
motivations (which were called “instincts“ in his day) get into practical
relations with the organism’s environment. Emotional feelings are
generated when our bodies and brains cope internally with pain and
pleasure. Motivational feelings concern our actions and the systems of the
brain involved in starting and maintaining coping behavior. Feelings
encompass our experiences both of emotion and of motivation: we feel
happy, sad, or irritable; we feel gung ho to write a chapter, to plan dinner,
to travel to a meeting.

Furthermore, in English we also use the word “feeling” to describe
tactile (and visceral) sensations. Our language often embeds much
wisdom. In close, intimate relationships we express our feelings of
emotion, motivation and attitudes best by touching and hugging—using
the tactile systems to communicate our emotional and motivational
feelings.

As described in Chapter 10, tactile and visceral sensations take two
routes from receptors to the brain. Disturbances of one route lead to loss
of the ability to localize the origin of the sensation, say a pinprick, in the
place where it occurs and the time when it happens. Disturbances of the
other route result in changes in the hedonic—pleasant and enjoyable or
unpleasant and painful—aspects of the sensation. The hedonic route
organizes the brain processes that lead to our feelings of emotion and
motivation. The difference between the localized processing of feeling and
the hedonic processing of feelings can be stated simply: The knife feels
sharp vs. I feel sharp today.

A chemical theory of the production of moods and emotions had
guided the medical profession for centuries: the chemicals were called
“humours,” substances presumably secreted by certain viscera. In a much
more sophisticated form, the chemical theory has emerged in what
Candice Pert of Georgetown University has aptly named in her 1997 book
Molecules of Emotion: Why You Feel the Way You Feel. The molecules
include not only the simple transmitters adrenalin and acetyl-choline used
by the autonomic nervous system, but also longer molecules, such as the
catechol and indole (such as serotonin and histamine) brain amines. As
noted above, the adrenal and sex steroids are involved in feeling great or
feeling stressed. Even longer chain molecules such as oxytocin and



vasopressin have been shown to regulate interpersonal attachments,
including pair bonding.

A current version of William James’s visceral theory of emotions
needs, therefore, to become conjoined with the facts of chemical
involvement in producing states of well-being and social engagement, and
their disturbances. These ancient and modern insights led me to agree with
Walter Cannon that much more is involved in generating emotions than
the body’s viscera. During the 1920s and 1930s, Walter Cannon had shown
that the removal of those nerves conveying the signals from the viscera to
the brain does not affect our experience or expression of emotion. Cannon
further suggested that the thalamus, the last way station of input to the
brain cortex, organizes our emotional experience and that the
hypothalamic region, located just under the thalamus, controlled the
expression of emotions such as fight and flight. My experiments showed
that electrical stimulation not only of the hypothalamic but also of the
limbic forebrain in human and animal subjects produces visceral changes
and—as in Cannon’s experiments—arousal, interest and even rage.

However, sensing visceral or chemical activation per se is not what
determines our experience or our expression of an emotion. Feeling an
upset stomach or a cocaine rush are not emotions. Rather, the results of
the experiments performed in my laboratory, based on Sokolov’s
experiments, demonstrated definitively that visceral activation is
important not to the sensing of an emotion per se, but to habituation, to
familiarization, the formation of an episode, certain type of memory and,
furthermore, to attending to novelty, a change from the familiar.

The question arose, therefore: What do the experiences of
familiarization and novelty have to do with emotion? The answer came
when I realized that I could think of emotion not solely as activating
visceral and chemical processes but also as memory based. It was Freud’s
memory-motive concept that had attracted me to his Project for a
Scientific Psychology. Motivation considered within the context of
processing the prospective aspect of memory (readiness) is a much richer
concept, and accounts for a greater range of data than a purely drive
(reduction or induction) based concept of motivation. Emotion considered
within the context of processing the prospective aspect of memory
(familiarity) is a much richer concept than one based solely on visceral
and chemically based theories and can account for the surprising fact that



practically all research into the psychophysiology of emotion has resulted
in data pertaining to attention and memory. The type of memory involved
in motivation involves the upper basal ganglia (caudate and putamen). The
type of memory involved in emotion enlists the limbic basal ganglia
(amygdala and accumbens).

Attitude
A comprehensive “attitude” theory of emotion and motivation can be

constructed on the basis of the results of these experiments and the
insights derived from them. Interestingly, Papez’s intuitions regarding his
views were on the mark: he published a chapter summarizing his
anatomical circuit, and what he thought it signified, in 1969 in Nina Bull’s
book The Attitude Theory of Emotion (in which she included motivation).

As summarized in the current chapter, attitudes are formed from the
prospective processing of memories: attitudes are motivational and
emotional. The attitude theory is rooted in earlier ideas of William James
and Sigmund Freud, and especially of James Papez and Nina Bull. Attitude
became an important concept in social psychology during the 1920s. Floyd
and Gordon Allport insisted, however, that attitudes were processes
attributable to individual persons, processes that become of social
consequence when the individuals interact. Gordon Allport once wrote a
handwritten note to me suggesting that I was wasting my time working
with monkeys; I should better devote myself to the study of human
behavior. He would be surprised at what we have learned about “attitude”
from those experiments on non-humans.

The Structure of Attitudinal Patterns
When Floyd Allport retired, he came to Stanford and regularly

attended my weekly lab meetings. He had more recently written his
Theories of Perception and the Concept of Structure (1955). We ardently
discussed his theory of event-structure, but the research on Gabor
functions and patch holography was still in the future. It would be
wonderful to be able to discuss the issues of event structure and attitude
with Floyd and Gordon, given what we know today.

Our discussion might well look something like this: Attitudes are
dispositional states that embody the experience of a person in compressed



form. Attitudes are formed and reformed (reset) by structuring
redundancy: each level of repetition is re-presented at the next level by a
single symbol—and this process continues until all levels have been
accounted for. In short, attitudes embody complexity—the construction of
codes. Isocortical input becomes processed by the hippocampus, is then
forwarded back to the isocortex via the basal ganglia (both upper and
limbic) to be interleaved into ongoing processing.

In Summary
The attitude theory of emotions and motivation, based on the

experimental and theoretical advances reviewed above, can be
summarized as follows.

1. Pain, as well as pleasure, is processed by homeorhetic systems that
surround the ventricles of the brain.

2. Emotions and motivations are processes that cope with pain and
pleasure.

3. Emotional processing stops at the skin. Motivational processing gets
into practical relations with the world we navigate.

4. Emotions and motivations are based on a two-stage attentional
mechanism: “what is it?” and “what to do?”

5. Emotional and motivational processing involves the basal ganglia of
the forebrain. Emotional processing centers on the limbic basal
ganglia: the amygdala and accumbens and related systems.
Motivational processing centers on the upper basal ganglia: the
caudate and the putamen.

6. The limbic basal ganglia are involved in familiarization, the
processing of an attended stimulus into an episode, a memory. The
process activates a visceral reaction. No visceral reaction, no
familiarization; no episode, no memory.

7. Attention—“what is it?”—is captured by a change in the arrangement
of a familiar pattern, that is, by a novel stimulus, an event. Novelty
stops ongoing processes. Repetition begets habituation, that is,
familiarization.

8. The intensity of a response to novelty varies from alerting, through
interest, withdrawal, and panic, according to what aspect of the
familiar is activated by the novel stimulus.



9. Three classical brain theories of emotion—the chemical, the visceral
and the activational—are encompassed by the attitude theory.
However, contrary to the visceral theory, visceral responses, our gut
feelings, are critical in restructuring what is familiar, the
restructuring of an episode, a memory: visceral activation is not per
se the immediate source of an emotion.

10. Motivation involves the upper basal ganglia: the caudate and the
putamen. The involvement of these structures is not limited to
organizing postural readiness, but also includes attending, the control
of sensory input.

11. The processing of the familiar utilizes the hippo-campus. The
electrical activity in the hippocampus indicates that the familiar
becomes, on repetition, the “always not the target,” the seemingly
“irrelevant” context in a situation.

12. Hippocampal activity provides efficiency to our processing of
emotion and motivation. This is accomplished by eliminating
attention to what is repetitively encountered. The processing becomes
automatic (unconscious.) This entails encoding a hierarchical
structure of redundancy. Thus, attitudes embody complexity; attitudes
embody the processes that embody codes. In this way, hippo-campal
processing efficiently brings together the response to novelty, the
“stop” emotional process, with the “getting into practical relations
with the world we navigate,” the motivational “go” process. In short,
hippocampal activity automatically determines the codes that form
our attitudes as we navigate our world.



Chapter 16
The Organ of Civilization

Wherein I explore the executive processes of our brain.

The frontal “association areas” [of the brain], sometimes
referred to as “the organ of civilization,” are intimately
connected with the limbic systems to form an internal core of the
forebrain. This most forward portion of the primate frontal lobe
appears to us to serve as a “working memory” where Plans can
be retained temporarily when they are being formed, or
transformed, or executed.

—George Miller, Eugene Galanter and Karl H. Pribram, Plans and the
Structure of Behavior, 1960

There is some danger that the neuropsychologically oriented
behavioral scientist may lose himself in the wealth of data and
free ranging speculation that are now possible to him. But this
danger is counterbalanced by the promise of a fresh view of
mankind. Western thought has alternated between two views of
man’s relation to his universe. One view holds that he is an
essentially passive organism shaped by the exigencies of his
environment. The other [currently re-emerging] emphasizes his
active role, manipulative and selective not only of artifacts but
of sense data as well. The American neuropsychological
contributions to behavioral science point to a resurgence of the
dignity of man as a scientific as well as a political and
humanistic tenet.

—Karl H. Pribram, “Neuropsychology in America,” in The Behavioral
Sciences Today, 1963



An Unexpected Finding
As I was moving from Stanford to Virginia in 1989, I was asked by

the editors of the journal Developmental Psychobiology to mediate a
controversy in the human brain electrical recording (EEG) literature. The
controversy centered on whether or not brain electrical activity mirrored
Piaget and Inhelder’s decades-long observations that the development of
cognitive processing occurs in distinct stages.

With my Stanford laboratory colleague Diane McGuinness, I had just
published a paper entitled “Upstaging the Stage Model,” in which we
proposed that the distinctiveness of such stages comes about when a child
is simultaneously learning many new skills and developing innate
capacities. We also claimed that stages can be discerned in adults
whenever they tackle a totally new set of problems. Such stages include a)
an initial “hands-on” effort to get acquainted with the problem, b) a period
of asserting control over the variety of factors involved, and c) a final
stage of more or less skilled performance. In literary circles, these stages
compose the initial part of what is called a “hermeneutical circle.”

In my course on “Brain in the Context of Interpersonal Relations,”
which I’ve been teaching for half a century, I compare the Piaget stages
with the stages of Freud’s clinical theory and Harry Stack Sullivan’s theory
of interpersonal development. Most professional psychologists are
shocked by the juxtaposition of these three “stage” theories, but I have
come to this juxtaposition with authority: In the early 1960s, I happened to
be in New York on the occasion of a lecture by Piaget at the William
Allison White Psychoanalytic Institute. I was totally surprised when
Piaget made the case that Freud’s oral, anal and sexual
“emotional/motivational” stages coincided with Piaget’s own “cognitive”
developmental stages. After the lecture, Piaget, Inhelder, his life-long
associate, and I had dinner together. When I expressed my surprise, and
noted that associating his own beautifully research-based theories with
Freud’s somewhat messy ones might be unwise, Piaget assured me that he
was serious. (This was before my own explorations of Freud’s Project.)

As a result of this exchange, I looked further into Freud’s clinical
theory and then added what I had learned from my colleague Gregory
Bateson, and from the group at the Palo Alto Psychiatric Institute, with
respect to Sullivan’s interpersonal theory. Given my interest in form,
especially form as pattern, I zeroed in on a critical point: Content may



vary, but the patterns of development appear to be similar for both
interpersonal (emotional/motivational) learning and for cognitive and skill
learning—and these “stages” are not limited to childhood and puberty. The
stages of our development are repeated whenever we tackle a totally new
endeavor.

I now return to the invitation I received in 1989 from the journal
Developmental Neurobiology to mediate the arguments regarding the
occurrence of developmental “stages” in brain electrical (EEG)
recordings. There is good anatomical evidence that the ages at which large
tracts develop in the human brain are coincident with changes in brain
electrical recordings. We can thus use the brain electrical recordings as
indicators of brain maturation.

My first step was to enlist my colleague William Hudspeth, a
psychologist and an expert in the analysis of human brain electrical
activity, to help gather all the published data that might be relevant to
settling the controversy as to whether there are Piagetian “stages’”
recognizable in the EEG. We next looked at the time segment over which
the recorded EEG had been used by various experimenters to determine a
“stage.” Some segments were as short as six months and others as long as
two years. Hudspeth and I chose an age group confined to a one-year
segment to bring the data into register. Within such an age group we had a
uniform segment over which we could average the data from all the
reports. That helped to eliminate some of the discrepancies.

But more important, the data from various laboratories around the
world averaged their EEG samples over the entire skull. All my previous
work had aimed at showing the differences between the behavioral
processes to which different brain systems contributed. In our attempt to
resolve the “stages” controversy, Hudspeth and I, therefore, divided the
human EEG recordings that we had collected according to six locations
across the skull.

These two maneuvers—using a standard time segment and dividing
the recordings according to location on the skull—allowed us to show that
the Piagetian stages were mirrored in the changes in local electrical
recordings. We found that the distribution of patterns of maturation of the
EEG across the skull were somewhat complex, but that in general these
patterns supported a maturation stage model which loosely fit the



Piagetian stages. The editors of Developmental Psychobiology felt that the
controversy had been resolved and published our findings.

As so often happens during experimental research, we also came up
with an unexpected, surprising finding —that maturation of the EEG takes
place during the late teenage years and that this maturation occurs in the
frontal region of the head. We found that there is a marked “growth spurt”
over the most forward part of the frontal lobe between the ages of 17 and
21 years. These are the years that kids go to college—and are sent off to
fight our wars. Two decades after we published our results, the judiciary
has taken note of our findings and other supportive evidence to debate
whether late teenagers should be held responsible as adults for their
behavior. What makes this part of our brains so important that the law
should be concerned with its maturation? Why has this part of the brain
been called the “organ of civilization”?

The Key
The key to understanding the most anterior part of our frontal cortex

—the part that is usually called pre-frontal—is that it lies anatomically
between the classical (upper) and the limbic motor systems of our brain:
between the cortical systems that modulate our motivations and those that
modulate our emotions. The anterior frontal cortex is thus situated to be
the arbiter of our brain’s attitude systems. Thus the phrase “the organ of
civilization” was born.

The relationship between the pre-frontal cortex and the limbic
forebrain was unthought of when I first began my research. What was then
known was that major changes in personality occur when the frontal part
of the brain is injured. Famous cases were quoted repeatedly (and still are)
such as that of a patient who had a crowbar shoved through the front part
of his skull: he changed from a careful, good provider and stable friend to
a carefree roustabout.

But what might the frontal forebrain contribute to the form that a
person’s personality takes? A few clues toward answering this question
resulted from studies of patients and research with animals. Neurosurgeon
Wilder Penfield’s sister had developed a brain tumor in her frontal lobe
and he removed it. Her main problem after the operation was getting a
meal on the table in the correct sequence. Often the soup was forgotten



until just before dessert—or served after dessert, Chinese style! Penfield’s
observations of his sister reminded me of psychologist John Stamm’s
mother rats, who, after removals of their cingulate cortex, could not
organize the retrieval of their babies to get them into their nest: a mother
would retrieve one of the pups only to take another out of the nest while
retrieving a third—and so on, leaving a field of scattered, unsheltered
pups, in her wake.

Non-human primates had been shown to fail a task that had originally
been developed to test at what age human babies could find a piece of
candy that had been hidden a few seconds or minutes before. For human
babies, failure at this task depended on whether a distraction intervened
between the hiding of the candy and the opportunity for finding it. For
monkeys, hiding was ordinarily accomplished by lowering an opaque
screen between the monkey and the hiding place. The screen was shown to
be a distracter: if, instead of using the screen, the room was darkened, the
animals always succeeded in finding the peanut. Distraction impacts our
ability to remember just as it impacts our ability to perform a sequence of
actions such as the retrieval of pups, or getting dinner on the table in
proper sequence.

At the Yerkes Laboratory in Florida, I had operated on several
chimpanzees, removing the anterior part of their frontal lobe, their
prefrontal cortex. After surgery, when we tested these primates, they found
ways to bridge the time that elapsed between showing them the hiding
place of the reward and giving them the opportunity to find it. If the hiding
place was on the left, the chimpanzee would pace along the left side of the
fence that enclosed their living space; if the hiding place was on the right,
the chimpanzee would pace the right side of the fence. Monkeys tested on
this task might somersault to the left when a peanut is hidden on the left—
and so on.

The Topectomy Project
In Florida, during 1947, while I was simultaneously maintaining my

practice in neurosurgery and performing experiments at the Yerkes
Laboratory, I had the opportunity to visit New York and New Haven. Fred
Mettler, professor of anatomy and neurology at Columbia University in
New York, had set up a project in which different parts of the anterior part



of the frontal lobe of humans were removed—rather than severing the
entire lobe as in the lobotomy procedure. I had corresponded with Mettler
because, in my Florida neurosurgical practice, I had been given the
responsibility for a psychiatric ward of patients upon whom I was to
perform frontal lobotomies. Being well trained as a surgeon, I decided to
first test the patients myself to determine their preoperative capacities. I
asked Karl Lashley for assistance in formulating a testing program, and he
provided me with some tasks that might be useful to me, such as the
“Porteus Maze,” a paper-and- pencil maze through which the patient was
to find the shortest path from the outside to its center.

I also devised several tests of my own: for instance, a matchstick test
in which the subject had to pick up every other match from an array in
which the pattern is asymmetric. Patients with frontal lobe damage
invariably had great difficulty in performing this task. Unfortunately, so
did a few of the normal subjects (nurses) I tested, so that I could not
publish any results. (I had not heard of statistics at that point in my
career!) I also played checkers or chess with those patients whose problem
was that they had intractable pain, to see if they could maintain
concentration despite the pain. And of course I tested all subjects on delay
tasks that had proven so helpful in our pre- and postoperative testing of
non-human primates.

72. Pick Up Every Other Match

I soon noticed that most of these psychiatric patients improved
remarkably, simply as a result of the attention I paid them during testing
and just talking with them about their problems. James Lyerly, in whose



neurosurgical office in Jacksonville, Florida, I was working, had devised a
procedure for performing the lobotomy procedure through holes placed in
the top of the skull—a much safer procedure than the standard Freeman-
Watts approach through holes on the side of the skull, just above and
behind the eyes, which Mettler and his colleague Rowland had shown was
directly through “Broca’s Area” in the brain. The Lyerly procedure allowed
the surgeon to angle the cut forward, thereby avoiding areas of the frontal
cortex closer to limbic structures.

On the basis of his experience and mine with the ward of psychiatric
patients, Lyerly decided to perform lobotomies only on patients with
intractable pain or those with severe obsessive-compulsive disorders. For
such disorders, the procedure consistently provided relief with no
observable side effects. The intractable pain patients I tested could now
play checkers and chess, whereas before surgery, their pain had kept
erupting, making it impossible for them to play. Now after the operation,
when I asked about their pain, the patient’s answer was “Yes, it is still
there, just about as severe as before.” But a few seconds later we were
again playing the game.

73. Freeman-Watts Lobotomy. (From Languages of the Brain, 1971)



The people with obsessive-compulsive disorders were totally relieved
of all symptoms: I remember a patient who walked around with her head
bowed, looking only at the ground in front of her. She was afraid she
would kill anyone she looked at. The day after her lobotomy, we played
Ping-Pong—she was rather good at it.

While I was making these observations in our clinic, I was also
testing chimpanzees on a task in which the animals became frustrated. The
task consisted of giving the chimpanzee a choice between two well-
marked objects, one of which was consistently rewarded. The procedure
was easily mastered to perfection—until I suddenly ceased giving the
expected reward. As a rule, the animals would withdraw from the task for
a while and return a few minutes later to see if the reward was again
available. I recorded the “frustration” of the animal by timing the minutes
before the animal would return to the task.

This procedure worked well for me and showed that the duration of
“frustration” was shortened in two chimpanzees in whom I had removed
the cortex of the anterior portion of their frontal lobes. But I need to add
that the procedure worked well except with one chimpanzee named Alpha.
Alpha was given her name because she had been the first chimpanzee
imported from Africa to become settled in the Yerkes Laboratories. Alpha
was apparently not frustrated by my testing procedure. She would keep on
testing, reward or no reward. This went on for ten days. On the eleventh,
she did indeed stop testing when the reward failed to appear. I was timing
the interruption when a load of feces came flying over the testing
apparatus onto my head. I did not know how to rate Alpha’s duration of
frustration.

Sometime later, at Yale, I repeated the experiment with monkeys in
whom I had removed the cingulate cortex. Their duration of frustration
was dramatically shortened from what it had been before surgery.

The duration of frustration is a measure of the anticipation that the
frustration will persist or be alleviated. Currently there are innumerable
studies using brain-imaging techniques that show involvement of the
cingulate cortex in a great variety of cognitive, motivational and
emotional processes. I have not yet tried to collate the results of these
studies to test the premise that “anticipation” is a common denominator
that characterizes these processes.



When I told Fred Mettler what I had been doing, he took me into his
confidence. His topectomy project aimed to replace lobotomy with more
restricted removals of cortex of the frontal lobes. There was a team
composed of a surgeon, clinical psychologists, and a group of psychiatrists
of various persuasions: Freudian, Jungian and eclectic. Only the surgeon
knew which part of the cortex of each patient had been removed.
Psychologists gave the same tests to all patients, both before and after
surgery, at one-month and at six-month intervals. Their role was only to
test the subjects; they did not know what psychiatric therapy was being
given. The psychiatrists gave their best shot at therapy—knowing neither
what the psychological tests were showing nor where the cortical removals
had been made.

About a year after the project had gotten under way, Mettler invited
me to New York, and we went over all the records together. After that, I
made yearly visits; and later, once I had established a laboratory at Yale,
some of his team came to work with me. What happened to the patients as
a result of the Topectomy project came as a surprise to me and can be
summarized as follows:

1. Whether the patient was rehabilitated into a socially competent and
useful life depended to a great extent on the charisma of the
psychiatrist.

2. The type of psychiatric therapy had no apparent effect on whether the
patient was rehabilitated.

3. The location of the cortical removal did not determine whether the
patient was rehabilitated.

An important byproduct of the study was produced when, in two
decade-long catatonic mute patients, the neurosurgeon carefully removed
the cortex of Broca’s area without injuring any deeper structures such as
the underlying white matter and the basal ganglia. Both patients spoke
fluently after the operation—one within an hour, another within a day!!!

Larry Pool, professor of neurosurgery at Columbia University, the
neurosurgeon who undertook this audacious procedure, did so because
Mettler had found out that all of the Freeman-Watts lobotomies had been
performed through Broca’s area, and there had never been any consequent
disturbance in speech. Broca had identified his “speech area“ on the basis



of four patients with massive middle cerebral artery strokes. The damage
in all four cases impinged on the frontal lobe only slightly—but Broca was
influenced by the then-current idea that the front part of the brain was
involved in the generation of language. During the era of phrenology, the
study of bumps on the head to predict personality traits, the front of the
brain had been identified as allegedly dealing with speech because humans
have high foreheads while primates with low foreheads (lowbrows) don’t
speak. To this day, neuroscientists and neurosurgeons still hold to the view
that injury to the cortex of Broca’s area produces Broca’s aphasia. As I’ll
document in Chapter 22, I believe that injury to the basal ganglia
underlying the cortex of Broca’s area may be necessary to produce Broca’s
aphasia.

74. Diagrams of frontal cortex of two patients showing bilateral removal of the inferior frontal
convolution which contains (on the left) removal of Broca’s area. No marked disturbances of
speech resulted. One resection extends forward to include entire inferior frontal gyrus. (From

Pool, et al., 1949)

John Fulton



As noted in Chapter 10, after visiting Mettler, I ventured to Yale to
meet with John Fulton. Fulton was most gracious. I told him of my finding
of cortical control of the autonomic nervous system, the work with Bucy.
And I also stated that I did not obtain the results he had had in removing
the frontal lobes of chimpanzees. The report he had made at the
International Physiological Congress of those results had stimulated the
Portuguese neurosurgeon Egaz Moniz to attempt frontal lobotomies
(“leukotomies,” they were called in England). Moniz was awarded the
Nobel Prize for his groundbreaking work.

Fulton’s important finding was that a neurotic, hard to handle and
almost impossible to test chimpanzee became tractable and testable. There
were two chimpanzees: Becky and Lucy. My frontally operated
chimpanzees showed much more subtle changes: the duration of their
frustration reactions were shortened, and they could perform the delay
tasks by marking the side of reward by some intermediate “trick.” Fulton
pulled two large volumes of records from his shelves and said, “Take a
look at these; I’ll meet you here for lunch at noon.”

The very carefully kept records were a revelation. During Becky’s
(the neurotic one’s) surgery, a bevy of visitors watched: among them,
Harvey Cushing, the father of neurosurgery in America. Because of this, or
for whatever other reason, Becky developed a frontal lobe abscess which
took months to become encapsulated (no antibiotics in those days) and
heal. She was duly tested, having become tractable; years later, the
massive remnant of the abscess was noted at autopsy. As I showed in later
experiments, such irritating lesions are much more disruptive to behavior
than simple removals of tissue. Lucy fared better and the results of her
testing were identical with those I had obtained.

As noted in Chapter 10, over lunch, Fulton asked whether I had found
out what I wanted. I answered, “Yes.” He inquired no further but asked if I
would like to come to Yale to work in his department. He had a grant from
the Veterans Administration pending but needed a neurosurgeon to head it.
All that I had been wishing for!—I have no idea how I got out of New
Haven and back to Florida.

Four months later a telegram came: Could I start July 1st? Fulton was
leaving for Europe and would like me on deck to take over. I had agreed to
give a talk to the Duval County Medical Society, had to close my share of
the neuro-surgical practice, and got Kao Liang Chow, a recently arrived



graduate student from Harvard, to take over the testing of my animals
(mostly almost finished) at the Yerkes Laboratory.

The Yale Lobotomy Project
I was immediately put in charge of the monkey colony by Fulton and

found that a considerable number of the animals had tuberculosis. I
isolated these animals but had to keep them as long as they were being
tested in the various projects under way. This responsibility did not endear
me to my colleagues. I set up my laboratory, hiring out changes in the
facilities as they were needed. Almost a year later I found out that there
were “procedures” that one had to go through and that Yale would do the
hiring. Fortunately, no one asked me to tear down what I had done. I rigged
up a Yerkes testing apparatus much as the one I’d used in Florida. Yerkes
came to visit—and suggested that I build something more substantial.
(Yerkes and I became good friends later when I had taken over the
directorship of the Florida laboratories from Lashley. He remarked that I
was the best correspondent he’d had in the 20th century.)

Besides behavioral testing and neurosurgery, I was able to embark on
chemical and electrical stimulations of what we now call the limbic
systems. Results were obtained rapidly and I got them into publication in
the Journal of Neurophysiology, which Fulton owned and edited. As noted
in Chapter 10, within a year I had established the beginnings of evidence
that the prefrontal cortex, including what we began to call the orbito-
frontal cortex, worked as a higher-order controller of the limbic forebrain.
I felt that the results of lobotomy in humans were due to this relationship.
Fulton wanted me to write a book, but I felt that I didn’t know enough.

Nevertheless, I was asked to consult all over the United States and in
England. Also, I was asked to write an editorial on the lobotomy procedure
that was published in 1950. As I tell my students today: “I was famous
then.” (Of course, I also tell them that the fame was as much due to the
fact that a neurosurgeon would eschew practice and its attendant fabulous
salary as to the research results that the project was harvesting.)

All did not go smoothly, however. The Veterans Administration
rightly wanted some research done on human subjects, so a team was
gathered by the Department of Psychiatry, a team that included therapists,
psychologists, physiologists and chemists. The research was concentrated



on two patients. At the end of a year, the patients had pretty much gotten
well—as had my patients in Florida. What to do? All that investment in
research, money and effort! The obvious answer, too bad; we’ve learned a
lesson. To make a weeks-long story very short: The Department voted to
go ahead; I resigned and went to my basement lab to continue the animal
research.

There were other political issues to be handled. It was the time of the
McCarthy investigations, among other unpleasant matters. I wrote Lashley
for advice. It was given: “Make up your mind; do you want to do politics
or do research?” Again, I went down to my basement laboratory.

A bit later John Fulton was relieved of his department chair and
retired to the library he cherished. I found space in a nearby mental
hospital, the Institute of Living, and built a laboratory (literally). I got the
hospital to immediately reduce the number of lobotomies from two a week
to two a month and gradually to two a year. The laboratory flourished as a
research haven for PhD students who were awarded their degrees from
Yale, Harvard, McGill, the University of California and Stanford (well
before I moved there) on the basis of their research.

Meanwhile, I traveled around, trying to stem the tide of
indiscriminate lobotomy. In these ventures I got to know Walter Freeman,
Sr., and happened to arrive in his office the week he had disposed of the
need for his neurosurgical partner by introducing an ice pick into the
frontal lobes by way of the bony orbit that encases the eye. He no longer
had to depend on a surgeon—or anesthetist: he delivered two
electroconvulsive shocks to the patient so anesthesia became unnecessary.
I saw the procedure done only once at one of Mettler’s later research
projects (sponsored by Columbia University and the Greystone Hospital).
When Walter moved to California and was “operating” on children, I
warned him that he might well soon land in jail if he didn’t stop. He did
stop.

Freeman was dedicated, and his career has to be looked at in the light
of what was available and the conditions of mental hospitals at the time.
Nonetheless, without making a differential diagnosis, wholesale
“procedures”—which are now done with drugs, some of which were
originally declared to be “substitutes for the ubiquitous use of
lobotomies”—is poor medicine as well as unforgivable surgery.



What, When and How
Today, after a half-century of research, I know a lot more about the

processes that become organized by the far frontal parts of the brain than I
did during those early years at Yale. Anatomy is often a great help in
defining the function of a brain system. The primate frontal lobe is highly
conspicuous because the back end of our eyeballs is encased in bone. This
encasement (called the “orbit”) indents the brain so that its front part
forms a “lobe” that occupies our foreheads and overhangs our eyes.
Animals who lack this bony orbit don’t have frontal lobes. If one is
interested in comparing the brains of cats and dogs or rats and mice to
those of primates, one needs to seek other criteria, such as the connections
from the brain stem to the front part of their brains, to identify cortical
systems that correspond to those of the primate frontal lobe.

Two different motor systems encircle the primate frontal lobe. The
classical system, discussed in Chapter 8, was originally outlined by
electrical stimulations of human brains during the Franco-Prussian War of
the 1870– 1871. These stimulations produced movements of the body,
including tongue, face, arms, hands, fingers, legs and feet. The classical
system overlies and is densely connected to the upper basal ganglia. It is
shaped according to its connections with the body—a distorted
“homunculus.” As described in Chapter 8, Malis, Kruger and I showed that
this homunculus also receives a direct input from the body structures it
“controls”.

As noted in Chapter 10, working in John Fulton’s department at Yale
during the late 1940s, graduate, postdoctoral students and I outlined the
second system—the mediobasal, limbic, motor cortex, by stimulations
that produced primarily visceral and endocrine responses (although eye,
head and body turning were occasionally observed). This mediobasal
motor cortex covers the limbic basal ganglia.

The more anterior part of the frontal lobe, usually called pre-frontal,
lies between these motor regions and contains the systems addressed in
this chapter. I have identified three systems that compose this pre-frontal
cortex on the basis of their anatomical connections to other parts of the
brain and the effects on nonhuman primate and human behavior when they
are injured.



75. The Classical Motor Cortex. (From Languages of the Brain, 1971)

The functions of these three systems can be characterized as
regulating:

1. An extension of the limbic basal ganglia’s “What is it?” by
identifying the proprieties of behaving;

2. An extension of the upper basal ganglia’s “what to do?” by working
out the practi-calities of how to act; and

3. An extension of the Papez circuit’s role in processing efficiency into
effectiveness by assigning priorities as to when to act, the order in
which an action is to proceed.



Proprieties
The bottom (orbital) part of our pre-frontal cortex is densely

connected with the amygdala in the temporal lobe. Given the role we have
seen the amygdala to play in the processing of novelty and familiarization,
it is not surprising that this frontal system deals with an extension of
“what to do?”: what we do in a novel situation depends on how familiar we
are with the situation that provides the context within which we are
processing the novelty.

I had to deal with a memorable patient who showed typical symptoms
due to a tumor lying at the base of her frontal lobe (a meningioma of the
cribiform plate through which the olfactory nerves enter the brain.) She
was incontinent (peed in her bed), swore at the nurses, refused to help keep
herself clean—in general she was a pretty horrible person. The tumor had
grown slowly and was so large that it required three operations to remove
it. The first thing the patient did after the third operation was to profusely
apologize to the nurses for her earlier behavior. Shortly, her husband came
up to me with tears in his eyes: “You have given me back the sweet girl I
married.” Notable about this patient’s symptoms is that she recognized
that her behavior was inappropriate, but was unable to correct herself.

On another occasion in Moscow, I argued with Alexander
Romanovitch Luria for several weeks, demonstrating to him that a patient
recognized that she was making errors, and was exasperated by failing
simple tests we devised—but she could not carry out the appropriate series
of actions. He argued that the patient was impaired in the motor aspects of
carrying out sequences and that she was not aware of the impairment. One
morning, Luria proclaimed that he had settled the matter: he had come in
the previous evening and tested the patient to his satisfaction. I felt a
moment of joy—until Luria made his next statement: “It all came out my
way.” His way was not my way, so when I went back to Stanford, Audrey
Konow, a graduate student in my laboratory, and I repeated the
experiments with several patients who clearly recognized their errors
without being able to modify their behavior on the basis of that
recognition, and published the results.



76. The Limbic Motor Cortex. (From Languages of the Brain, 1971)

Some years later, I published a paper reconciling Luria’s and my
views: What Luria had not taken into account is—as described in Chapter
8—that the motor systems of our frontal lobe deal with actions, the
outcomes, the consequences of our behavior, not with those movements
involved in producing our behavior. Luria’s and my frontal lobe patients
were impaired in their ability to manage actions, their “images of
achievement”; they were not impaired in their ability to make the
movements, the sequences of muscle contractions. They knew they were
making errors, that they were off target, but they could not correct their
behavior accordingly.

Practicalities
Normal and optimal function of our brain and our body depends on

patterns. Where there is no pattern, there is no function. All the major
tracts in our brain and spinal cord are primarily inhibitory. The spinal cord
is constantly discharging nerve impulses at a high rate of activity. This
activity needs to be controlled by the brain’s motor cortex (the pyramidal
tract.) If it is not, all the muscles of the limbs contract simultaneously, so
much so that the limbs contract into spastic “scissor-like” positions.

It is neural inhibition that produces patterns; patterns are what neural
processing is all about. The middle part of the pre-frontal cortex, when
acting normally, influences the motor systems of the frontal lobe, as well
as the rest of the brain, through inhibition to allow one or another pattern
to become effective in regulating behavior and experience.



Neural inhibition achieves the patterned practicality by which, when
necessary, the pre-frontal cortex controls processing in the rest of the
brain. This is the “how” of frontal lobe function.

Priorities
The part of the pre-frontal cortex that organizes when we should do

something, the order in which we choose to schedule our actions, lies on
the upper inner surface of the lobe which is close to the cingulate cortex
(which is part of the Papez hippocampal circuit). The hippocampal circuit
helps to code what is being learned in such a way that it can be readily
retrieved. Hippocampal activity aims at efficiency: As we get older, the
hippocampus is one of the first brain structures to suffer deterioration. As
a consequence, we have trouble quickly recalling names, remembering
what to do next, even though we’ve rehearsed the names and the schedule.

But sometimes it is necessary to reorganize a schedule because of
changed circumstances. Computer scientists distinguish between the
efficiency of a program and its effectiveness: an efficient program
conserves time and energy. But an efficient program may not be effective,
so when it needs to be adjusted to new circumstances, the procedure of
adjusting breaks down. Writing a paper or protocol efficiently confines it
to as few paragraphs or pages as possible. But the efficiently written
paragraphs may be incomprehensible to those who need to read and use
them effectively. Running a shop or a laboratory efficiently often becomes
difficult because the people involved feel unduly pushed under the time
constraints imposed by management.

Sometimes a reshuffling of time slots and responsibilities leads to
more effective arrangements and thus greater productivity. A simple
example that I have experienced: I have my day planned to go to the bank,
then to the auto shop, then to the psychology department, and so on. But
the person I need to see at the bank hasn’t arrived, so I have to return after
I go to the auto shop. I had planned to return to the auto shop after I
finished at the department, but the department discussions are taking
longer than expected— so I dash out in the middle of it all to pick up my
car. The ability to reshuffle priorities quickly can save us time and energy
and money.



I became acquainted with the difference between efficiency and
effectiveness during the early 1960s while assigning access to my
laboratory computer. Priorities need to be assigned, and these priorities
need to be shuffled in case of emergency or if the person whose money is
keeping the facility going unexpectedly has a “great need.” We used to call
such programs “flexible noticing orders.” The term “noticing order” calls
attention to the experience aspect of controlling the program and thus our
actions. The noticing order is the “image of achievement” that organizes
action.

The upper part of our pre-frontal cortex, by virtue of its affinity to the
cingulate-hippocampal circuit, has access to the patterning of our brain
processes and adds needed flexibility to those processes. By the same
token, the process of “what procedure to carry out when” is grossly
disturbed when the pre-frontal cortex is severely injured. But injury is not
the only cause of lack of flexibility. In patients who have obsessive-
compulsive disorders, the frontal lobes appear to be working overtime,
with a resulting limitation to their patterns of behavior similar to that
which occurs when the lobe is damaged. This shows that our brain
circuitry can malfunction either by virtue of an absence or excess of a
process. Anesthesia provides an extreme example of both absence and
excess: most anesthetics work by way of abolishing or minimizing brain
activity. But the same result—“anesthesia”—can be produced with drugs
that overstimulate the system, essentially producing an epileptic state that
encompasses the entire cortex.

Conscious experience as well as unconscious processing depends on
patterns of brain activity. To paraphrase the Clinton administration’s
dictum on economics, “It’s pattern, stupid.”

Complexity
I wondered for many years how I might measure the rearrangement of

patterns that is involved in the executive process, the rearrangements that
rely on a flexible noticing order, a process that so dramatically produces
the novelty that results in an orienting response. These rearrangements of
patterns are distinctly different from rearrangements of shapes. A clue to
what might be involved came from the experimental result that so clearly
distinguished the cognitive behavior of monkeys in whom I had removed



the prefrontal cortex of both hemispheres. Monkeys with such removals
perform perfectly well on tasks in which they must choose between clearly
marked stimuli, such as painted figures on the lids of boxes, when such
stimuli are consistently rewarded. The ability to make such choices is
dramatically impaired when the “association” cortex covering the back of
the brain is removed.

By contrast, when the reward alternates between unmarked boxes, a
monkey’s ability to make choices is impaired by removals of the pre-
frontal cortex, the amygdala and the cingulate gyrus. Such situations can
be designated as alternating between “go right and go left,” or between “go
and no-go.” (The ability to make choices among such alternations is not
impaired when the cortex covering the back of the brain is removed.)
During the 1970s, I performed a series of experiments showing that
amygdala removals impair a monkey’s ability to make choices when a
variety of patterns of alternations in reward are made. I called such
patterns portraying the structure of repetition “the structure of
redundancy,” in the technical language of information measurement
theory.

I immediately realized that I had no measure, no precise idea of what
I meant by “the structure of redundancy.” William (Bill) Estes, one of the
most eminent contributors to mathematical psychology, then at Stanford
and later to become the William James Professor at Harvard, was editing
my publication and asked me whether I really wanted to use the term
“structure of redundancy.” After some discussion, I said yes, and Estes
agreed to let me have my way.

It was decades before I could take the next step in understanding what
I really meant by the structure of redundancy. The answer came in a form
familiar to us throughout this book: the mathematical field of inquiry that
had developed during the 1980s called “non-linear” dynamics or
“complexity theory.” Webster’s Dictionary notes that “complexity” is
derived from the Latin cum (meaning “with,” “together”) and plicatum
(weaving). Complexity is weaving something together. This “definition”
provides an excellent metaphor for what I was looking for regarding the
structure of redundancy but did not help me in discerning how to come up
with a measure of complexity. I went to conferences and wrote papers
expressing my disaffection.



The resolution to my quandary came through an unexpected route.
During the 1990s, my colleagues and I were performing a series of
experiments measuring brain electrical activity (EEGs) in humans
performing a variety of tasks. I had wondered whether we could find out
whether the EEG mirrored the changes in the difficulty (complexity?) of
the tasks being performed. We tried several measures of complexity that
had been developed in other laboratories but found that the EEG is
produced by so many sources of variability that we could not tell whether
the variability was random, produced by irrelevant changes (noise), or true
complexity (whatever that meant.)

Paul Rapp, then of the Department of Neurology of Drexel University,
had developed a data compression scheme that searches for a pattern that
repeats itself within a string of symbols. We took the EEG recording, one
electrode at a time, and followed that recording to where a change in
pattern takes place. We mark the pattern with a letter. We go along the
record until another change in pattern occurs. If the next pattern is the
same as the earlier one, we mark it with the same cipher. If the pattern is
different, we mark it with a different cipher. Next we go along the set of
ciphers to see if there are any repetitions of sequences of ciphers, for
instance, “xxy” in a series “abxxycdaxxyfghxxyabf.” In this series “ab” is
also a repeat. Now we go to a new level, generating a new, shorter string
by marking the “xxy” with the cipher “p” and the “ab” with “n.” For this
example, the level of complexity is two; that is, no repeats of identical
strings such as those composing “p” and “n” are found. By proceeding to
analyze the brain electrical activity of 120 electrodes in this fashion, one
electrode at a time, we observed many more levels than two where
repetitions occurred in an electrode.

Algorithmic complexity thus turned out to be a useful measure of
complexity—and the measure of the structure of redundancy, of
repetitions, that I had been searching for. I’m glad I persuaded Bill Estes
to let me retain “structure of redundancy” in the title of my paper so many
decades earlier.

There is one more aspect to this story of the structure of redundancy:
the measure of complexity and the amount of compression of our
experience by our brain. In a recent discussion of my EEG results during a
lecture, I was asked: How did we divide the series of changes in the
recording when searching for repetitions? I answered, we didn’t divide



them at all. But looking at the sequence of those EEG results on the
blackboard, it became obvious that making divisions, “chunking” the
sequence in various ways, would allow much greater flexibility in
structuring our redundancy—that is, we could come up with many
different ways of compressing our experience for future guidance in
navigating our world. Chunking is a function of the pre-frontal cortex of
the brain.

As an aside, Kitty Miller coined the term “chunking” while she,
George Miller, and I were having dinner in Cambridge, Massachusetts.
George and I were pondering the issue of how we make plans and the
problem that our working memory is limited to about seven items, plus or
minus two. How do we handle larger sequences as we do when we make
and execute plans? We divide the task. We needed a name. Kitty
volunteered: we were at the Chun King Restaurant; why not call dividing
the task into chunks, “chunking?” The name stuck. Multiple ways of
chunking provides a key to the practicality, the “how” of frontal lobe
function.

The How of Chunking
But what about the “how of the brain process” that allows chunking

and re-chunking to occur? A possible explanation is rooted in work
performed by Robert Thatcher of the University of Florida. Thatcher has
shown that changes in the EEG are initiated at the thalamus. The EEG is
constituted of a gradient of potential difference created in the brain cortex
from surface to depth. These potential differences, these de-polarizations
and hyper-polarizations occur in the small fibers of the cortex, the
dendrites. The polarizations occur at various frequencies and, as I’ve noted
earlier, their appearance in the EEG changes about 100 times per second.
Thatcher’s contribution is his discovery that not only does the EEG
change, but the relations within the EEG (the phase, the spectrum) also
change rapidly. Scientists had already noted that changes in phase of the
Alpha rhythm occurred with changes in visual input such as opening and
closing our eyes. Thatcher measured the rapidity of the onset of changes in
phase and found that they were too rapid to be carried out simultaneously
over the entire expanse of the brain cortex: the origin of the onset of these
changes was more likely in the thalamus. But once a phase has been set,



the time taken for further changes in phase can be occurring across the
cortex as demonstrated by simultaneous changes occurring in the cortical
EEG.

This made sense because in several discussions with Thatcher I had
also suggested to him that the thalamus might be involved in setting the
phase relationship. In contrast, my suggestions centered on the projections
from the thalamus to the limbic and prefrontal cortex.

The thalamic projections to the cortex take two very different forms.
The thalamus is a three-dimensional structure; the cortex is essentially
two-dimensional. Thus one dimension in the connectivity becomes “lost.”
In the 1950s, I had shown that for most of the thalamic projection the
missing dimension is essentially outside to inside. By contrast, for the
connections to the limbic and prefrontal cortex from the thalamus, the
front to back dimension is absent. Thus, in primates, a file of cells in the
thalamus projects to a “point” at the tip of the frontal cortex.

My suggestion, therefore, was that choosing a particular setting of a
phase relationship is due only to the frontolimbic thalamus. My proposal
has two advantages: it can account for the importance of attitudinal
influences on cortical processing; and it can account for chunking and the
changes in chunking that the prefrontal cortex is processing. These
changes are accomplished by activating the reticular nucleus which
surrounds the entire thalamus that, as Thatcher suggested, initiates the
setting of the phase.

There is a relationship between a) my finding that removals of
prefrontal and cingulate cortex alter the duration of an animal’s or person’s
reaction to novelty and frustration and b) the finding that such removals of
frontal cortex alter a person’s ability to process complexity. Processing
complexity, undoing a compressed, stored memory in the brain, takes a
while. Pitfalls need to be avoided: solutions to a problem can be achieved
which fall short of being the richest solution achievable.

Obsessive-compulsive behavior is such a solution to the problem of
feeling anxious. Putting this into the terms of non-linear dynamics, in
order to resolve a difficulty, we create landscape of the process in which
there are valleys, that is, “wells” that the process can settle into—and the
best solution, the richest valley, is never reached because the process
becomes “stuck” in a shallower well. As noted so often, persons with
frontal lobe injury appear to their families and friends as having become



“shallow”—that is, less complex. Obsessive-compulsive behaviors
demonstrate extreme lack of complexity, extreme shallowness.

In Summary
The what, how, and when of pre-frontal cortex function comprise its

role as the executive processor for the brain. Reflex behavior does not
need executive control, nor does automatic behavior: As Sherrington put
it, “The more reflex the behavior, the less mind accompanies it.” Minding,
determining our attitudes, paying attention to sensory input, planning an
action or thinking through a variety of compressed memory-based
processes, recruits frontal lobe activity. Executive decisions search for
proprieties on the basis of familiarity; decide what is practical by
multiply chunking what is available; and assign priorities on the basis of
current possibilities. In short, the pre-frontal cortex helps us organize the
complexities that enrich our actions and our lives.

Very civilized.



Chapter 17
Here Be Dragons

Wherein I dispute a common misconception concerning the brain
processes involved in organizing our massive aggressive behavior (such as
warfare and ethnic cleansing) against one another.

Xerxes, we are told by the Historians, when from an eminence he
viewed the numerous army with which he Intended to Invade
Greece, wept bitterly, when he reflected that every man in that
vast multitude, would be dead and rotten in less than one
Century. This Thought moved the Tyrant, and drew tears from his
eyes, when the prospect of the murders and butcheries he was
preparing to commit, to gratify his vain ambition . . . This shows
us that Tyrants, are not altogether Incapable of pity or
compassion . . . These tender passions which Nature has planted
in him, should be bent upon proper objects. We may hence Infer,
that most, if not all Tyrants have been made so in great measure
by Education, and, that parents and teachers, have very
Industrially Contributed toward making monsters of them . . . .

—An account of this story from Herodotus 7.46 appears in The History of
the Tuesday Club

The utility of all passions consists alone in their fortifying and
perpetuating in the soul thoughts which it is good it should
preserve, and which without that might easily be effaced from it.
And again, all the harm which they can cause consists in the fact
that they fortify and conserve those thoughts more than
necessary, or they fortify and conserve others on which it is not
good to dwell.



—René Descartes, Treatise on the Passions of the Soul, 1649

It appears that all of us share the same moral networks and
systems, and we all respond in similar ways to similar issues.
The only thing different, then, is not our behavior but our
theories about why we respond the way we do. It seems to me
that understanding that our theories are the source of all our
conflicts would go a long way in helping people with different
belief systems to get along.

—Michael S. Gazzaniga, The Ethical Brain, 2005



It is time to pause to examine a tenet of what I have called
“neuromythology” that, in my view, is severely harming our social
existence. The tenet is that we have inherited “dragons” in the form of
aggression and sexuality. According to that story, the dragons derive from
our base ancestors and we must use our brain cortex to manage them. This
version of the origins of human aggression has been latched on to by
authors as famous as Carl Sagan and Arthur Koestler, lending to it
credence and credibility when there is much evidence against it. My view
and the evidence supporting it shows that our problems lie not with the
“dragons” but with the very cortex that supposedly can tame them. My
critique has several roots that I will develop one at a time.

Aggression
During my years of research, we learned a good deal about aggressive

behavior. Most helpful were my experiments that provided insights into
the brain processes that are involved in organizing our attitudes, our
emotions and motivations. During the 1970s, with a number of Russian
and European colleagues, I helped found a “Society for the Study of
Aggression,” and I presented its first keynote address in Paris. My theme
was that there are two very different modes of aggression: One mode is
explosive, such as in the “sham rage” described in an earlier chapter, that
results from maximal electrical stimulation of our hypothalamic or
amygdala systems. Under ordinary circumstances, outside the laboratory,
such behavior is elicited in us by intolerable frustration. Such explosive
behavior is not clearly targeted, but rather it takes its toll on whomever or
whatever happens to be around us. It is an emotional outburst that is not
aimed at “getting into practical relations with the environment.” In short,
it is really not motivated in the sense that William James specified.

A totally different type of aggression is both motivated and “cool.” A
lioness is stalking a herd of gazelles. She stays out of sight, downwind, out
of scent. She spots a laggard gazelle and waits until it is sufficiently
separated from the herd before she pounces. She brings the gazelle’s
carcass to a safe place and allows her mate to have his meal. Then it is her
turn to eat, and that of her cubs. From my male chauvinist standpoint, that
is an ideal family arrangement! The lion doesn’t even have to worry about
the ethics of eating meat—he didn’t do the killing. After eating, the lion is



docile and out of the way so that the family can groom and nest. Leakey
was fond of telling the story that on several occasions he had approached
such a recently fed lion and was able to pet him. No hunger, no need for
motivated aggression. No frustration, no emotional aggression.

Of course there are combinations of the emotional/ frustrated and
motivated/calculated types of aggression, and these combinations can be
especially dangerous as shown by “wars” among chimpanzees in which
one tribe tries to annihilate another. The calculating political initiators of
human motivated warfare often try to whip up emotional aggression
against an “enemy”—but this must be limited by minimizing frustration
and bonding among “friends” or the emotion becomes self-destructive on
the battlefield.

Male Aggression?
A standard image put forth by social anthropologists has been: male

the hunter, female the gatherer. Under this image males must be more
aggressive than females. But evidence has shown that hunting, especially
in early societies, was a cooperative venture, while gathering berries, nuts
or grains can be done in isolation, even if a whole group surrounds the
picker.

It’s true that the Y (male) chromosome, especially when doubled, has
more often been shown to be present in criminal aggressive behavior than
the X, the female chromosome. Likewise, high levels of testosterone,
rather than of estrogen, have often been regarded as being the responsible
agent in fomenting aggression. However, my experiments with monkeys
have shown that under ordinary circumstances, genetic and endocrine
factors increase the intensity, not the frequency, of aggression.

One of my failed experiments was a plan to put dogs and cats
together, watch them become aggressive and then remove their amygdalas
to observe the resulting taming. The animals were brought in from “death
row” at the pound and housed in individual cages in my laboratory for a
week. Some days prior to the contemplated surgery, I cleared a room, got
cameras set, and enlisted enough people to pull the animals apart if things
got out of hand. I let a half dozen dogs and a half dozen cats loose into the
room. During the next hour, there was lots of sniffing and a bit of
mounting. Then came nap time. There were cats snuggled up with dogs;



dogs with their “arms” around a cat: A really wonderful and impressive
sight. There was no need to perform an amygdalectomy in order to
produce taming. I surmised that a lot of the “aggression” we ordinarily see
in domesticated animals starts with a chase which is often initiated simply
by movement of the to-be “prey.”

On another occasion, after a severe winter snowstorm and during the
succeeding cold spell, my family and I put out food in a line of dishes for
birds and squirrels and other wildlife that might be hungry. Shortly we had
a Noah’s Ark of animals all lined up next to each other—big birds, small
birds, chipmunks and squirrels in a mixed array, side by side—with no one
trying to usurp the contents of his neighbor’s dish. My comment at the
time was: “It goes to show that if you give people enough to eat they
behave agreeably.”

The lesson I took away from these observations is that the context in
which the behavior is observed constrains behavior. Genetics, endocrines
and the brain provide possibilities—potentials—but these possibilities
become actualized according to the situation in which the organism finds
itself. Social scientists are well aware that the situation within which
behavior occurs is critical—but neither they, nor neuroscientists, nor
practicing physicians are fully aware that this applies as well to how brain
processes operate in a particular situation.

The Triune Brain
After World War II, brain scientists began to study the brain from the

inside out, peeling or coring it as if it were an onion, rather than slicing it
from bottom up, floor-byfloor, skyscraper style. At Yale, my group
working in John Fulton’s laboratory was using my neurosurgical
experience to study the forebrain in this manner. With the results of my
experiments in mind, my office-mate, Paul MacLean, came up with the
idea that we really have three forebrains, one inside the next. With his gift
for naming, he called the innermost brain “reptilian” because, according to
the teachings of comparative neurology at the time, in reptiles this part of
the brain, the basal ganglia, has become large and reptiles don’t have
much else. The next layer he called “mammalian” because the anatomical
structures that compose Broca’s lobe limbique are clearly evident in all
mammals. The structures composing the limbic brain are the hippocampus



which is archicortical (archi, “original”) and the cortex surrounding the
amygdala, which is paleocortical (paleo, “old”). Finally, in primates
including humans, MacLean noted that the “neocortex” dominates the
forebrain.

MacLean made it clear that he had an agenda when he made these
three divisions of the forebrain. His claim was that deep inside us there is
an aggressive reptile that can become uncorked at the slightest
provocation unless controlled by the flexibility provided by our more
recently evolved brain structures. He asserted that the limbic forebrain’s
functions provide this flexibility. The neocortex provides additional
control based on reason.

MacLean’s theory hit a responsive chord in some scientific and many
popular circles. Homo sapiens sapiens—the “wise one,” the cortical
primate—is misled into aggressive action by the reptilian dragons buried
deep in his ancestral brain. This makes for a great story, but MacLean’s
story is an example of a myth gone wrong. It is wrong not only because its
factual base is flawed, but also because it leads to wrong actions based on
false premises.

Arthur Koestler, the novelist and popular science writer, spent some
time at the Center for Advanced Studies in the Behavioral Sciences at
Stanford during my tenure there. We held a joint seminar that was to lead
to a joint publication, and we soon became lifelong friends. My daughter
Cynthia was named after Koestler’s wife. One day, the Koestlers suddenly
decided to leave California: it was never clear why. In departing, they left
their Dalmatian coach hound, Tycho Brahe, with us as a gift to me and my
family. (Tycho Brahe and Johannes Kepler gathered the data which Isaac
Newton used to formulate his gravitational theory in the 17th century.)

On the way back to London, the Koestlers stopped in Washington,
DC, for a visit with my colleague Paul Ma-cLean. A year or so later,
Koestler published a book titled The Ghost in the Machine. I was unhappy.
The first part of the book was based on the notes I had written up from the
Stanford seminars that he and I had conducted together. The title of the
book was borrowed, with no acknowledgment, from a phrase coined by the
Oxford don Gilbert Ryle. The second half of the book was devoted to the
triune brain. When I asked Arthur why he had not acknowledged Ryle, he
replied that he had never believed in scientists’ addiction to dating
discoveries and acknowledging sources. As for MacLean, Koestler was



enthralled with his theories, which supported his own beliefs about the
innate destructiveness of mankind. He told me, “If I had not met him
[MacLean] I would have had to invent him.”

Some years later, I visited the Koestlers at their home. They gave me
autographed copies of all of Arthur’s books, which he had stored in an
adjacent barn, and we had a lovely tea together. Koestler had developed
severe Parkinson-like symptoms that were controlled by medication, but
this condition left him with only a few hours a day to work. I suggested
that he and I publish a dialogue on how we viewed emotions—similar to
the one that Eccles and Popper had done in their book The Self and Its
Brain. Koestler was not sure whether he would be up to it, but we thought
his wife Cynthia, who had edited and re-edited all of his works, could
carry the major part of the burden of getting such a discourse down on
paper. Six months later, the Koestlers jointly committed suicide: he did
not want to go on in his condition, and she did not want to go on without
him.

Subsequent books, such as Carl Sagan’s The Dragons of Eden, have
further popularized the idea that the oldest parts of our brain were
reptilian, hence we are predestined to be combative and warlike. However,
evidence in three categories indicates that exactly the contrary is true: the
design of our brain’s anatomy; the functions actually performed by those
anatomical structures; and hard evidence from the laboratory concerning
the effects on behavior of these functions. (For an excellent review of
current comparative neuroscience see “One World, Many Minds” by Paul
Patton, Scientific American MIND, December 2008/January 2009.)

The anatomical record shows that when fish landed and transformed
into amphibians, a major change occurred in the basal forebrain (which in
later evolutionary stages begets the basal ganglia) and the cerebellum.
Tad-poles and frogs, salamanders and toads made the really big change—
the change from living in a homogeneous aqueous environment to living
in an inhomogeneous environment composed of storms and gravity. From
the standpoint of the triune brain or Carl Sagan’s Dragons of Eden, their
disadvantage is that they are rarely aggressive or vicious and therefore
don’t fit the mold of “dragons inside us” that need to be vanquished by
cortical humanity.

The basal forebrain, especially the basal ganglia, did become larger
and more complex in reptiles, as MacLean claims, but the epitome of this



development came in those reptiles that evolved and took to the air: birds.
Parrots can be taught to identify objects with spoken words and even
repeat strings of words from memory: the so-called “reptilian brain” par
excellence. A visitor asked Richard Restak’s grey parrot Toby, “Can you
talk?” The parrot replied “Yes, I can. Can you fly?” Just as in the case of
amphibians, the highly evolved reptiles known as birds just don’t fit the
mold of the “dragons”’ presumably within us.

The limbic “old brain” is truly old. The oldest, the original cortex
which grew into our hippocampus, is already present in sharks, one of the
most ancient of the fishes. And recent research has shown that a part of the
forebrain of birds is equivalent to mammals’ hippocampus. Shouldn’t
sharks and birds share the characteristics of mammals, if those
characteristics are due to the hippocampus as the triune brain theory
proposes? The amygdala, another part of the limbic system, is not quite so
ancestral as the hippo-campus, but is, as detailed in the previous chapters,
one of the basal ganglia. But the basal ganglia, in the triune brain theory,
characterize reptiles, not mammals!

In primates, including humans, the limbic allocortex surrounding the
amygdala and hippocampus has accrued neo-cortical additions. These
evolutionary changes have enabled us to experience and express complex
emotional and motivational attitudes. In this respect, despite the
anatomical misconceptions, the triune brain theory is correct.

In my view, the most glaring error in the triune brain theory concerns
its conception of the functions of our brain cortex in the control of
aggression, an error that is unfortunately shared by many throughout the
brain sciences, psychology, and the humanities. The error is that our
aggression, especially as it is expressed in warfare, is generated by the
evolutionary older parts of our brains and that control of aggression must
be exercised by our cortex over these older brain systems.

Bluntly stated, the correct proposition, based on evidence from both
biological and social observation and experiment, ought to be: the
expression of aggression (and sex) in humans is related to our cerebral
cortex. The refinement and duration of agressive experience and behavior
expressed in humans is a function of the evolutionary later-developed
outer “shell” of the brain rather than of processes organized by the core
parts of our brain. In short, no animal lacking this highly developed cortex



would distinguish between Roman Catholics and Orthodox Catholics and
carry out “ethnic cleansing” accordingly.

I’ll come back to the topic of aggression later in the chapter. But first
I want to show, using a less emotionally charged topic, that the same error
is made in attributing the organization of human sexual behavior more to
core-brain than to cortical processing.

Sex and the Brain
The fallacy regarding the brain process that presumably generates

aggression is also commonly held for the brain processes that organize
sexual behavior. Some of the evidence for questioning the common view
was presented in Chapter 11 in describing the experiments undertaken
under the heading of the Four Fs. The conclusion reached in that chapter
was that the apparent increase in sexual behavior that follows
amygdalectomy is produced by a dissolution of the normal animal’s
territorial constraint on his behavior, his “stop” process, rather than by an
increase in the amount of sex hormone circulating in his blood.

My colleague at Yale and subsequently at the University of California
at Berkeley, Frank Beach, the experimental psychologist who devoted his
life to understanding sexual behavior, asked me to look at the brains of
cats that had been operated on by someone else. He had studied the sexual
behavior of the cats and found that the females didn’t want sex, even when
their hormones signaled that they ought to be in heat. Beach wondered
which part of the limbic brain had been invaded to produce such an oddity.
The results of my anatomical examination were even odder than was the
cats’ behavior: I discovered in each brain that only the cortex had been
removed! This was surprising: Why would removal of the cortex reduce
sexual behavior in an animal whose hormone systems were still intact? All
the female cat has to do to consummate the sexual act is stick up her rear
end, a maneuver that could readily be accomplished by an animal with an
intact “reptilian brain.” I would have expected some effect of the cortical
removal in male cats, who have a much more complex ritual and postural
behavior to succeed in sexual mounting.

In a different experiment, I collaborated with Richard Whalen of the
University of California at Irvine to find out if, perhaps, removals of the
amygdala would have an effect on cats’ sexual behavior. Whalen had a



group of male cats he had been studying for years. He had completed these
studies, so he and I planned to see whether the cats’ sexual behavior would
change after amygdalectomy. We chose some virgin females as targets for
the males’ sexual advances; virgin female cats have to be courted very
patiently before they will give up their virginity. Once more we were in for
a surprise: the amygdalectomized male cats were patient, skilled and
successful in their courting. These old rogues had honed their intact brain
cortex, which they now used successfully in the current situation. I had
planned to present the results of this experiment in a talk at an upcoming
conference on sexual behavior and was left empty-handed.

On the basis of these and the other studies previously described, I
have concluded that the brain cortex has a great deal to do with the
execution of the skills involved in both sexual and aggressive behavior,
and that these skills include “wanting” sex or “entertaining aggression.”

Many years ago, while I was on a somewhat delayed honeymoon, my
wife and I met another couple, also on their honeymoon. They were lovely
people and fun to be with, but on the second day the young man, who had
chosen to be a minister, came to me in confidence. As I was a physician,
perhaps I could help. His wife totally refused to have sex. He was
completely surprised. I talked to this very beautiful young lady: she was as
surprised as we were; after all, she had married a minister because she
assumed he would be too holy to want sex!

Hormones? Limbic forebrain? Only the brain’s cortex could be
involved in surmising—or hoping— that ministers, like Catholic priests,
might forgo a sexual relationship.

What is true of the critical role our cortex plays in sexual mating is
true also for aggression. As I described earlier, aggression comes in two
very different basic forms: emotional outbursts and motivated “cool”
planning. We sometimes go to war or become terrorists because we are
frustrated and angry. We sometimes go to war because of territoriality:
“This is our turf, stay out.” These forms of aggression do involve the
amygdala and its relationship with other subcortical parts of the brain such
as the hypothalamic region. They are reactive, defensive, and are
expressed toward anyone who happens to be in the way.

But some of our most vicious and prolonged wars have been
“predatory.” They are fought on behalf of ideologies that only cortical
sapiens sapiens could concoct—within economics: communism vs.



fascism; within Islam: Sunni vs. Shia; between Christianity and Islam;
between Islam and Hinduism; within Christianity: Catholic vs. Protestant.
Aggressors hope to “convert” or “eat up” the infidel. And all who wage
war must have the lioness’s skill if they are to succeed. These are our
“dragons,” but they are not buried deep within us; they are due to ever
more polished refinements of our attitudes, made possible by our cortex
and embodied in our cultural constraints.

A “Sinister” Confirmation
There is a relationship between left-handedness (sinister is Latin for

“left”) and warfare. A study of traditional cultures found that the
proportion of left-handers—that is, they preferentially use the cortex of
their right hemispheres—is correlated with its homicide rate. For instance,
violence, raiding and warfare are central to Yanomamo culture of Brazil.
The murder rate among this society is 4 per 1,000 inhabitants per year
(0.4% compared with 0.068% in New York City). Left-handers comprise
22.6% of the population. In contrast, Dioula-speaking people in West
Africa are virtual pacifists. There are only 0.013 murders per 1,000
inhabitants (that is, 0.0013 %) among them and only 3.4% of the
population is left-handed.

The study (reviewed in The Economist, Dec. 11, 2004) notes that
individual left-handers are not more aggressive than right-handers. What
appears to be the case is that left-handers have an advantage in hand-to-
hand conflict because right-handers are not used to combat with the few
left-handers they engage. This is true even today in sports like tennis and
baseball. Thus the left-handers often score more victories and this
becomes part of the cultural lore. Pretty sinister!

Whatever the chemical processes that underlie left-handedness—and
there have been studies indicating a correlation with amounts of
testosterone secreted during gestation—the cultural manifestation of
raiding and warfare are clearly correlated with the human brain’s cortical
development rather than some ancestral remnant of vicious dragons. So
much for the triune brain and “the dragons within.”

To summarize: The triune brain concept continues to be popular
because it supports an accepted but erroneous myth that man is disposed to
violence and aggression due to the operations of evolutionarily old brain



structures that lie buried deep in the core of his brain. However, it is not
his core brain but this very aptitude for myth that distinguishes homo
sapiens sapiens and leads to his aggressive behavior. Episodes become
encoded in stories, in narratives. Narratives extend the potential
effectiveness of an episode in time and amplify it in intensity: revenge,
“an eye for an eye, a tooth for a tooth.” The discriminative capacity of our
brain cortex and its prodigious memory-processing capability carry the
burden of ideological warfare.

The “Dragons of Eden” myth is faulty on anatomical, functional, and
behavioral grounds. The acceptance of this myth does severe damage to
the way we currently view all our social and political arrangements. In
fact, we are neither helpless in the face of our basic instincts, nor do we
need to “control them” by the use of our brain cortex. Rather, we must
reorganize those political musings resulting from the operations of our
cortex, which, when carried out in the extreme, wreak havoc upon the
community of humans.

Currently, almost all practitioners of psychoanalysis as well as the lay
public still espouse a version of the “dragon myth” by believing that the
“superego” must keep the “id,” the basic drives, the “dragons,” in check.
But that is contrary to what Freud said. As described in Chapter 12,
Merton Gill and I document in our book Freud’s “Project” Reassessed
that for Freud, the superego, “the origin of all moral motives,” is derived
from what a caretaking, caring, person does to satisfy the needs of the
baby, not to prevent satisfaction. We can cope with pains and pleasures,
not by suppressing them, but by utilizing experiences gained from our
caretakers. These acquired attitudes can become “dragons.” The dragons
are not instilled in us by our inherited biology or by our own experiences
per se.

The Cultural Context
Human biological evolution did not select for aggression. Hunting is,

to a large extent, a cooperative venture. Gathering can be cooperative or
performed in isolation. In neither activity is war-like aggression a required
skill. Current non-industrial societies, like those in Borneo, have ways of
facing off symbolically in a virtual reality—in a dance representing some
tribal ideology. If someone is seriously hurt, both sides join in lamenting



and mourning. Creating a virtual reality is, par excellence, a process
dependent on the human brain cortex.

The Cold War was carried out in this same symbolic fashion: Five
atom bombs on each side would have been enough to totally wipe out the
“other side,” but each side kept building more and more bombs in a dance
carried out in a virtual world of deterrence. We were not really after each
other’s territory; we only wanted to project the grandeur of an idea, an
ideology, communist or capitalist,

I had an interesting experience with regard to the crisis initiated by
Soviet Premier Nikita Khrushchev when he started to build a missile site
in Cuba that nearly turned a virtual reality into a real conflagration. I was
in Moscow at the time, enjoying a great dinner served at the home of
Leontiev, a high-ranking member of the Soviet scientific community.
Conversation was lively and carried on almost simultaneously in four
languages: Russian, English, German and French. Most of us could
understand one or two but not all four languages. The topic of war came
up: we drank to friendship. Without friends who could cross the barriers as
we were doing, how could any war come to a halt? As far as I know, none
of us was aware, as we drank our toasts of vodka, of the crisis taking place
overseas—Leontiev might have known, but certainly did not let the rest of
us know.

A few days later, as I traveled from Russia into Czechoslovakia, I
encountered street demonstrations damning the Americans for our (failed)
invasion of Cuba and for our resistance to having a friend (Russia) help a
misunderstood (Cuban) populace. As the Cuban crisis was resolved and I
came back to the States a few weeks later, I was to receive another
surprise: Americans had been terribly afraid of the Russian incursion into
our hemisphere so close to our border. On further inquiry, I learned that
neither my Russian friends nor the Russian people in general could
imagine that the great American nation might be afraid of a feeble marker
—made of a few missile silos—set near their borders. After all, the
Americans had missiles in the Pacific and in Western Europe that
surrounded Russia at every corner.

From my experience with the Russian people, I began to realize
something about the difference between the attitudes of the American and
Russian people toward this virtual game of deterrence that was being
played: I had once made an unintentionally offensive remark to friends in



Leningrad who had asked if I wanted to share a cup of coffee. I answered,
“No, thanks—you Russians make wonderful tea, but your coffee is
horrible.” Their reaction was a strong withdrawal, fortunately temporary,
after I tried to make amends. I had insulted them and the culture they held
dear. In the context of the Cuba affair, I realized that the Russians weren’t
really afraid of us Americans and our bombs—they had much more
serious problems to deal with right there in the Soviet Union. They felt
insulted by our placing missile sites so close to their homeland. The truce
was forged: we promised not to invade Cuba again; Adlai Stevenson
arranged to have the most insulting of our missile sites, those in Turkey,
removed.

These musings of mine are, of course, only part of the story. But
much of the warfare waged by homo sapiens sapiens, the cortical primate,
over the centuries has been carried out symbolically: Knights in armor
tilting their lances and that sort of thing. Actual, real life bloodshed on a
major scale came in with agriculture, an invention made by the cortical
human whose memory processes engendered foresight. Agriculture
allowed us to accumulate large stores of food, which could be stolen or
traded for other goods; hence, agriculture marked the beginning both of
realistic warfare and of business.

There is a human biological problem that is partially resolved
through business: One male can fertilize many females. Thus, in any
reproducing group, there are “surplus males,” which poses a problem for
the group. These males have a choice of becoming integrated into the
group, going elsewhere and starting a new group, or killing each other off.
Historically, whenever territory was available, daughter groups were
formed, and often there was interbreeding with the parent group. But when
space is no longer available, frustration and territoriality can lead to
emotional outbursts and aggression. With the advent of business, the
surplus males can be sent out on the road, exchanging goods and services.
Business keeps them busy and out of mischief.

But the accumulated stores of grains, livestock, materials and
equipment can also attract human predators, raiders who “must” be
repelled, lest all that stored “wealth” disappear. Before there were such
large stores, accumulated at one site, the populace could flee, or leave a
few token bits to appease the predators who weren’t really eager to fight
when the possible rewards were so slim. Even from the socio-economic



point of view, this is hardly the scene of packs of dragons looking for a
fight.

It is time to re-emphasize the glory of our emotions and motivations,
our feelings. Just as art, during the past century, has often focused on
deformation, on the ugly, science too, as in the dragon myth, has focused
on our flawed, deformed reptilian nature. But the time is now ripe—even
long overdue—to re-emphasize the constructive shapes and patterns of
human forms of creativity.

Even during my (now outgrown) radical behaviorist period, I wrote
papers declaring the dignity of humankind. Aristotle noted that happiness
is the pursuit of that which we are uniquely fitted to accomplish. We form
and perform music, we build architectural edifices, we write poetic epics
and novels, we construct engineering marvels, we create sumptuous menus
for mouthwatering meals and formulate credible and incredible theories.
Only the cortical primate accomplishes these forms—and only the cortical
primate accomplishes the horrors of human warfare in the service of
ideology and religion.

The “dragon”—if there is one—lies in our highly evolved and
trainable cortex, not coiled like a serpent waiting to strike unless
constantly controlled, buried in some ancestral remnant of the brain. So
we need to remember: what is constructed cortically can also be
deconstructed.



Re-Formulations



Chapter 18
Evolution and Inherent Design

Wherein I explore the role of inherent design in the evolution of mind.

There have always been, and still are, reputable biologists who
doubt that random hereditary changes can provide a sufficient
basis for evolution . . . An organ like the eye is not simply a
collection of elements—it is something which is gradually
formed—.

—C. H. Waddington, quoted in Arthur Koestler, The Ghost in the Machine,
1967

The impotence of Darwinian theory in accounting for the
molecular basis of life is evident not only from the analyses in
this book, but also from the complete absence in the professional
scientific literature of any detailed models by which complex
biochemical systems could have been produced. The conclusion
of intelligent design flows naturally from the data . . . design is
evident when a number of separate, interacting components are
ordered in such a way as to accomplish a function beyond the
individual components. The greater the specificity of the
interacting components required to produce the function, the
greater is our confidence in the conclusion of design.

—Michael J. Behe, Darwin’s Black Box, 1996

Darwin and evolution stand astride us, whatever the mutterings
of creation scientists. But is the view right? Better, is the view
adequate? I believe it is not. It is not that Darwin is wrong, but
that he got hold of only part of the truth.



—Stuart A. Kaufman, The Origins of Order, 1993



Inherent Design
Gary Marcus has documented the observation that evolution has left

us with a ”kluge,” a piecing together of working parts that are often far
from ideally suited to accomplish the tasks we face. This is certainly
correct. He uses the human backbone, the vertebral column, as an
example. For those animals that walk on all fours, the backbone works
well. For those of us who are bipedal, it often takes a bit of surgery to
repair the damage that ordinary getting about produces. In our brains, the
hippocampal system, the oldest piece of cortex, allows us to interleave
neo-cortically organized cognitions into our emotional and motivational
processes to determine our attitudes—with a variety of useful and
deleterious consequences. But my favorite example of a kluge is the
placement of sex between peep and poop. For most animals, and even for
us, the kluge works because the dynamic evolutionary process of creating
attractors builds on the ground plan with which we are endowed.

To achieve a working whole by putting together kluges, each kluge in
itself needs to be formed. To achieve a working whole, the formation of a
kluge cannot solely be dependent on selection of possibilities created out
of random variation. And furthermore, as indicated in the quotation that
introduces this chapter:

“… [D]esign is evident when a number of separate,
interacting components are ordered in such a way as to
accomplish a function beyond the individual components. The
greater the specificity of the interacting components [that is]
required to produce the function, the greater is our confidence in
the conclusion of design.“

To call evolutionary design “intelligent” (as proposed by current
creationism) carries an unnecessary connotation of an agent. A more
appropriate description is that the design is “inherent:” Acknowledging the
inherent self-organizing processes that form attractors, stabilities far from
equilibrium, enriches evolutionary theory; it does not diminish it.
Furthermore, by acknowledging that inherent design plays an important
role in the science of evolution, the wind is taken out of the creationists’
sails.



Dynamical system theory, the formation of stabilities far from
equilibrium is not just a frill in our understanding of evolution. A
complete theory of evolution needs to be based on the contextual
interrelatedness of all parts of living beings and the interrelatedness of the
living beings themselves. A self-organizing diversity and refinement in
design is continually being created by these relationships. For humans,
self-organization becomes extended throughout life by way of learning—
which entails self-organizing processes in the brain. As described in the
next chapter, this self-organization of the brain occurs by the same
“genetic” process that formed the person initially. The result is a more or
less inherently workable kluge which each of us experiences daily.

Darwinian Evolution
The usual scientific explanation of how our minds have been

achieved and how we form our civilization is by way of evolution. As
noted in the introductory quotations, some theorists, by contrast, have
proposed that we face the past and the future by way of “inherent design”
that pervades the world we navigate. But these viewpoints are not really
antagonistic: there is a complementarity between Darwin and Design. The
complementarity involves classification, agriculture and horticulture,
emphasis on processing and on complexity theory.

As Charles Darwin so beautifully described in his 1830s journals that
recorded observations he made as he served as a naturalist on the HMS
Beagle as it toured South America, the Galápagos Islands, Tahiti, Australia
and New Zealand, a multitude of unique biological creatures populates the
earth. Darwin noted the immense diversity of plants and animals and that
each species was exquisitely fitted to its environmental niche.

For the most part, the fit is the result of environmental influences that
shape the biology of the species, but, occasionally the species shapes its
environment, as in the case of beavers building dams and sand crabs
heaping sheltering mounds of sand. The ability to shape the environment
reaches a zenith in Sapiens sapiens.

Darwin worked within a century-old tradition that had established a
classification of biological organisms based on how closely organisms are
related to one another. Carolus Linnaeus, during the 18th century had
begun to classify this relatedness into orders, families, genera and species,



depending on the closeness of the relationship. Closeness was determined
not by surface similarity of form (called “phenotypic”)—such as that
between whales and sharks—but by relations among deeper forms (called
“genotypic”) that have to be diligently traced—such as that between
whales and elephants. This distinction between surface form and deeper
form is a theme relevant to how the human brain works, a theme we have
encountered throughout The Form Within.

Before Darwin, the most expert 18th and early 19th century
biologists, called “rational morphologists,” used the Linnaean
classification to describe “laws of form.” The accomplishments of
genetics now underpin Linnaeus’ observations and these laws of form by
cataloguing the biochemical substrates—Desoxy Ribonucleic Acid (DNA)
and Ribonucleic Acid (RNA)—of how the relationship among creatures
actually becomes formed.

It was during the latter part of the 19th century that a focus on
process began to surface. William James at Harvard University and Franz
Brentano at the University of Vienna were concerned with the “stream” or
“act” of thinking. Freud, a pupil of Brentano, developed a therapeutic
technique based on process: the tracing of motives that determine the form
of current experiences and behaviors. As we noted in Chapter 12, Freud
recognized that motivation, the patterns by which we navigate our world,
is formed by how we have processed our past, by our memory.

Darwin’s insight was based on the process utilized during millennia
of human practice: the diversification and selective breeding of a variety
of domestic forms through horticulture and agriculture. Agriculture and
horticulture do not use radiation to change a character; the breeding
process is much more deliberately designed. Darwin’s great contribution
was to introduce diversification and selection to account for the unique
fittingness of all biological creatures to their environmental habitats.

Selection was accomplished through survival by adapting to niches in
an otherwise hostile and competitive physical and social environment.

The deep form (the genotype) of diversification came in part
haphazardly, randomly, in part through sexual transactions. Sir John F. W.
Herschel, Darwin’s teacher, countered that random variation could not
account for the diversity of creatures; that some sort of design seemed to
be at work. In the January 2009 Scientific American devoted to a
comprehensive review of current Darwinian theory, Peter Ward, in an



article on the future of human evolution, notes that “evolution has indeed
shown at least one vector: toward increasing complexity.” This increase in
complexity is accomplished for the most part by regulatory genes
operating within constraints. When the constraints are loosened, new
stabilities far from equilibrium can become established. Thus we have at
hand a viable theory of inherent complexity to supplement (not replace)
“randomness” in accounting for diversity in evolutionary theory. Would
Herschel approve?

Forming Complexity
In short, there appears indeed to be a considerable amount of

“inherent design” not only in human achievements but in the natural order
of the world we navigate. It was left to 20th- century science to develop an
experimentally based theory of design that complements current
Darwinism. The theory, which we have repeatedly met in the experiments
described in these chapters, is called “dynamical systems,” “complexity”
or “chaos theory.” Ilya Prigogine, in a 1984 book Order Out of Chaos,
showed how temporary stabilities develop in chemical processes far from
equilibrium. The whirlpools that develop around rocks in a river are
everyday examples of such temporary stabilities.

For the theory of evolution, dynamical systems theory provides a
critical supplement to current evolutionary theory. Current theory
awkwardly accounts for initiating variety in forms and their refinement by
relying on random mutations. Such mutations more often than not
eliminate an embryo rather than provide fruitful change. By contrast, self-
organizing dynamic processes are inherently adaptive: Once conditions
are propitious, the self-organizing process tends to generate greater
complexity. Given tail feathers, a peacock has the potential, over
generations, to decorate and display a fan of such feathers.

In an interesting article entitled “Complexity and Adaptability in
Human Evolution,” published in a volume entitled Probing Human
Origins published in 2002 by the American Academy of Arts and
Sciences, Richard Potts makes the case that in order for organisms,
especially humans, to have persisted in variable, novel or shifting
environments, evolution of adaptability is essential. Adjusting to
environmental dynamics consists of refining adaptability by building a



larger storehouse of alternative genetic variations; a greater phenotypic
plasticity; and decoupling the organism from any single habitat—a freer
mapping of behavior onto environment—in short a “complexity response.”

Provided that specific complexity responses are not selected against,
the discovery of stabilities far from equilibrium helps account for the
progressive refinement we observe in the panoply of plants and animals, a
refinement that cannot be accounted for by random mutation and sexual
selection.

(For a detailed and exciting description of this theme (and his own
research) see Stuart Kauffman, At Home in the Universe: The Search for
the Laws of Self-Organization and Complexity, 1995.)

I hold that adding self-organization to Darwinian theory is important
because the form of the creatures we see around us need no longer be just
those that are the product of survival of the toughest. The survivors can
simply be decorative. Refinements of design such as the peacock’s tail
feathers, a human concerto, or the green and purple leaves in the trees of
our gardens make for the beauties of nature and of human endeavors,
important not only during sexual selection, but also for “us” to enjoy.

The English language has an ambiguous meaning to the term “fit.” Fit
can mean “fittingness” as when a glove or a shoe “fits”—as Darwin
described his observation during the voyage of the Beagle. Alternatively,
“fit” can have the meaning of being tough rather than weak, as advertised
by gyms and spas. Darwin shifted towards the meaning that fit means
tough in his later writings and his supporters parlayed his ideas into
“Social Darwinism,” which declared that the tougher (that is, the
wealthier) in our society are wealthier because their evolutionary heritage
had made them “fitter.” “Eugenics,” a term coined by Darwin’s cousin, Sir
Francis Galton, the purposeful breeding of the fittest and the elimination
of the unfit, became an early heritage of Darwinism.

In an essay (1985) Daniel Robinson, at that time head of the
Department of Psychology at Georgetown University and I developed the
theme, under the title “A Brainless and Mindless Evolutionary
Psychology,” that the (Darwinian) theory of evolution “displaced the
individual organism, including human individuals, from the center of
scientific concern and replaced it with the species en masse. Psychological
individuality, like biological individuality, was simply one of the entries in



the large table of natural variations from which the pressures of the
environment would select winners and losers.”

Skinner (1971)—whose more supportable contributions are described
in other chapters—in tune with classical Darwinian theory, called for a
society in which “the freedom and dignity of man were recognized as
anachronistic feelings of no utility in the process of survival.” Several of
us were appalled and, in rebuttal, published a group of essays (1973)
Beyond the Punitive Society: Social and Political Aspects (ed. Wheeler.) In
my contribution, “Operant Behaviorism: Fact, Factory and Fantasy?” I
declared that “Skinner accuses Koestler (some of whose work I have also
quoted in other chapters) and rightly so, as being some seventy years out
of date in misrepresenting behaviorism. But Skinner is equally out of date
in misrepresenting the physical and biological sciences, especially in their
information-processing aspects (and in misrepresenting the science of
psychology itself). Perhaps that is why Skinner and Koestler found
responsive chords in each other—we are privileged to watch and enjoy an
encounter between such antediluvian titans.”

The Theme of Self-Organization
Inherent design supplements and enriches the Darwinian approach to

the process of evolution. Our brain processes have made possible the age-
long evolution of self-organizing structures. Freud described the memory-
motive structures in the brain, showing that our cortex allows us to create
greater diversity, making it possible for us to more capably handle
complexities in navigating our world. What has been added since Freud is
that this diversity stems from using the same genetic processes
(algorithms in computer language) as those that form our heritage. But
complexity describes not just the organization of the brain’s substance but
also the form that organizes brain processing and is organized by it.
Cortical processes enable us to progressively self-organize; that is, to
implement and experience ever more diverse and detailed memories and
hence perceptions, attitudes and plans. The next chapter details how this is
accomplished.



Chapter 19
Remembrance of Things Future

Wherein I continue to develop the theme that the brain encodes a
compressed form of our experience, a pattern that serves as an attractor
that guides us in navigating our world.

. . . [T]he Muse of the Greek triad was named Mnemosyne,
‘Memory, one can have memory of the future as well as of the
past.’

—Robert Graves, The White Goddess, 1966

No preformed and complete structure has pre-existed anywhere,
but the architectural plan for it was present in its very elements .
. . The necessary information is present but unexpressed . . . the
building of a structure is not a creation; it is a revelation.

—Jacques Monod, Chance and Necessity, 1970



The Form of Memory
We call neural organizations that respond to patterns of excitation

“memory,” but biologists and psychologists refer to different processes
when they use the term “memory.” Biologists are interested in how
patterns become stored and how the stored patterns become transformed
over our lifetime. Psychologists are primarily interested in how we
remember patterns that have been stored. When storage is holographic, the
issue becomes how does a dis-membered store becomes re-membered?

Remembering
The process of remembering comes in a variety of forms, each form

dependent on the brain system involved:

1. One form of conscious remembering is composed of episodes of
experienced events and eventualities. Episodic memory is formed by
processes developed by the limbic basal ganglia such as the amygdala
as well as by the hippocampus and the prefrontal cortex.

2. Remembering skills and habits depends on processing by the central
part of the brain: the classical motor cortex and the corpus striatum
(cau-date nucleus and putamen.) Skills are processed unconsciously:
conscious intervention interferes with skilled performance. The
saying “keep your eye on the ball” (not on your arm swinging a bat or
tennis racquet) catches this aspect of skilled, habitual performance.

3. Remembering names (semantic memory) is processed by systems of
posterior cortical convexity. We strive to do this consciously.

4. Remembering images such as visual images and music is processed
by the primary sensory receiving systems, ordinarily a conscious
process.

Remembering is dependent on having stored (biologically) what is
being remembered. In addition the storage has to be organized if the stored
contents are to be retrieved. Thus storage has another component: the
stored items must be organized into labeled files—codes—that can be
more or less easily accessed. Coding has to be part of the storage process.
(The memory code basic to remembering is described later in this
chapter.)



Memory as Transformation
From the point of view of biologists, memory is embedded in matter,

and matter, in turn, is embedded in memory. That is, the form of the
memory-store becomes continually transformed: For instance, from
genetic patterns contained in a small acorn, a great oak grows. And the oak
reproduces that pattern in each acorn. Memory is a pattern that gives form
to matter. In the quotation that opens this chapter, the great French
biologist Jacques Monod has called memory an “architectural plan.”
Memory, when in use, is not just something stored in order to be retrieved.
Rather, the initial form of memory is a potential to become revealed, the
remembrance of an expression of something as yet unexpressed.

Thus memory is embedded at different scales of processing: from
molecules and membranes, to cells and patches of fibers, to body systems
including those in the brain—on to language, and to political, economic
and social-cultural groupings.

Indeed, from this viewpoint, our language itself is a memory system
that enfolds the wisdom of the ages. This view, which I share with many
scientists like Monod, is directly opposed to that of the “eliminative
materialists” encamped at the University of California at San Diego. These
materialists hoped to eliminate what they condescendingly like to term
“folk psychology” and would rather reduce our investigations to
knowledge of brain function (sometimes, as constrained by the findings
obtained in experimental psychology). The late Francis Crick, an advocate
for the “eliminative” agenda, epitomized the extreme of this group’s view
when he stated, “If we knew what every neuron was doing, we could
eliminate psychological formulations altogether.”

By contrast, I hold that our language not only embodies our shared
human experience but also provides us important entries to any
meaningful investigation of brain function.

A prime example of my view is the word “mind.” Its roots in Old
English are mynde and gemynde meaning “memory.” In turn gemynde was
composed of ge, which meant “together” and moneere meaning to warn.
Thus “mind” is based on memory and is used collectively to warn, or to
keep an eye on things, as in “please mind the store while I’m away.”
Minding is attending, looking out and ahead, grounded on looking back
and within.



There is both a retrospective and a prospective aspect to memory. The
retrospective aspect, re-membering, is familiar to us; its prospective
function, as addressed by Monod, has rarely been emphasized in the brain
or the behavioral sciences. The reason for this is that, until recently, we did
not understand the specifics of how an acorn begets an oak tree or how a
human sperm and an egg can blossom into the child you love. Memory
forms our present which determines our future.

Plus Ça Change
As the French saying goes, “The more things change the more they

remain the same.” Plants, insects, animals,— all things composed of
biological matter actually depend for survival upon a fascinating ability to
modify their memory without “changing” it. Our biological memory is
often grounded in material components with a short “shelf life.” In
mammals, our red blood count itself varies little over time, yet each
individual red blood cell disintegrates within a month. I was made aware
of this short cell life in a dramatic episode: During World War II, we came
to realize that blood serum divested of red cells was much more stable
than whole blood and worked just as well to pull wounded soldiers out of
shock. During my medical internship, I heard of a factory that was
producing serum to send overseas. I asked what they were doing with the
removed red cells: nothing. So I asked the factory to suspend the cells in a
saline solution, bottle it, and give the bottles to me. I had established a
small blood bank for my patients, using blood from their relatives, and
these surplus red cells amplified the contributions to the bank.

All went well for several months until one day, using one of the
bottles of red cells, I transfused a patient with ulcerative colitis who badly
needed a large quantity of red cells. Shortly she was passing reddish black
urine. This could be extremely dangerous if the hemoglobin of the disinte-
grated red cells should crystallize and block or damage her urinary system.
I immediately neutralized the urine so that crystals would not form. I sat
through the next hours with the patient, taking her blood pressure from
time to time, chatting with her. She felt fine all day and had no adverse
effects. With a sigh of relief, I called the factory to start controlling the
date when the red cell samples were procured: No bottles were to be kept



more than a week, lest the dis-integrating red cells leave us with “raw”
hemoglobin. All went well thereafter.

The same functional stability in the face of cellular change is true of
the brain. As we age, about 10% of our brain cells die off and fail to be
replaced. However, the places emptied are quickly filled with new
connections. Experiments performed during the early 1960s by some of
my former students and their collaborators first indicated the magnitude of
the opportunities to form such connections. The experiments were done in
order to discern the location of the input to the cortex from the thalamus
but unexpectedly yielded much more important results. A cyclotron was
used to radiate the brains of rabbits. The cyclotron emits radiation that
decelerates abruptly in soft tissue; thus circumscribed lesions can be
made. The destruction is sufficiently local so that the usual reaction to
grosser injury of neural tissue does not occur. Rather, space is made for
active new growth of nerve fibers.

As detailed in my Languages of the Brain (1971), much to everyone’s
surprise, within two to three weeks . . . “large numbers of normal-
appearing, well-directed fibers which were invisible either before or
immediately after the exposure to the cyclotron” totally filled the space
available.

These experiments were complemented by experiments on rats raised
either in restricted environments or in cages filled with toys, tires
suspended from the ceiling and so forth. “Comparison of the brains of
animals reared under different conditions resulted in a measurable
thickening of the cortex of rats given the richer experience.”

The route to reaching this conclusion was not straightforward. The
experimenters involved at the University of California at Berkeley (David
Kretsch and Mark Rosenzweig) were interested in assessing the role of
acetyl choline in learning. They found a 5% increase in acetylcho-line
esterase, a compound involved in acetyl choline metabolism. Someone at
the National Institutes of Health (NIH) which was funding the
experiments, pointed out that there was at least a 5% error in the technique
the experimenters were using. Also, the measurement of the cortical area
excised as samples for chemical assay was deemed inaccurate. The NIH
threatened to withdraw funding.

Several of my colleagues and I were appalled. Kretsch and
Rosenzweig were acknowledged reputable scientists and professors. The



funding was for a grant— not a contract. With a contract, the investigator
is bound to carry out what he has applied to do. With a grant, the
investigator is free to use the money for whatever research he/she chooses
to pursue. If the granting agency disapproves, it does not need to renew the
funding. But it cannot and should not interfere in midstream.

My colleagues and I got on the phone to NIH and the whole matter
was dropped. But, properly, Kresch and Rosenzweig took the criticism to
heart—as did their students. One of them solved the area-of-cortex issue
immediately: he plunked the excised area of cortex into a beaker and
accurately measured the displace volume of water—à la Archimedes.
Marian Diamond, a superb neuro-anatomist, joined the project and found
that the cortex of the “enriched” animals had thickened. Detailed
histological analysis of the thickened cortex showed that “the number of
nerve cells per unit volume actually decreased slightly. The large part of
the thickening of the cortex was due to an increase in non-neural cells, the
glia. Also, there was an increase in the number and distension of dendritic
spines, small hair-like protrusions on dendrites, presumed sites of
junctions between neurons.” More on the role of glia shortly.

The impact of these findings can be gauged from the following
incident. In the mid-1970s, the United Nations University hosted a
conference in Paris on the subject of the brain/behavior relationship. The
conference was very up-beat as we all had a great number of new findings
to report. There was one exception: the reports made during one session on
the progressive loss of brain cells during aging and other “insults” to the
brain. For instance, a small blow to the head would result in the loss of
thousands of brain cells.

I was sitting next to an elderly gentleman who progressively slouched
further and further in his seat. In an attempt to cheer him up, I wrote a note
to him regarding the research findings described above. The note worked.
He sat up straight and listened more carefully to the ongoing presentation,
and then left. We were not introduced; I gave my presentation somewhat
later in the program.

Imagine my surprise when the person asked to summarize the
program was introduced: It was the elderly gentleman and his name was
Lord Adrian, now 90 years of age. My further surprise came when Lord
Adrian introduced his summary by stating how depressed he’d been by
hearing about all the cell loss we experience, until “Dr. Pribram introduced



me to new data that show that as cells disappear they make room for new
connections to be made.” He then gave a brilliant summary, mentioning
from time to time that the results he was reviewing were making new
connections in his brain!

The important point here is that biological memory is a form, a
pattern that is composed of relations among parts, among shapes that are
often individually, themselves, short-lived. Our skin, the linings of our
lungs and intestines, and yes, even the brain, each cycle their component
cells while maintaining their form—their structural integrity—their
memory. In biology, psychology and computer science we therefore refer
to process—the pattern of relations—as structure. Thus Miller, Galanter
and I could write a book entitled Plans and the Structure of Behavior (as
did Maurice Merleau-Ponty in his The Structure of Behavior), indicating
that often brain and behavioral processes work in similar ways.

The Being and Becoming of Electrical Brain
Activity

In the early 1990s in my research laboratory in Virginia, we made a
fascinating discovery, summarized briefly in Chapter 16, that should not
have surprised us, but did. I had recently moved my laboratory from
Stanford, California, and we were setting up new brain electrical recording
equipment. I served as the first subject during tests of the equipment. I
read, rested, and occasionally dropped off to sleep while we recorded my
brain electrical activity. Min Xie, a superb researcher who had received his
training in China and was now obtaining his PhD at Virginia Polytechnic
Institute and University, recorded my brain waves. He selected, at two-
millisecond intervals, the highest (squared) amplitude recorded from any
point on my skull. The resulting video of my “brain scene” came as a
complete surprise to me. The electrical activity of my brain changed so
rapidly that it was hard for the computer to keep up with the rapid
fluctuations. Once we were able to quantify what we had just recorded, we
realized that my brain electrical activity was continually changing at the
rate of around 100 times a second or more, no matter whether I was asleep
or awake or what I was doing! How could we humans ever be able to
construct memories of any duration, given this astonishing rate of
fluctuation in brain activity?



The answer to this question came when we obtained another
unexpected result. We now recorded from subjects who were performing a
variety of tasks. Min Xie’s wife, Bibo Xang, plotted the pattern of change
in the location of the highest amplitude of brain activity recorded every 2
milliseconds. The patterns are shown in the accompanying figure. As
might be expected, the brain patterns were unique to each individual and
also differed from task to task. The surprise came when we repeated this
experiment using recordings over longer periods—20 seconds instead of
the original 5 seconds. We found that the patterns were the same in the two
recordings. In fact, even after a lapse of six months, when we recorded
from the same person performing the same tasks, we again saw nearly the
same pattern. The patterns matched so closely from one recording to the
next that I was actually able to identify the person whose record I was
looking at.



77. Brain Electrical Activity Over a 5.5- and a 20-Second Period

These experimental results demonstrate a stability of pattern, of
form, despite extremely rapid changes in our moment-to-moment brain
electrical activity. I therefore searched for a measure that would
demonstrate this stability. My intuition suggested that complexity theory
(which also goes by the names “non-linear dynamics” and “chaos theory”)
would provide the necessary measurements.

We tried several measures of complexity that had been developed in
other laboratories but found that the EEG is produced by so many sources
of variability that we could not tell whether the variability was random,
produced by irrelevant changes (noise), or truly structured complexity.

Finally I got in touch with a friend, Paul Rapp, then of the
Department of Neurology of Drexel University, who had been working on
measuring the complexity of EEGs for a decade and was considered the
“expert” in this line of research. He had published many papers by the
time I got in touch with him. He was not very happy and was writing a
critique of his own work! He had just realized that many of the measures
he had used could not distinguish complexity from random fluctuations in
the brain’s electrical activity.

But, as discussed in Chapter 16, he did have one technique that
seemed to work: a data compression scheme that searches for a pattern
that repeats itself within a string of symbols. To do this we take the EEG
recording, one electrode at a time, and we follow that recording to where a
change in the pattern of the electrical brain activity in that electrode takes
place. We mark the pattern with a letter. We go along the record until
another change in pattern occurs. If the next pattern is the same as the
earlier one, we mark it with the same cipher. If the pattern is different we
mark it with a different cipher. Next we go along the set of ciphers to see if
there are any repetitions of sequences of ciphers. Next we go to a new
level, generating a new shorter string until there are no repeats of identical
strings. By proceeding to analyze the brain electrical activity in this
fashion, one electrode at a time, we observed in each electrode more levels
where repetitions occurred.

We analyzed the brain electrical activity obtained with 128 electrodes
using this technique and found that the “algorithmic complexity,” as it is
called, of the EEG increases significantly the older we get. Since the
measure of algorithmic complexity was devised as a method of



compressing data, this means that as we grow older our brains manage
more and more to compress our experienced memory. No wonder it takes
older people longer to remember, to decode the compressed experience,
and to act on what they remember!

The results of this experiment, if they can be generalized, show that
memory becomes compressed for proactive use, much as in a digital
camera. You store the image until you are ready to use it (print it or store
it in your computer.) As we will see, the process in the digital camera and
in making images during tomography and fMRI follow the same
principles as those we followed in processing our EEGs.

A Useful Metaphor
Paul Smolensky, in a 1996 paper outlining the foundations of his

harmonic theory of memory processing, wrote my favorite description of
the form, the patterns that our cortical processing must take. Although it
might be considered a masterpiece of indecipherable jargon, I like this
quotation because it encapsulates both what the brain’s self-organizing
process, its memory, is not like and what it has to be like. Paul Smolensky
puts it this way:

The concept of memory retrieval is reformalized in terms of the
continuous evolution of a dynamical system toward a point attractor
whose position in the state space is the memory; you naturally get
dynamics of the system so that its attractors are located where the
memories are supposed to be; thus the principles of memory storage are
even more unlike their symbolic counterparts than those of memory
retrieval. (Smolensky, 1996)

Crystal Clear? No?
Here is an analogy, based on the fact that a musical tone can be

described in terms of Gabor wavelets, that may help clarify Smolensky’s
meaning: “The concept of memory retrieval is somewhat like the concept
of conducting an orchestra. At any moment, the performance is a
dynamical evolution of musical phrases.”

The evolution develops as a consequence of structured repetition.
Leonard Bernstein exclaimed in one of his broadcasts: “Repetition, ah
there’s the rub!” In Chapter 16 and above I described the complexity of



brain electrical recordings in terms of the structure of redundancy
(repetition) that allowed us to compress a representation of brain electrical
events into a hierarchically organized code. Formation of the hierarchy
consists of a continuous evolution until the highest level of compression is
achieved. This highest level constitutes an attractor toward which the
process strives.

The complex sounds of music that make up a sonata can be similarly
“chunked” and compressed into a hierarchy of phrases that are represented
by the score. A score— the memory—is shared by the performers and
guides the conductor who has the task of de-compressing the score in his
own image. But the signs that are scrawled on the score and the gestures of
the conductor are very unlike the music we actually hear. They are not just
correlated with the music—they are coordinate(d) with what we
experience.

Furthermore, the musical phrases are constrained by “bars” which
indicate time limits on the expression of the tones. Tones are described in
terms of their frequency while bars determine tempo. Thus, the musical
form can be described as a windowed Fourier transformation, a Gabor
function or related wavelet.

If we change this metaphor slightly, we can see that the “bars” of a
symphony are very like the bar codes used today at supermarket checkout
counters. The scan of the patterns of bars addresses the inventory (the
musical tones) present in the store. The permutations of width and spacing
are nearly infinite. The process is dynamic in that the store’s inventory is
updated with every sale. The bars in a score of music are constraints, the
“windows” on the Fourier process that describes the tones that make up
the Gabor-like windowed Fourier process.

Based on descriptions of the maps of receptive fields of cortical cells,
I have proposed that the deep structure of our memory, as it is encoded in
fine fibers in the brain, is much like a windowed Fourier transformation.
The surface structure of memory, our retrieval and updating procedure,
resembles what goes on at a checkout counter. It is only a step to conceive
of a name as a bar-code trigger identifying a purchase.

The recording of a bar code form of memory has been recently
accomplished. Joe Z. Tsien, now head of the Shanghai Institute of Brain
Functional Genomics, translated the records of the activities of neural
ensembles into a string of binary code in which a 1 represents an active



state and 0 represents an inactive state for each coding unit. His recordings
were made with an array of 260 electrodes that followed the activity of
hippocampal cells in the laying down and retrieval of memories of
startling events that mimicked a falling elevator or the shake of an
earthquake. Tsien was able to predict within a 99% accuracy, which event
had been experienced and where it had happened.

In my opinion, Smolensky’s description, implemented by Tsien,
encapsulates a great deal of wisdom. Smolensky alerts us to what memory
in the brain is NOT. Whatever memory is, it cannot be described as having
the same form as does our experience or behavior. Thus, the forms of
memory cannot be identical with those of our experience nor with the
world we navigate; they are transformations that coordinate our
experiences with that world. These transformations change the
coordinates—for instance, firing rates of axons to a binary code and vice
versa or neuro-nodal activity to [musical] scales and vice versa.
Successive transformations form the oak from the acorn.

Order Out of Chaos and Autopoiesis
Another attempt to bring measurement to chaos theory was the

concept of autopoiesis, or self-creation. Autopoiesis provides an important
example of how we scientists often stumble onto refreshing ways of
thinking about our existing observations. Francisco Varela, a Chilean
biologist working in France, in his 1979 book Principles of Biological
Autonomy wrote a highly technical and mathematical treatise on
“autopoiesis,” a term he coined. Autopoiesis appeared to me to be similar
to the concept of “temporary stability far from equilibrium” for which Ilya
Prigogine had received his Nobel Prize in Chemistry.

For instance, the pendulum of a grandfather clock under the influence
of gravity swings to and fro until it comes to rest at the nadir, the bottom,
of the arc described by the swing. But there is also another more
temporary point of stability at the uppermost point, the zenith of the same
arc. This feature of temporary stabilities is clearly demonstrated and used
to good advantage in roller coasters and other “gravity defying” rides in
amusement parks.

Temporary stabilities far from equilibrium are considered to be
“attractors.” The term “attractor” derives from the fact that we plot the



paths by which the stabilities are attained. These path-plots serve to
indicate where we will find the stabilities in the future. For a roller
coaster, such plots are essential: we need to be sure that stabilities (far
from equilibrium) over the course of the coaster remain stable lest some
major tragedy occur. Thus these paths toward future stability serve as
indicators as to where the stabilities are to be found. That is, they serve as
attractors defining the future.

Prigogine and Isabelle Stenger’s landmark 1984 book Order Out of
Chaos describes the principles by which these findings are generated. The
science of such temporary stabilities has since become known as “chaos
theory.” However, if we take into account these temporary stabilities (as
for instance in our weather), “turbulence theory” might be a more
appropriate term. Eddies that form around rocks in a river provide a good
everyday example. The river flows by, always changing, but a visible
pattern, the form of the eddies, remains constant.

One evening in 1962 when Francisco Varela and Ilya Prigogine were
both dining at my home in Palo Alto, a heated argument took place:
Prigogine insisted that Varela’s autopoeisis was a totally different
principle from Prigogine’s forms of temporary stabilities. The argument
was not settled that same evening, but shortly thereafter, our colleague and
friend, the MIT mathematician Heinz von Förster, was able to clarify this
issue to our common satisfaction: Varela had described form as self-
sustaining despite changes of the system’s components, while Prigogine—
as detailed in his 1980 book From Being to Becoming—is instead
describing the process of initial formation, the architectural plan by which
such a system self-organizes in order to become formed.

But as we saw in the recordings of brain electrical activity and shall
see throughout these chapters, both a self-sustaining and a self-creating
system require some form of self organization.

Are We Sleepwalkers?
Arthur Koestler’s 1959 book The Sleepwalkers describes the partial

and often befuddled insights of the scientists who initially changed the
earth-centered view of the cosmos to a sun-centered view. Our
understanding of such concepts develops gradually. Our current groping in
the brain/behavioral sciences reminds me very much of this groping of the



early cosmologists. For instance, we know that in AD 1500 Copernicus
had already been informed by his contemporary Portuguese navigators
about the discoveries, some two thousand years earlier, of Pythagorean
astronomers and mathematicians. Aristarchus had shown, as early as 280
BC, that our cosmos was heliocentric, sun-centered. These early
mathematicians had also figured out that the earth was round, and even,
with surprising accuracy, what its circumference might be.

This same information—from the same Portuguese sources—was
available to Columbus before he undertook his voyage of discovery. But
Copernicus had been too unsure of his own rather sparse findings to
publish them. His students prevailed. Copernicus’s findings were
published, and after his death his followers continued to promote his
views. Kepler filled in the essential data that would have made Copernicus
feel more secure in his views. Nevertheless, Kepler had still failed to
attain the insight regarding the “forces”involved in securing the
heliocentric view, which Galileo and Newton would provide, enabling us
to complete what we now call the Copernican revolution.

Remembering as Self-Organization
Ordinarily, remembering is divided into a short-term process, which

George Miller, Eugene Galanter and I baptized a ”working memory” in
our 1960 book Plans and the Structure of Behavior, versus a process that
persists for a longer period. Different brain processes serve these two
different ways of remembering.

Psychologists have devoted considerable effort to investigating how
short-term memory “consolidates” itself into a long-term store. During a
series of conferences at Princeton in the 1960s that I organized and
chaired, I addressed this issue. The proceedings were subsequently
published in four volumes entitled Learning, Remembering and
Forgetting. Among the notable scientists I had brought together at these
conferences were my Stanford colleagues Gordon Bower and Richard
Atkinson. Atkinson, with his student Al Shiffrin, had proposed a model of
learning that still remains popular, in which they describe the progression
of memory as moving from short- to long-term storage.

At these Princeton conferences, as well as later, I consistently
disputed this model by pointing out that our short-term memory actually



addresses our long-term store, since we pull out currently relevant items
into a temporary “register” for current use. For instance, in a frequently
used example, when we try to remember a phone number, we are
addressing our long-term memory base of numbers, phones, phone books,
and so on—a base from which we are able to choose the currently useful
sequence of numbers. If that sequence becomes important to us, it must
become incorporated into our long-term memory. “Consolidation” is,
therefore, not a solidification of an experience, as the term seems to imply.
We use the short-term store to address and to transform our long-term
memory—a working transformation that incorporates our current
experience into our long-term memory to tag it as valuable for possible
future use. Thus, remembering is a self-organizing process.

Recently K. Nader and his colleagues (2004) have demonstrated such
self-organization to occur by showing that the protein synthesis that takes
place in the amygdala in the formation of a memory becomes
“reconsolidated“ with every new exposure to the provoking stimulus.
Reconsolidation involves proteins that are configured differently from
those taking part in initial consolidation.

Recall the experiments described earlier in this chapter, in which I
showed that the structure of redundancy in the EEG forms the persistent
pattern that we could recognize. According to the data just reviewed, this
pattern becomes formed by “reconsolidation” at a rate of more than 100
times per second by the electrical patterns in the brain!

Boning Up
In my 1971 book Languages of the Brain, I describe how our memory

processes become formed by electrical excitation of a nerve cell. Such
excitation leads to the activation of a messenger molecule—ribonucleic
acid (RNA)— that, in turn, activates deoxyribonucleic acid (DNA), the
molecule that carries our genetic memory. I based my description on the
work of Holger Hyden, of the University of Gøtesberg in Sweden, who, as
early as 1955 had shown that electrical stimulation of neurons leads to
copious secretions of RNA which were transferred to the glial (Latin for
“glue”) cells surrounding a nerve cell. I conjectured that the RNA would
then stimulate the glial cell to divide, and thus allow the nerve cell to grow
and divide, making new fibers. The growth of the nerve fiber would thus



be guided by the glia to make new connections—connections that would
contribute a structural basis for forming our memory processes.

However, when I first read Hyden’s reports on RNA and glial cells, I
had some initial reservations. He claimed that he had dissected—by hand,
under a microscope—glia from the nerve cells that they enveloped. I
invited Hyden to a meeting sponsored by the University of California at
San Francisco, and we soon became friends. It has been my habit,
whenever possible, to visit a laboratory when the investigators report an
important finding that puzzles me. So, shortly after the California
meeting, I went to Sweden to see for myself how Hyden was performing
his dissections. Hyden sat me down and handed me tissue from the
vestibular nucleus of the brain stem, along with a microscope and a
needle. By the end of the morning, my training in surgery had made me as
adept as any of his staff at separating glia from neurons: I had a pile of
nerve cells and a separate pile of glia cells ready for Hyden to measure the
amounts of RNA in each.

Today and over the past few years, the focus of research on the
secretion of RNA has been on the question of how its copious secretion is
generated by nerve cells. The membranes of fine nerve cell fibers are
composed of channels through which electrolytes—small molecules such
as potassium—are exchanged. These electrolyte exchanges are regulated
by the concentration of calcium. Our brain cells live in a virtual sea of
calcium ions—that is, in a calcium-rich watery swamp just outside each
cell’s membrane, a swamp created by the phospholipid composition of the
membrane. (The lipids compose the tough part of the membrane; the
watery part is contributed by its phosphates.)

However, within an individual nerve cell and its fibers, the
concentration of calcium ions is ordinarily extremely low: 20,000 times
lower than the concentration outside the cell. When the electrical
excitation of a nerve cell and its fibers reaches a certain threshold, the
calcium channels open briefly. A spurt of calcium ions enters the nerve
cell and initiates a chain reaction that activates a part of the DNA in the
nucleus of the nerve cell to “liberate” messenger RNA into the cell’s
cytoplasm—and from there into the adjacent glia.

The most interesting part of the story (as told in Scientific American,
February 2005) is that it is not the amount of the electrical excitation that
is important in the transcription from the electrical to the chemical code.



Rather, it is the form, the temporal and spatial pattern of the electrical
signal, that becomes encoded in our brain cell’s chemical transactions.

As Douglas Fields, the author of the Scientific American article, put
it: “We observed that [electrical] signals of different temporal patterns
propagated through [spatially] distinct pathways that were favorably
tuned to those particular patterns and ultimately regulated different
transcription factors and different genes.” [My emphasis.]

Where in our intact brains do such spatially distinct temporal patterns
arise?

As a large nerve fiber approaches a synapse where it will connect
with other cells, the large fiber branches into many fine fibers. I had
recognized early in my research that learning ought to involve the
strengthening of the synaptic bond of some of these branches, rather than
all of them. But for years, I had no suggestion as to how such selective
strengthening might occur. At many conferences over the years, my
colleagues and I explored the possibility that feedback from postsynaptic
sites could produce such selectivity—but for decades no evidence for such
an effect was forthcoming.

However, recently evidence has accrued that selectivity in the
presynaptic fibers does result from feedback initiated by the postsynaptic
membrane. This evidence was obtained from recordings made in
hippocampal tissue, and the selective feedback is potentiated chemically
(by different endogenous cannibinoids).

The postsynaptic origin of this feedback accomplishes the selective
presynaptic patterns of spatial and temporal frequencies described by
Douglas Fields in his Scientific American article. Such patterns, once
initiated, perpetuate themselves and form the kernel for further
development—the development of self-organized complexity.

I love the fact that calcium ions play a significant part in the process
of remembering: we really do “bone up” before an exam.

Learning as Self-Organization
Operant conditioning is a vehicle for self-organization. In order to

begin the conditioning procedure, we “shape” the animal’s behavior by
using a reward whenever the animal behaves in the way we want. After
that the animal (or human) controls his own opportunities to obtain



rewards in an ongoing self-organizing process. In Chapter 8, I explained
the functions of the motor cortex as forming “acts”—that is, forms of
activity that flexibly use component movements to achieve targets. As B.
F. Skinner, professor of psychology at Harvard University, was fond of
pointing out, the same is true of operant conditioning: For the
experimental psychologist, “behavior” is not the description of a
movement but ”the (paper) record the psychologist takes home at night to
study.” For the experimental psychologist, ”behavior” is an “act,” an
environmental consequence of movement, not a movement per se. For
Skinner it mattered not whether the behavioral record was made by the
pecking of a pigeon, the press of a panel by a rat, or the depression of a
lever by a monkey—or, for that matter, the repeated movement of a
human.

I had great fun in using the operant technique during lectures given
by Ilya Prigogine in the decades (1970s and 1980s) when we repeatedly
interacted. I was always most interested in what Prigogine was saying and
found myself making generally approving gestures. I noticed that
Prigogine was moving toward where I was sitting in the front row of the
lecture hall. So, on several occasions I deliberately continued nodding
enthusiastically until Prigogine was eventually standing right in front of
me and addressing his lecture to me, to the exclusion of the rest of the
audience.

In my 1971 book Languages of the Brain, I note that there is
continuity between how our bodies become organized as we are in the
process of growing from an embryo and how our memories become
organized as we are in the process of learning. In that book I spelled out
the parallels that exist between our genetic development and how our
behavior becomes conditioned. I suggested that our memories are
“induced” in our brain much as our tissue development is “induced”
during embryological development. Recently, my Georgetown students
have updated this terminology by pointing out that induction is composed
both of promoters and inhibitors.

In Languages of the Brain I outlined the similarity between biological
induction (promotion and inhibition) as it occurs in embryological tissue
and behavioral (psychological) reinforcement as it occurs during operant
conditioning. The similarities are striking:



a. Inductors (like RNA) evoke and organize our embryonic genetic
potential. Reinforcers (in operant conditioning) evoke and organize
our behavioral capacities.

b. Inductors are specific in the patterns they evoke in a tissue, but they
can evoke these patterns in all sorts of organisms and tissues.
Reinforcers are specific in the behaviors they condition but they can
evoke them in all kinds of organisms—pigeons, rats monkeys or
humans—and all kinds of tasks: running mazes, pressing levers, or
capturing a lecturer’s attention in a conference hall.

c. Inductors determine the broad outlines of the induced pattern: that is,
whether they are promoters or inhibitors; details are specified by the
action of the substrate, the tissues upon which they are working.
Schedules of reinforcement determine the general pattern, increase or
decrease, of the change in behavior; details such as pecking a panel,
running in a running wheel, depressing a lever or turning toward
someone in an audience, the details of the behavioral repertoire
achieved are idiosyncratic to the organism.

d. Inductors do not merely trigger our development; they are more than
temporary stimuli. Reinforcers do not just trigger changes in
behavior during learning; they are a continuing influence on our
behavior during subsequent performance.

e. Inductors, in order to be effective, must be in intimate contact with
the tissues, their substrate, upon which they are working. Temporal
and spatial proximity between a stimulus and the opportunity for
response, such as a lever, is a demonstrated requirement for
reinforcement to occur.

f. Mere contact, though necessary, is insufficient to produce an
inductive effect; the induced tissue must be ready, must be competent
to react. Mere proximity, though necessary, is insufficient to produce
reinforcement: the organism must have previously been “shaped,”
that is, made ready, competent to learn.

g. Induction usually proceeds via a two-way interaction between an
agent and the substrate, the tissue, in which it operates—by way of a
chemical conversation. Reinforcements, the consequences of
behavior, become the “images of achievement,” the targets of further
actions.



Back in 1971, when I wrote Languages of the Brain, I also described a
few chemical reactions that occurred in the brain during the reinforcement
process. But at the time there were few experimental results to draw on
that showed how the changes that occur could produce the patterns of
learning and of memory. Since I wrote those passages, we have learned a
good deal, as I’ll describe in the next section.

Creating Form in Biology, Brain and Behavior
In his superb 2004 book The Birth of the Mind, Gary Marcus

addresses the very issue that had so concerned me in 1971: the connection
I had sensed between our embryological development and the brain
processes that are involved in learning and the formation of our memory.
In what follows, I gratefully acknowledge my indebtedness to Marcus’s
detailed descriptions and his insights regarding the results of these recent
experiments—insights which happily resonate with those that I had
already begun to develop as early as the mid-1930s in an honors paper I
wrote at the University of Chicago entitled “Meiosis and Syngamy.” In
that paper I described the importance of syngamy, a process in which a
cell’s mitochondria and other protoplasmic forms are guiding development
through an interaction with the nucleus of the cell while it was dividing;
that is, meiosis.

A great deal of current popular and even scientific thinking rests on
the mistaken notion that there is some sort of one-to-one correspondence
between a particular gene or portion of our DNA with a particular
characteristic of our body, our brain, or our behavior. But as been
effectively demonstrated by the Human Genome Project, we have too few
genes to account for the incredible complexity of our bodies. Furthermore,
the same DNA is involved in creating our nose as in creating our toes! The
RNA generated by the DNA must somehow produce proteins that are
specific to a nose or a toe. There must be some local effect produced by
our cells, an effect that potentially composes a nose and a toe, and which
also can determine their eventual difference. Neither the DNA itself
(which is the same all over the body)—nor its RNA—determines
characteristic membranes, organs and behaviors.

Gary Marcus invokes the particular connections of the cells as
forming the substrate within which the RNA operates to express the DNA’s



potential. He notes that we can divide the connections in the brain into
systems or modules: some are specialized for processing vision, others for
processing hearing. However, Marcus indicates that specific patterns of
genes do not characterize the formation of one or another module. Rather,
the modules are determined by patterns of genetic molecules that operate
as gradients that can overlap to produce the brain’s modular structure.
Thus, the discovery in my laboratory of receptive fields in the visual
cortex that respond to specific bandwidths of auditory stimulation can be
explained by such an overlap between auditory and visual genetic
gradients.

Gary Marcus points out that the genes that produce genetic gradients
do not operate singly, they work together in forming specific modules.
Marcus uses chess as a metaphor to describe the power of genes to create
diversity of patterns in our brains. He takes the example of a chess-board
of 64 squares as the initial base upon which the game is to be played.
There are thirty-two chess pieces (genes). They come in two sorts: black
(promoters) and white (inhibitors)—sixteen of each. Let the game begin!
A huge range of possible moves results in a mind-taxing set of patterns of
moves (memories) on the chessboard and the resulting momentary
patterns of arrangements of chess pieces. By dividing the chess pieces into
only two categories, the game (the process) can generate an infinite
variety of patterns from even a limited number of initial conditions.

Thus, the processes that operate in our brain that enable learning and
that form our memories are identical to the genetic processes that initially
form our bodies, brains and behaviors. Marcus sums up our insights
elegantly:

Nature has been in the business of building biological
structures for an awfully long time, enough to dope out many of
the best tricks and . . . stingy enough to hold on to those tricks,
once they were discovered. For the most part, what is good
enough for the body is good enough for the brain . . . We can now
understand better than ever just how deeply we share our
heritage—physical and mental—with all creatures with whom
we share our planet.

In Summary



I have emphasized several things we have learned about memory that
are not commonly acknowledged in texts on how brain processes organize
the form of memory.

First: Matter is embedded in memory. Memory is not matter itself,
but memory is the form that organizes matter. Diamonds and coal are
made of the same matter, but in its crystalline form, diamond is
considerably different from a chunk of black soft coal.

Second: Our biological memory depends on “separating” a long-
lasting form from its material components, the form of memory from its
material substrate. In the philosophy of science this ability to separate
form from substrate is more generally called ”functionalism.” It matters
little whether the computer is an IBM, Dell or Apple. Properly constructed
word-processing programs (plans) will run on any of them.

Third: Biological and especially brain memory is composed of
complexly structured forms. A hierarchical structured memory is formed
by repetitions from which identical repetitions of sequences are extracted
and represented in a separate layer—a process that is repeated until no
more identical sequences are formed. The hierarchy is thus a compressed
representation (a code) of what is to be remembered.

Fourth: This compressed code serves as an attractor that guides
navigating our world: memory is the basis for our ability to remember the
future.

Fifth: Musical notation and superstore bar codes provide useful
metaphors that allow us to formulate specific questions and answers about
the design of our brain’s processing procedures.

Sixth: The form of our memory is different from the form of our
experience and of our behavior. Thus we ought not to be satisfied when we
just correlate these forms. For full comprehension, we need to understand
the “how” of the relation between them, how they become co-ordinated.
Coordination involves trans-formation.



Chapter 20
Coordination and Transformation

Wherein I indicate that transformations among complementary
formulations of brain/behavior/experience relationships often have greater
explanatory power than do emergence and reduction.

You wonder that there are so few followers of the Pythagorean
opinion, while I am astonished that there have been any up to
this day who have embraced and followed it. Nor can I
sufficiently admire the outstanding acumen of those who have
taken hold of this [Pythagorian] opinion and accepted it as true:
They have, through sheer force of intellect, done such violence to
their own senses as to prefer what reason told them over that
which sensible experience showed them to be the contrary.

—Galileo Galilei, Dialogue Concerning the Two Chief World Systems,
1632

“The other possibility leads in my opinion to a renunciation of
the space-time continuum, and to a purely algebraic physics.”

—Albert Einstein quoted by Lee Smolin, The New York Review of Books,
2007

One day William Rowan Hamilton . . . was crossing Phoenix
Park in Dublin, when foreknowledge came to him of an order of
mathematics which he called ‘quaternians’, so far in advance of
contemporary mathematic development, that the gap has only
recently been bridged by a long succession of mathematicians.
All outstanding mathematicians have this power of making a
prodigious mental leap into the dark and landing firmly on their



feet. Clerk Maxwell is the best known case, and he gave away the
secret of his ‘unscientific’ methods of thought by being such a
poor accountant: he could arrive at the correct formula but had
to rely on his colleagues to [try to] justify the result by
pedestrian calculation.

—Robert Graves, The White Goddess, 1966



Coordinates
In the previous chapter I emphasized how changing coordinates—

rather than just making correlations— is important to understanding
current explorations in the behavioral and brain sciences. We must look to
changes in “order,” in the co-ordin-ates within which such descriptions are
made.

In the chapter on codes and their transformations in Languages of the
Brain (1971), I stated that:

. . . whether something is remembered is in large part a
function of the form and context in which it is experienced. In
technical language this is the problem of transformations or
transfer functions that make coding and recoding possible. When
we address our computers in a natural language, such as English,
that input is transformed via a series of processing ‘languages’
such as WORD, PASCAL and the like until the configuration of
that language can address an octal (or hexadecimal) number
code which, in turn, can communicate with the binary, on and off
code of the computer hardware. In turn, when the computer has
to communicate with my printer the reverse set of
transformations has to occur.

When I was first confronted with a general purpose computer in the
1950s, it had to be started by entering a set of 20 commands, each
command composed of setting a sequence of 12 switches in an up (on) or
down (off) position. The demands on memory (20 times 12) were great:
any single mistake would require the entire set to be repeated.
Programmers were quick to get around this difficulty by “chunking,”
dividing the 12 switches into sets of three and labeling each set with a
number:

0 indicates DDD; 1 indicates DDU; 2 indicates DUU; 3 indicates
UDU; 4 indicates UDD; 5 indicates DUD; 6 indicates UUD; 7 indicates
UUU. Thus the programmer had to remember only four numerals for each
command (e.g. 1213.) This transformation is called an “octal” coding of a
binary system. The coordinate “binary” was changed to the coordinate
“octal.” The procedure made possible a transformation from a pattern
where complexity resides in the arrangement of simple elements to an



arrangement where complexity resides in the unique meaning of each
component element. Today I would modify this statement only by noting
that meaning does not reside in the component elements themselves;
rather these elements are able to convey meaning when placed in the
appropriate context.

Transformations due to changes in coordinates are not new, but their
power as such has not been highlighted. As an earlier example, take our
experience of the alternation of day and night. Early man observed the
correlation of this alternation with the rising and setting of the sun. Our
almanacs still indicate the time of sunrise and sunset for each day of the
calendar, even though everyone is well aware that it is the Earth that is
moving. Correlation is, however, the first step in our ability to come to an
understanding of our observations.

Many scientists and philosophers believe that the next step after
correlation would be the establishing of an “efficient cause” that explains
the relationship. In the above example, the “ efficient cause” of our
experience of day and night is the alternation of “sun-rise and sun-set.”
This is a step forward in relating the patterns of our experience to the
patterns of the “cause” of those experiences. But to establish a meaningful
understanding of the “cause” of the sun-rise and sun-set, and therefore of
our experience of them, this is not enough. In fact, we now know that the
sun does not rise or set: the Earth rotates around its axis, making it appear
that the sun is rising and setting. This insight, a part of the Copernican
revolution, demanded a total change in the coordinates that describe our
experience: a change from coordinates centered on our earthbound
experience to coordinates that centered on a rotating Earth—one that
included a further set of coordinates that described the relation of the
Earth to the sun—and later, yet another set of coordinates that included the
relationship of the sun to its galaxy.

In a similar fashion, it is not enough for scientists to depend on
studying simple correlations or even simple “cause-effect” relationships in
our efforts to relate our brain processes to our experience or to our
behavior. In order to understand how our brain processes relate to the
organization of our behavior, as well as to the organization of our
perceptions and feelings, we need to refresh our science, as indicated in
the previous chapter, by establishing the coordinates of explanation at



each scale of inquiry and changing the entire form of explanation from
Aristotle’s efficient to his formal causation.

Our perception of color can furnish us an excellent example of the
transformations that occur as we process a bandwidth of radiation through
our visual system. Processing in our retinal receptors is characterized by
two or three bandwidths of sensitivity: low and middle frequency with a
small contribution from a high frequency band. Within our retina these
bandwidths are organized into three opponent processes: red-green, blue-
yellow, black-white. As I noted earlier, such opponent sensitivities are
arranged in the retina in a center-surround fashion. This is the first
transformation.

Then, further along, deeper into the brain, at a later stage of
processing, these opponent processes are subsequently reorganized into
double opponent processes: that is, each color also “turns off” the part of
its receptive field that had been turned on by its opponent. These changes
are portrayed as the second transformation in coordinates within which
each pair of colors is arranged.

Another example of a change in coordinates occurs when we are
viewing a scene under infrared or ultraviolet illumination. We “see” as
white what we experience as red under ordinary light; we “see” as yellow
what we experience as blue under ordinary light. In general, the colors we
perceive and the relationships among them are changed dramatically by
changing the ambient light in which we are viewing the scene. The way we
conceive of and perceive our “color universe” and the brain processes that
enable those conceptions and perceptions changes radically when we take
these observations into account.



78. A complete diagram of the proposed stage 3. In the horizontal rows, cone-opponent units
with the same center sign are added together (Lo + Mo +So) and (– Lo—Mo—So) to give

achromatic units, since the luminance RFs of the cells add, but their color RFs cancel. In vertical
columns, we add cone-opponent units of different center sign. Hence luminance signals cancel
and color signals add. Note that S-opponent units serve a modulatory role, being added to or

subtracted from each combination of Lo and Mo units. (From Brain and Perception, 1991)

The Limitations of Eliminative Materialism
During periods of great change, holding to efficient causation

provides temptations to exaggerate a particular view or a specific
technology. These temptations seduce both the “old guard” and the “avant
garde.” I want next to call attention to some of these excesses.

Perhaps the most flagrant of the exaggerations is a popular movement
called “eliminative materialism.” In formulating a science that deals with
the relationships among our brain, our behavior, and our experience, my
efforts have been directly opposed to those of the “eliminatists” who claim
that “If we could describe what every neuron is doing, we could supplant
‘folk psychology’ with neuro-science.” In a sense, it is a return to
behaviorism, but we are now asked to identify the efficient causal effects
of the behavior of brain cells with those of the behavior of organisms, a
mistake Skinner never made.

For the eliminatists, “Folk Psychology,” expressed in everyday
language, is to be eliminated because it is not sufficiently precise and is
often misleading. By contrast, I have found much wisdom imbedded in our
everyday use of words. I use the dictionary repeatedly when I am



searching for an explanation of an unexpected laboratory or theoretical
finding. For example, when I found, on the basis of my experiments
concerning the amygdala and the hippo-campus, that episodes of our
experience apparently precede the events that make up the episode, the
dictionary told me that the word “event” comes from the Latin ex-venire,
“out-come.” So, events are the outcomes of an experienced episode, the
outcomes of, the consequences of an act! Events don’t just happen —they
are “eventualities.” Materialists face the problem of creating order among
properties they observe and measure. Items whose properties are well
defined often shed those properties when the items combine and new
properties emerge. The standard example is water: hydrogen and oxygen
are gases at earth temperatures, while water is liquid with peculiar
properties such as wetness and flotation. Atoms are not “wet” even when
they liquefy under pressure: they are apt to damage our skin. Wetness is a
property that we come to explore from our experience with water. Once we
have experienced wetness, we can try to find out what it is about water that
makes it wet.

In studying the brain we need to take into account various levels of
inquiry that reflect different scales of processing: the different functions
of deep and surface structures; the modules and systems composing the
brain; and the relations between brain systems and behavior, and between
behavior and our conscious experience. When brain scientists try to
formulate how, from the biological properties of nerve cells, such as a
brain cell’s electrical response, a person’s or an animal’s viewing of a hand
or a face emerge, formulations quickly reach a limit. However, when
complemented with a top-down approach, scientists can again make
headway, as in the case of the discovery of cells electrically responsive to
an organism’s viewing of a hand or face. But to make such discoveries,
there had to be a way to conceive of faces and hands—in this case, photos
were available. Pictures and the words to describe them are the tools of
communication in Folk Psychology.

My claim is that not only is it easier, but it is imperative, to start
explicitly with our own experience, so richly expressed in the language of
“Folk Psychology.” But as cortical primates, we do not stop there: we
continuously sharpen our experience by making ever more distinctions. In
the case of the brain sciences, starting with observations of objects we call
“faces” and “hands” allows us to determine not only their neural correlate



but also how “higher-order” processes can influence, through learning,
those of a lower order. Furthermore, the success of the Fourier-based
harmonic analysis in describing processes in the primary visual cortex
permits us to use a similar analysis to describe the temporal lobe’s
processing of the fingers of the hand.

Reliance on efficient causation as it is applied in the description of
emergent properties and the attempts at reduction of those properties to
more elementary (material) entities is cumbersome and almost always
ends in failure. A more comprehensive alternative is to relate the patterns,
the forms of different orders, different scales, by relating them by way of
transformation.

The Limitation of Experimentation
Experimenters are tempted to conclude that the brain’s response to

the stimulus they use—usually the one that is initially successful in
eliciting a response—is a “detector” of that stimulus (e.g., a line.) The
saying goes that when one has a hammer, everything looks like a nail.
Today there is the constant danger that this error will be made in
interpreting the results of brain imaging by tomography (PET scans) and
functional magnetic resonance procedures (fMRI). When a brain area
“lights up,” the temptation is to interpret that area as being a center for the
single experience (such as “fear”) that is being tested.

Another example comes from the explorations begun in the feature
and the frequency modes. Communication between the two schools of
brain scientists has been limited, but engineers and mathematicians have
been more creative in combining the contributions of each mode of
investigation. One reason for a lack of communication within a particular
branch of science is that laboratory research places constraints on
experiments. On the positive side, this means that a clean result—an
efficient causal trail—can be obtained when the experiment is well
designed and carried out. But even here there is a caveat: the meaning of
that result is frequently limited by the conditions of the experimental set-
up. In other words, the result is often a foregone conclusion based on how
the experiment was set up.

In order to overcome this limitation, experimental psychology went
through a period of developing overarching grand theories that, however,



often failed because they became too cumbersome when trying to
encompass new data. As an example, in the 1950s, Clark Hull, professor of
psychology at Yale University, tried to build an efficient causal input-
output, behaviorist theory that would encompass the works of Pavlov,
Freud and Thorndike. Hull developed equations in which the terms of the
equations represented the results obtained from experiments and
observations. He copied his approach from engineers, who are often faced
with situations when an input and an output are known, but the
experimenter has to determine what the intervening connecting factors
might be. Hull’s equations provided room for unknowns that would be
found through further experimentation. This approach brought Hull a great
following, and experiments on rats were performed in profusion at a great
number of universities Each finding generated yet another equation until,
at last, the whole structure fell of its own weight. About a month before
Hull died, he said to me in discouragement, “Karl, this is no way to do
science.”

In reaction to this unwieldy process, several experimentalists,
including Hull’s pupil Jack Hilgard, suggested that experimentalists
should develop mini-theories. But such mini-theories would fall into the
very trap that the grand theories had attempted to escape. Each
experimental result is, in itself, the test of a mini-theory: a hypo-thesis. In
the clutter of hypothesis testing that soon engulfed experimental
psychology and neuroscience, the thesis—the attempt to form a deeper
understanding of relationships to other data sources—has tended to
disappear.

There is a middle way between overarching theories and hypothesis
testing: the key is to abandon deductive reasoning as the only
“scientifically acceptable” route to understanding. Both of the extreme
forms of doing science are based on deductive reasoning that, in turn, is
based on efficient causation: grand theories such as Hull’s made their
postulates explicit; hypothesis testing often fails to do so.

In the mid-1960s the journal Psychological Reviews was installing a
new editor, David Zeaman, professor of psychology at the University of
Connecticut. While we were chatting, he said that his aim was to make the
journal truly scientific: only papers based on deductive reasoning would
be accepted. I was appalled. Most of psychology has not as yet gone
through an inductive process that would lead to agreement on the names



that identify processes, much less classifying these processes,
classification that ought to precede deductive ordering. Exceptions have
been psychophysics and, to some extent, developmental psychology.

Charles Sanders Peirce, the noted pragmatist, pointed out that both
inductive and deductive reasoning bring little new to understanding in
science. Induction organizes known observations while deduction places
them into a formal relation with one another. To bring a fresh perspective,
Peirce noted, one must use abductive reasoning, that is, the use of simile,
metaphor and analogy.

The brain sciences have been enriched by the use of metaphor: We
used to think of the brain as a telephone switchboard; this gave rise to the
notion of the brain as an information processor. Then the metaphor
changed to thinking of the brain as a computer; the computer taking the
place of that angel, the switchboard operator. This metaphor gave rise to
thinking of mental activity in terms of computer programs. The hologram
is another such metaphor; this one accounts for maps of dendritic fields—
and the psychological processes that cannot be organized by the neural
circuitry of a telephone-computer-like system.

Metaphors are useful in bringing new perspectives to a problem. In
science we proceed from metaphor to analogy, discerning which aspects of
the metaphor are useful and which are to be discarded as inappropriate. We
then proceed to making models that incorporate the new perspective into
the data that gave rise to the problem. Dennis Gabor invented Fourier
holograms mathematically; Emmet Leith made them palpable, using
lasers. No one would take seriously that brain processes use actual laser
beams—although one newspaper article proclaimed: “Stanford brain
scientist finds laser beams in the head.” Despite such fla-grant
misunderstandings, progress was made during the 1970s and 1980s in
sharpening Fourier-based mathematical models to map dendritic fields,
what I have called deep processing in the brain.

Unfortunately, a sizable group of established scientists is always
reluctant to accept the results of experiments when the data do not fit their
own, usually unexamined, metaphorically based thinking. Today, findings
that are framed within the metaphor of information processing based on
shape are readily accepted: data concerning cortical columns, membrane
channels and the molecules that traverse them. By contrast, data
concerning patterns of transactions among oscillatory phenomena



continue to have difficulty in being accepted, except where they have
become standard fare, as in optics.

Theses and Hypo-theses
A problem inherent in the limitations of current experimental science

is in testing hypotheses. What is being tested is whether an efficient causal
relationship exists between a proposition and its effect in the experimental
situation. The idea is that deductive reasoning, hypothesis testing, can
provide a hard-science explanation for the effects being investigated. This
approach assumes that the observations and their relationships have been
adequately examined and that there is no need for bringing in new
perspectives. Neither of these assumptions currently holds for
observations in the brain/behavior/experience sciences.

Hypothesis testing is useful in a circumscribed system where the tests
sharpen our focus on the assumptions underlying the deductions that make
up the system. The fallacy of much of current hypothesis testing in its
search for efficient causal relationships—and especially of testing “what
isn’t,” called “the null-hypothesis”—is that, in my experience, any
reference to the assumptions in the deductive system is most often lacking.
Reference to assumptions entails examining complexities, formal causal
relationships.

Today, as I read current experimental reports, it is often hard to tell
what results were obtained because they are buried in a welter of
shorthand describing statistical significance. “Significance” suggests that
the data were “signs” of something of importance—but that “something”
is often hard to locate. The current overuse of the practice of formulating
null-hypotheses—which, if disproven, prove that you were right all along
—actually demonstrate that you didn’t have to do the experiment since
you already knew what you have now found out. The reason results of such
experiments are so welcome is that they so often confirm what the
experimenter and the scientific community think they already know.

Several papers have been delivered in recent years at the meetings of
the American Psychological Association sharing my unease with what we
teach, or fail to teach, our students. And in the January 2010 issue of the
American Psychologist an article declared that “a quiet methodological
revolution, a [statistical and mathematical] modeling revolution, has



occurred over the past several decades, almost without discussion. In
contrast, the 20th century ended with contentious argument over the utility
of null hypothesis significance testing.” The current revolution regarding
learning and performance parallels that described as taking place a half
century ago for understanding perception and memory storage, as
described earlier in Chapter 2 of The Form Within.

Pythagorean Pitfalls
Vision is not the only one of our sensory modes that has been

afflicted by established scientific myopia! I was given a book, The
Emperor of Scent authored by Chandler Burr, which chronicles the
adventures of experimental scientist Luca Turin in trying to have his work
published. Turin has a remarkable gift for distinguishing among, and
identifying different odors. He is also a respected scientist who decided to
find out how our sense of smell works. His research demonstrated that
different odors could be identified by the different vibrations of chemicals
that are absorbed by the receptors of our nose.

I had been interested in the possibility of resonance as a determinant
of odors ever since the 1950s when, at Yale, Walter Miles told me that
Michael Faraday had made just such a proposal in the early part of the
19th century. Faraday had been impressed with the capacity of the nasal
chamber to maintain a stable environment of heat. “Heat” is radiation: it is
a vibration whose wavelength is lower than that of infrared in the
spectrum of “light.” Faraday suggested that the olfactory mucosa absorbs
molecules of olfactants (smells) that display different resonances.

Although the vibratory theory of olfaction has been mentioned, and to
some extent tested occasionally since Faraday’s day, the current
establishment view is that the shape of the odorant molecule holds the key
to its smell. Shape determines which molecules are absorbed by which
nasal receptor. But shape and the patterns of vibration can be compatible:
shape determines the location of the absorption; vibration is involved in
the process by which the olfactants, once absorbed, stimulate the receptor
membrane channels. The establishment, however, viewed Luca Turin’s
research as heresy. The Emperor of Scent details the results of his research
and also his efforts to have the journal Nature publish the reports of his
experiments.



Just recently (2006) the journal Science has reviewed this whole story
on the occasion of the publication of Luca Turin’s own book, The Secret of
Scent: Adventures in Perfume and the Science of Smell. Turin describes his
vibration theory in the context of chemical shape theories and notes the
limitations of each. He concludes that there is no reason to exclude either
theory, that they address different levels of processing—much as I had
noted (before the publication of Turin’s book, in the paragraph above). The
reviewer for Science comes to the same conclusion.

I felt completely in tune with Chandler Burr’s account covering the
year and a half of carefully phrased rebuttals by Luca Turin to the wishy-
washy replies of the journal’s editor and to rejections by “peer review
readers” for Nature—interestingly called “referees”—who seemed to have
failed even to understand what his experiments were about. As I read, I
was rooting for Turin and swearing at the referees. As do all laboratory
scientists, I have a number of my own startling tales to tell.

The issue is not whether the findings are “correct” but whether they
are the result of a carefully carried out scientific procedure that can be
duplicated by others. In no case should hard data, the result of significant
research, be kept from publication due to prejudices held by an established
coterie. The advances of science depend on nourishing the enterprise of
scientists, especially the young, whose paths to publication of the results
of solid research should be nourished, not hindered.

As Chandler Burr puts it, in The Emperor of Scent:

It is therefore disorienting, almost surreal, to enter a story
through one side and then gradually—I thought I must be
imagining things—find that that side is so loathed by its
opponents, so vilified by and toxic to them, and so axiomatically
unworthy in their estimation of serious consideration that they
actually refuse to share their own view with you.

I began this book as a simple story of the creation of a
scientific theory. But I continued it with growing awareness that
it was, in fact, a larger, more complex story of scientific
corruption—virulent and sadly human [in the] sense of jealousy
and calcified minds and vested interests. That it was a scientific
morality tale.



But this is not the whole story. I have talked to my friends who have
performed beautifully carried out experiments on the olfactory system and
shown how patterns in the brain that are initiated by olfactants are
sensitive to combinations with taste stimuli to form flavors and, in turn,
how these “flavor” patterns are changed by more general perceptual and
cognitive influences. What upset these investigators was the hyperbola of
admiration of Turin’s work portrayed by Chandler Burr: stating that he
should receive the Nobel Prize as ”The Emperor of Scent.”

Such excessive admiration comes when a particular discovery
resonates with the personal experience of the admirer. Unfortunately, the
admiration plays poorly with more established ways of doing science and
therefore does more harm than good to the person admired. Achieving
fame is a two-edged sword. Serious scientists, including Turin, with whom
I’ve discussed this, prefer accountability and respectability.

Fields and Things
The vibratory view of sensory perception has historically been harder

to grasp because it relies on mathematical language that is unfamiliar to
non-scientists and even to many scientists. Also, the vibratory theory is
about relationships, oscillations, and patterns which can be
mathematically described and even mapped. But they are not shapes and
“things,” like points and lines, and thus not as tangible and familiar to us.
In the latter part of the 19th and early part of the 20th centuries, great
strides were made in our understanding of “things” in terms of the
probability of their occurrence. This new approach overshadowed then-
current theories such as those of Faraday and Clerk-Maxwell, based on
relations. Interest in developing field theories gave way to thinking in
terms of particles and lines (scalars and vectors).

The most famous example of describing the results of experiments in
terms of particles rather than fields was when the German physicist Max
Planck divided electricity and light into “electrons” and “photons.”
Tremendous strides in understanding were achieved by such
conceptualizations and then by naming them; nonetheless, the particle
approach obscured the oscillatory properties of radiation— which had to
be re-introduced in the 1920s by the French theoretical physicist Louis de
Broglie in his doctoral dissertation, and by the Viennese atomic scientist



Schrödinger. As I’ve noted on several occasions, while we may be helped
in our thinking by “thingifying and naming”—studied as “reification” in
psychology—we are at the same time constrained by doing so. One of my
most perceptive students noted, if “it” doesn’t have a name, “it” doesn’t
exist. The converse of this is that if it has a name, it and only it exists.
Quantum field theory has managed to overcome the constraints of
reification, but many interpreters of the theory slip back into
communicating in terms of things, which explains the often-voiced
“weirdness” of the theory.

Just recently, I attended a lecture by Yakir Aharonov at a university in
Jerusalem, a distinguished contributor to the current advances in quantum
physics. In his talk, he used the term “electrons”—things—but when he
made his formulations on the blackboard he drew wavelets when
symbolizing them!

Once, while I was in Chicago visiting the Museum of Science and
Industry, I came up with an interesting insight regarding statistics. I was
attracted by a loud racket that proved to be caused by a display of falling
steel balls ricocheting through a symmetrical plastic maze. The display
had an explanatory card as well as a voice recording that described the
importance of the “normal distribution”—the bell shaped statistical curve
that had been formed by the balls as they landed. Nowhere in the
description, visual or auditory, was there a mention of the symmetrical
plastic maze through which the balls had tumbled. Without that
symmetrical maze, the balls would have splattered on the floor in a flat
plane. Normal distributions reflect a symmetrical structure.

Recently, several of my students at Georgetown University who were
taking both physics and psychology courses were delighted when they
learned about the relationship between such a normal distribution and
symmetries.

Complexity Theory and Formative Causation
What this book is about is the search for “form”—that is formal

rather than efficient causation. Swedish physicist Niels Bohr gave a
powerful example of the necessity to break out of the mold of efficient
causality when dealing with newly discovered complexities. In a 1929
lecture before the Scandinavian Meeting of Natural Scientists, Bohr



stated: “the domain of . . . quantum mechanics occupies . . . an
intermediate position between the domain of applicability of the ideal of
[an efficient] causal space-time description and the domain . . .
characterized by teleological arguments.” Bohr correctly saw what was
needed in the new enterprise and proposed the concept “complementarity”
to deal with it. Transformations among complementary forms were
focused by him on the type of measuring apparatus used (such as a single
or double slit) in the experiments.

79. An Example of Formative Causation

J. A. Scott Kelso, in a book co-authored with David A. Engstrom The
Complementary Nature (2006) has comprehensively reviewed
“complementarity” as it forms and transforms the non-linear dynamics
(chaos theory) of complexity. However Kelso uses the terms
“complementarity” and “coordination” differently from the way I use
them. For Kelso and Engstrom complementarity is, among other things,
something like a symmetry set, a mirror image. Co-ordination is the way
several sets of variables become integrated—but not specifically by
relating them by way of separate coordinates, as I am proposing here.

We have available today a formal procedure by which we can study
the domain intermediate between efficient causality and teleological
arguments. As noted in Kelso’s work, we can now describe, with the
techniques of non-linear dynamical systems, the relationship between
initiating conditions, complex self-organizing processes, and the outcomes
of those processes. The procedure entails repeating an observation a



sufficient number of times until a reliable pattern emerges—emerges out
of the “noise,” that is, out of non-repeatable observations. The resulting
pattern is called “a basin of attractors.” The attractors, as their name
applies, indicate the destination toward which the process is headed,
Aristotle’s “final causation.” But more than final causes can be plotted:
slices, sections, can be made across the basin of attractors to ascertain the
form of the paths taken during the process. This formal process I have
referred to as “formative” because it is proactive. Formative causation is
called by a variety of names: complexity theory, non-linear dynamics,
dynamical systems theory, chaos theory. In Bohr’s domain of quantum
theory, Yakir Aharonov recently proposed a technique of “weak
measurement” that develops a set of “history” vectors and another set,
“destiny” vectors, that describe the complexity of quantum transactions.

The simplest example of the construction of a basin of attractors is
tracing the path of a pendulum. If the pendulum is not energized, it comes
to rest at a point. Its path is simple to chart, and the rest point is a “point
attractor.” If the pendulum or a swing at a country fair is swung with
sufficient or repetitive force, a second hesitation, a temporary stability in
the path, occurs at the zenith of the swing, far from its nadir at
equilibrium. Thus two point attractors are described. When the to and fro
of the device becomes more permanently energized into an oscillation, a
“periodic attractor” is described.

Much more complex paths are described if the shaft of the pendulum
has a joint in it or a device (or a system such as the solar system) has three
rather than two oscillating elements. But beginnings have been made in
describing the paths taken to such multiple or “chaotic attractors.” These
techniques are truly in the spirit of formative causation and have been
taken up when relevant throughout the chapters of this book.

My colleague at Georgetown University and an Oxford don, Professor
Rom Harré, recently has made an insightful proposal that articulates the
manner in which formative causation applies to the concepts of emergence
and reduction. Harré suggests that structures can be considered to be
complementary forms that need not be viewed as entities composed of
components. In this view, scientists, philosophers, and I are spared trying
to overcome the limitations of viewing relationships as consisting of an
emergent that needs to be reduced to its components. Rather, the
relationship between complementary forms becomes a search for



transformations, as in relating one level of computer programming to
another. Mathematically, the search consists of specifying the transfer
functions that relate one form to another.

Different tools of observation provide complementary forms. In
1929, physicist Niels Bohr wrote that “in the domain of atomic theory . . .
the usual idealization of a sharp distinction between phenomena and their
observation breaks down.” Bohr went on to describe differences in
observations made using different instruments as yielding
“complementary” results: Is the form of subatomic phenomena to be
conceived in terms of waves?—or particles? or both? I have appended a
paper (See Appenndix A: “Minding Quanta and the Cosmos”) describing
how our biological apparatus constructs our experiences (phenomena) and
complementary theories about those experiences.

Formal, formative causation and non-linear dynamics are the tools
Bohr sought. Those tools have become available to us in exploring the
complex phenomena that are being discovered in exploring the brain
processes that guide us in navigating our universe.

Viewing relationships among complementary forms in terms of
transformations provides a fresh perspective on emergence and reduction.
The ordinary approach to emergence focuses on the emergence of
properties, usually material properties, whereas approaching emergence
from the standpoint of transformations entails the specification of transfer
functions, of translational forms. Transformations of acorns to oak trees,
embryos to children and then to adults are prime examples of the
development of complexity, the development of complementary forms
through transformations.

Transformation also gives a different flavor to reduction. By
specifying the transfer functions that relate one form to another, a
common vocabulary relates one form to another. The periodic table of
elements, the most successful attempt at reduction, is a prime example of
the reduction among forms: chemical forms to atomic forms by way of the
transfer function “atomic number.”

The coordinates of the patterns of our conscious experience are
formed by transformations that can be specified by their transfer functions
from (complementary forms of) the patterns of our brain processes.
Equally important, the patterns of our brain processes are formed by
transformations that can be specified by their transfer functions from



(complementary forms of) the patterns of our conscious experience. The
remaining chapters of this book continue to specify these coordinate
transformations.

In any of these ways of dealing with the mind/brain relationship we
are using the structure of language to talk about what it is that we are
talking about. In one case, brain is the subject and mind (psychological
processes) is the object. In the second way of approaching the relationship,
our experience is the subject exploring brain as the object. As in the case
of seeing red, does the language reflect our mode of analysis or is the
mode of analysis a reflection of language? Or both?

Taking the route of “both” lands us in a different sort of dilemma. Do
brain and psychological processes coexist without interacting, or do they
interact? (Coexistence is called “parallelism”—the position advocated by
Ernst Mach, a position rebelled against by the philosophers of the Vienna
circle and by Karl Popper). Karl Popper and John Eccles, though they
differed considerably in their approach to the problem, espoused the
interaction view and wrote a book together. Popper traced the interaction
between psychological processes and the brain through culture; Eccles had
mind operating directly on brain, which poses the problem of how this can
be accomplished.

Popper’s way affords a simple answer: We do something or
communicate something. This influences others in our culture who then
implement the idea, and we experience the implementation through our
senses, which in turn influence our brain. I have added that we can
influence our brains through our bodies and through sensing our behavior,
not only through culture.

I also hold that much of conscious experience does not immediately
become embodied. I look at the flowering spring landscape—perhaps this
produces an endocrine surge (of adrenal cortical hormone)—but perhaps
not. Much of what I experience consciously, what I sense or think about, is
just that—an epiphenomenon, as philosophers call it. Only when the
experience has consequences—and is thus embodied (embedded) in
memory—does it fulfill the requirements of downward causation.

But this brings me to Lévi-Strauss’ injunction: when we are dealing
with complexity, the efficient causal approach fails—we need to look at
the formal structure of the problem. It really is not much help if we phrase
the mind/ brain issue as ”an egg is a chicken’s way of reproducing” or “ a



chicken is an egg’s way of reproducing.” Neither tells us much about the
composition of chickens or eggs.

Free Will
I want now to illustrate the power of taking formal causation

seriously by initiating a discussion of one of the most debated issues
within the brain and behavioral sciences: the question of whether or not we
are capable of acting deliberately. The debate centers on whether we can
voluntarily “cause” our actions (in the sense of efficient causation).

With respect to free will, dynamical systems descriptions of attractor
basins—descriptions of sections through the variety of “paths” taken by
brain processes involved in forming the basins—are much more
appropriate to charting the degrees of freedom of action within which we
operate than any attempt to use, in the sense of “efficient causation,” the
time course of a single brain event as “causing” or “not causing” a willed
muscle contraction or movement.

In order to behave deliberately, by definition, we must be aware—that
is conscious—of what we are about to do. However, brain scientists have
recorded changes in the human brain cortex that are correlated with action,
but occur a half second or more before a person becomes consciously
aware that he or she is about to act. (I have seen such anticipatory
electrical events occurring even earlier in the brain stem.) These
“readiness potentials,” as they are called, have been interpreted to mean
that volition at best can stop a neural “program” that is already set to go.
The conclusion reached by many scientists and philosophers is that we, as
conscious subjective beings, are pretty much at the mercy of whatever has
set our behavior in motion and that free will is an illusion: the initiating
event, whether a tennis ball approaching my tennis racquet or someone
asking me to raise my arm, initiates a process beyond my control, a
process predetermined. In analogy to David Hume’s example, it is as if the
cock’s crowing were causing the sun’s rising.

We already know from experiments performed in the 19th century
that what we are aware of is an entire situation within which a behavior
occurs, not the particular muscle groups involved in the activity. Hermann
von Helmholtz in Berlin had analyzed the difference between automatic
and voluntary behavior and had shown that voluntary behavior involves a



simultaneous and parallel activation of two brain systems. Helmholtz
demonstrated this by paralyzing the eye muscles of a test subject and
asking the person to move his eyes. As a result, the scene flashed across
the person’s visual field as if a screen—a “frame” upon which the visual
world had been projected—had been moved. Recently, experiments have
shown that this frame is “constructed” by brain cells near those that
control eye movements in the brain stem. Thus, Helmholtz demonstrated
that seeing a scene by voluntarily moving our eyes depends on two outputs
from the brain: one moves the muscles of the eye, the other compensates
for the movement.

In order to voluntarily move an arm, we must already be aware of the
“frames” or contexts within which to move it. There is a visual frame, a
body frame, and the frame of the muscles of the arm itself. The
experiments that describe the readiness potential address only the
movement, the sequence of muscular contractions—not the visual or total
body frame within which the movement takes place. As in so many of the
more encompassing brain/behavior relations, several brain (and body)
systems are enlisted in willing an action to occur. And it takes around a
half- second (called the “reaction time”) to mobilize these several systems
and our awareness that they have been activated.

But the willed action does not consist of “telling” each muscle what
to do. Rather, the motto “Keep your eye on the ball” is more appropriate.
In tennis or baseball, we are not consciously aware of the arm that holds
the racquet or the bat. We are aware instead of the context of the game,
focusing on the ball, but also on the position of other players, the
boundaries of the playing field, and so on. The context provides
constraints within which we voluntarily play the game.

My willed behavior is subject to a variety of constraints such as
gravity, the clothes I wear, the values that I hold dear, and the social
situation within which my behavior is intended. Actions that are available
to us within the context of various constraints are described scientifically
as operating within certain degrees of freedom. Subject to these
constraints or limitations, however, I can act, seek to experience and
express myself freely. We “cause” our actions, but not in the immediate
“efficient causal” sense. We often act with an eye to the future, a final
cause. And, within the larger context provided by the degrees of freedom



that characterize the situation, our wills operate by way of formal
causation.

Efficient causality is practical and effective in explaining how we
solve problems in limited and simple situations. It fails to resolve more
complex issues such as “free will” because the richness of relationships
we study in brain/behavior/experience science demand enlisting the more
comprehensive formal causation in order to foster understanding.

What Do Brain Processes Look Like?
While lecturing to students at Harvard one summer, with the

psychology faculty sitting in the back row of the classroom, I was
introduced to the folly of what happens when we expect brain processes to
look like our experience. My lectures are always informal in order to
encourage active audience participation. In this case, the faculty needed
little encouragement and pitched in whenever they felt it to be appropriate.
George Miller and I had already forged the friendship that shortly was to
produce our book Plans and the Structure of Behavior. Because Miller is
very tall, he occasionally leapt up in class to clarify a point at issue, using
the top half of the blackboard that I hadn’t been able to reach. Students,
faculty and I have all looked back at that summer as both illuminating and
fun.

B. F. “Fred” Skinner and I tackled many questions we held in
common, such as what might be the brain structures that organize the
process of reinforcement during conditioning. One day during that
summer’s Harvard class, he surprised me by asking, “Karl, what is your
position on isomorphism?” (As noted earlier, isomorphism—iso, “same”;
morph, “shape”—refers to the proposal that brain processes are shaped
geometrically the same as the shapes we perceive.) I replied that there
must be something going on in the brain that makes it possible for us to
get around, something that allows us to navigate our surroundings. Skinner
then said, “Imagine what is going on in your brain when you are watching
green grass growing in your lawn. Pause. Now imagine mowing that
lawn.” Of course I immediately saw a lawn mower sweeping across my
brain cortex that had sprouted green grass.

No. The brain doesn’t work that way. Still, my reply to Skinner’s
question had to be, in some respect, correct: processes in our brain must,



in some significant fashion, reflect our surroundings.
A half-century after that Harvard class in which Skinner posed his

question, I can now answer it in detail: there are trans-formations of the
world-out-there into the processes of perception and memory going on in
our brain, and there are also trans-formations that change our brain
processes into our ability to undertake effective action. The relationship is
algebraic, Pythagorean, not geometric.

Coordination as the Basis for Transformation
Most often, brain scientists and psychologists are satisfied when they

can establish correlations between mental and brain processes. This is an
important first step but does not describe the processes by which those
correlations are achieved. My interest in the mind/brain relationship goes
beyond correlation: How Do brain and mental processes become
coordinated?

My answer, since the 1960s, has been that our brain processes
become coordinate with our behavior and our experience by trans-
formations, that is, by changes in form. Changes in form are accomplished
by modifying coordinate structures, such as those composing our memory,
that form the relationship of our experience to our brain. Examples of
such changes in coordinates are a change from two to three dimensions in
a visual scene, or a change from an Earth-centered to a sun-centered
cosmic system. Coordination describes relationships among the
coordinates themselves. (Hertz referred to changes in such relations
among coordinates as “holonomic transformations,” a term that I have
adopted.) The coordinates that form brain systems are certainly different
from those that form behavior and experience, as those who are trying to
create prostheses are finding out.

Changes in form, transformations, describe living a life. The acorn
changes its form as it grows into an oak tree. The same constituents,
carbon, nitrogen, oxygen, water and more complex molecules, take on, not
only different arrangements, different shapes, but also different patterns
of relationships with their environment. Initially, prompted by their ability
to include the radiation from the sun and nutrients from the soil, these
forms produce changes in form, transformations, in their environment.
Trees and other plants transform the climate. In Ankara, Turkey, the



university planted acres and acres of trees. After a few decades, heavy
rains and snowfalls were recorded for the first time in over a century.
Beavers make dams and change streams into ponds. Humans build dams
and roads—and houses that hold paintings and libraries that hold books
and concert halls in which glorious music is performed. The changes in
form, the transformations that characterize human culture, are even more
differentiated—make more distinctions—than those of plants and non-
human animals.

In order to accomplish these transformational processes, the
processes must, at the same time, be stable and dynamically adaptive—
continuously changing. The form of the process provides stability while
the substance can change with regard to biological memory. The
unchanging form can also be enshrined in a library of books, in the
architecture of a cathedral, or in the stories we tell our children. In short,
much of the stability of remembrances-of-things-future is embedded both
in our brain and in human culture.

Transformational Realism
Are our experiences constructed by way of transformations? Or are

they ”real”? Philosophers have argued the pros and cons of this question.
What I have reviewed in this and previous chapters indicates that forms
are constructed by transformations that coordinate a distributed brain
process that “looks like” nothing that we experience. Are our experiences
therefore not “real”? Of course they are. When I bump my shin on a bed
rail as I‘m about to climb in, I experience the resulting pain as real—my
endorphins are low and it hurts like hell. But our reality is constructed.
When we are babies, we encounter the world we navigate with the variety
of our senses. Each sense provides a wholly different experience.
Somewhere, early on, we fuse those disparate experiences, different
perceptions into a single coherent reality. The process continues as we
navigate our social world and is called “consensual (putting our sensations
together) validation.” I like to tease my fellow philosophers by noting that
those who don’t make that consensual transition become solipsists:
philosophers who question reality.

Despite the explanatory power for ordinary communication that
follows from an emphasis on form and transformation, there are also



stable forms of memory embedded in the brain that do not fit the
transformational explanation. These forms of memory can lead to
extraordinary feats of remembering. We ordinarily remember the alphabet,
simple multiplications and names (most of the time); however, there are
phenomenal feats of remembering that defy us to formulate questions as to
how they are constituted.

I have heard a six-year-old girl listen to a piano sonata once and then
play it without a score, beautifully, with feeling. Luria has documented the
performance of a mnemonist who remembered everything to the point
where he was unable to make ordinary life choices.

This extraordinary sort of memory is called “eidetic.” It is most
common in children and disappears with the surge of sex hormones at
puberty. One of my colleagues studied a young lady whose eidetic memory
allowed her to see a figure that ordinarily depended on binocular vision
(using both eyes at once) when several days elapsed between exposure to
one eye and the other. She had an excellent “ordinary” memory and was
very clear as to which type of memory she was using. As so often happens,
my colleague and his subject fell in love, married and had children. With
each pregnancy (with its endocrine upheavals) the eidetic memory faded
more. My colleague had to turn to other aspects of psychophysics.

I have not been able to formulate even a reasonable question that
would address how the brain makes eidetic memory possible. This
contrasts with so many questions that have been posed and answered over
the past half century. The journey is not completed.

In Summary
In this chapter I have indicated how the use of formative causation as

applied to coordination and to transformation can resolve some (but not
all) puzzling results of experiments obtained in the
brain/behavioral/experiential sciences. A prime example has been the
difficulty in obtaining acceptance by established journals and granting
agencies of work that has shown how vibratory phenomena can lead to
perception: the dominance of theories of shape (chemistry, lines as
features) has blinded reviewers to the possibility that both shape and
pattern can be related to one another via transformation (transfer
functions). Another prime example was how a resort to correlations gave a



very limited view to our experience of free will. These examples were
supplemented by a journey through the limits of some current practices in
the behavioral and brain sciences. The chapter ends with some more
history and insights into the importance of transformation in the
establishment of memory—in anticipation of the topics of language and
consciousness taken up in the next chapters.



Chapter 21
Minding the Brain

Wherein I propose that Aristotle’s formal, formative causation is more
appropriate to understanding complex brain/behavior/experience
relationships such as “free will” than the more routinely applied efficient
causality.

The pendulum has been swinging; its ticking will be heard
through the entire course of history, as the blob alternates
between the extreme positions of ‘all is body’ to ‘all is mind’, as
the emphasis shifts from ‘substance’ to ‘form’—from atoms to
patterns,—and back again.

—Arthur Koestler, The Sleepwalkers, 1959

The Pythagoreans’ chief accent was on form, proportion, and
pattern; on the eidos and schema, on the relation, not on the
relata. . . . [Their] vision of the world was so enduring, that it
still permeates our thinking, even our vocabulary. The very term
philosophy is Pythagorean in origin. The essence and power of
that vision lies in its all-embracing unifying character; it unites
religion and science, mathematics and music, medicine and
cosmology, body, mind and spirit in an inspired and luminous
synthesis. It was the first step toward the mathematization of
human experience—and therefore the beginning of science.

—Arthur Koestler, The Sleepwalkers, 1959

. . . {N}umber now appears in physics as the only entity with
sufficient permanence to be taken seriously by the physicist. It is
but a step from this to the assertion of the Pythagoreans that



numbers are the very essence of things.—An essential
Pythagoreanism is called for . . . (provided, of course, any
mystical or magical implication of the term is duly discarded.)

—Bernard d’Espagnat, In Search of Reality



Have I not erred in applying to historical thought, which is the
study of living creatures, a scientific method of thought which
has been devised for thinking about inanimate Nature? And have
I not also erred further in treating the outcomes of encounters
between persons as cases of the operation of cause and effect?
The effect of a cause is inevitable, invariable and predictable.
But the initiative that is taken by one or other of the live parties
to an encounter is not a cause; it is a challenge.

—Arnold Toynbee, A Study of History, 1972

Coordinating Mind Talk and Brain Talk
For millennia, humans have been intrigued by the relationship

between what we experience and the three-pound organ inside our skull.
For thousands of years, injuries to the brain have provided us hard
evidence that there is such a relationship and that it is multifaceted.
Physicians since the time of Hippocrates have amassed such evidence,
demonstrating that many different types of psychological processes are
disturbed when the brain is injured.

The observations of Gall at the beginning of the 19th century
triggered an innovation in studying the brain/ mind relationship: Gall
attempted to correlate different sites of brain injury to differences in
psychological profiles of the injured subjects. His correlations were crude:
our high foreheads proclaimed the frontal lobes of the brain to be involved
in our ability to speak. A patient with a cerebellar tumor had a history of
having his sexual needs routinely satisfied by at least two women: ergo,
it’s sex and the cerebellum. These attempts at “localizing” faculties of
mind within regions of the brain shortly led to phrenology: reading the
bumps on the skull to indicate the size of the underlying brain region as
indicating the development of a particular faculty. High forehead =
language-based intelligence; a marked “inion,” a bony ridge at the base of
the back of the skull = sexiness. As I have noted earlier, Broca took off
from these speculations to localize a language center in what is now called
“Broca’s area.” William Uttal of the University of Hawaii has called the
current readings of brain scans, such as fMRI, “the new phrenology.”

Today, the fact that correlations can be made between various
conscious psychological processes and brain function by way of brain-



imaging procedures (PET and fMRI) is well established. These
correlations have been viewed in a variety of ways. The most accepted
way, adopted by most brain and behavioral scientists, is to affirm that
psychological processes emerge from combinations of elementary brain
cell activities, much as the liquidity of water, at ordinary temperatures,
emerges from a combination of two gases, hydrogen and oxygen. Given
this “emergence” view, we are led to explore various properties of brain
cells and their combinations to see which combinations lead to which
psychological “faculties,” such as the ability to talk or make music. In this
search we need to know the way in which the elementary properties of our
brain cells bind together to form a psychological property in the same way
that valences characterize the binding properties of oxygen and hydrogen
to make water wet. The problem of specifying “valences” for
combinations among brain cells to form behavioral traits is, however, a
difficult one.

In science we recognize two types of emergence: one type claims that
emergence occurs when two or more properties of disparate substances
combine in specific ways to form the properties of a new substance.
Occasionally this alternative has been become realized, at least in part, as
in the periodic table of elements. However, scientists, more often than not,
insist that such reduction has been attained when it is not— and that if it is
not attained, then it certainly will be when more effort is expended. This
attitude faces many problems.

The second type of emergence asks only that the language of several
fields of inquiry be made compatible. This is a reasonable aim that is
attainable in the brain and behavioral sciences. It provides an alternative
to raw emergence that I’ll take up shortly.

Psychological properties, especially those of conscious awareness,
emerging from brain processes face a problem (called “supervenience”):
How do the emergent properties influence the constituents from which
they emerge? How do thinking and feeling influence the way the brain
works? A tentative answer of sorts can be given by an analogy from other
emergent instances: The liquidity of water at ordinary earth temperatures,
and the fact that when it freezes into ice it floats, has redistributed oxygen
and hydrogen on the earth’s surface. The liquid water, by pooling in ocean
basins, has taken oxygen and hydrogen out of the atmosphere and thus
redistributed their location.



For psychological processes, the redistribution of components by the
operation of their emergent is encapsulated in the slogan “The pen is
mightier than the sword.” Thoughts and ideas are powerful in changing the
distribution of useful “brains” in the world, whether by outsourcing the
development of computer programming or by velvet (bloodless)
revolutions in Central Europe, such as those that occurred with the
dissolution of the Soviet Empire.

One view that mind emerges from operations of the brain assumes
that mind can be reduced to the way brain cells work. As I noted earlier,
Francis Crick took an extreme stance of this view by suggesting that if we
knew what every cell in the brain is doing, we could do away with mind as
psychology altogether. This view would leave us with a totally material
world-view in which we need not take responsibility for our actions. After
all, when our actions are totally determined by the doings of the cells in
our brains, those cells cannot be held accountable.

An, alternate, and more useful, starting point for inquiry can be the
fact that we are asking the question “How does the brain work to enable
conscious experience?” We start with our own conscious experience and
the “phenomena” that compose it—in this case augmented by the
cumulative experience of families and friends and teachers. We learn that
there is a relationship between psychological processes and functioning—
especially malfunctioning, injured—brains. We then explore the
relationship and find out as much as we can about brain processes and
their influence on conscious experience. In this view, the mind/brain
relationship is often characterized as a gap: When we begin with our
conscious experiences and try to relate different brain process to those
experiences, we find that there appears to be a gap (technically called the
Cartesian cut, which I will explore in Chapter 24) between our ineffable
experiences and the processes going on in the material brain.

A rough metaphor helps me to understand what I am writing about.
The wetware of my brain is operating while I write this paragraph.
Coincidentally, so is the hardware of my computer. If I examine the
various levels of programming, I find that binary, hexadecimal, low- and
high-level operating systems, Java- and natural language-codes are all
operating. I can obtain, for a moderate price, the transformations that lead
from the binary operations of the computer hardware to my natural
language, English. The transformations lead from the operations of the



deep and surface processes of the brain to my natural language, English,
the subject of the investigations described in this book. Note that what
goes on in the computer at any moment does not change as a function of
the level of coding which I am examining. The same is true of how my
brain works: I can address my brain/mind research at various levels from
synaptic membrane channels, the operations of the dendritic web, neural
circuitry, brain systems analysis to behavioral, linguistic, conscious and
social levels of inquiry. The workings of the brain remain the same
through all these levels of analysis. What is gained is that with each
transformation we achieve a substantial increase in understanding and in
efficiency and effectiveness in navigating our world.

Currently, many brain/behavioral research endeavors do not try to
bridge the gap between brain and mind by understanding the how of the
relationship; rather they try just to fill the gap with correlations. Much of
what I see happening currently is this gap filling, even though the
scientists involved vow that they are searching for how psychological
properties can be reduced to, or emerge from brain.

The ordinary ways to conceive of emergence and reduction as well as
gap filling all have their limitations and are unsatisfactory because, for the
most part, they lack any way to answer the “how” of the mind/brain
relationship, the “particular go of it.” Recently, as described in the
previous chapter, an alternative to emergence and reduction has been
developed.

The Challenge
The issue raised by Toynbee’s quotation that introduces this chapter

describes a limitation in the very nature of today’s conventional science as
it is applied to the experience-behavior-brain interface. There is no
question that “the hard or thick” sciences, as they are practiced, have
proven to be efficient and effective. The effectiveness is in part due to the
fact that, for the most part, scientists try to find direct causal relations
among the observations they make. This works to some considerable
extent: the bacterial and viral causes of disease and the causal
effectiveness of immunization and medication are examples of what has
come down to us through David Hume and British empiricism as
Aristotle’s “efficient causation.”



However, when such relationships become more complex—as they do
in the brain and behavioral sciences, as well as in thermodynamics and the
communication sciences, or in quantum and cosmological physics—the
explanations currently in vogue, based solely upon efficient causation, are
stunted. In such circumstances Aristotle’s other modes of causation, final
and formal, become more relevant to understanding how processes relate
to one another.

Final causation deals with teleology (telos = “ends”) the purpose
toward which a process aims; computer programming provides an instance
of final causation. Formal causation encompasses both efficient and final
causation; formal causation is invoked when an entire structure, a context,
has to be taken into account in order to understand the relationships
involved. Here I shall slightly modify Aristotle’s terminology and use the
term “formative causation” in keeping with the future-oriented theme of
these chapters. My aim in this book is to show how substituting analyses
based on formal, formative, rather than efficient causation can resolve
many current perplexities in studying experience-behavior-brain
relationships. Some practical examples will illustrate this approach.

When I examine the relationships among experience, behavior, and
brain processes I am faced with a multiplicity of possible “causes”. When
the relationship between a brain process and what we experience and how
we behave is closely knit, I note that the brain process “enables” that
particular experience or that particular behavior. (In philosophical jargon,
the relationship is termed essential but not sufficient.) Thus certain brain
processes, as those of the cortical motor systems, enable skilled
performances such as speaking or playing the piano. But just as important
is the culture we grow up in and the invention of the piano, that enable the
particular language in which we communicate and enable the performance
of the concerto.

We often conflate what we do with what our brains do. Some
scientists want to reduce our experience and our behavior solely to the
workings of our brain. It is tempting to attribute someone’s thoughts to
“the way his brain is wired” or his outlandish behavior to “having a
mixed-up brain.” There is a basis for such reasoning: whatever we
experience and how we behave at the moment has to be processed by our
brain. But this answer excludes the broader reaches of causality: the



immediate environmental triggers; the more extended influences on our
brains by way of learning.

Today, the use of brain-imaging techniques has brought causation to
the forefront of our attention. We find that a part of the brain “lights up”
when the subject performs a given task. Many scientists and philosophers
of science have concluded that the part of the brain so identified has
genetically made possible the behavior of the subject. But we’ve also
discovered that the patches that light up in the brain during imaging
change in size and location as a function of learning. Therefore, neither the
genetic nor the learning explanation by itself can be an adequate
explanation of why this part of the brain lights up. Genes determine
possibilities; experienced cultural input determines the form, the formal
structure of relationships that these possibilities take.

For instance, I speak English; my colleague speaks Parsi. Both of us
were endowed at birth with the capability for language, the capacity to
speak. But which language we speak depends on the culture within which
each of us was raised—as well as the culture within which each of us is
communicating at the moment.

I was introduced to the issue of causation during the late 1930s, at the
University of Chicago, by my undergraduate professor in physiology,
Ralph Gerard. Gerard used to throw a piece of chalk at us students if he
found that our reasoning was teleological—that is, if he saw that we were
attempting to explain the occurrence of an event in terms of what it was
“good for.” Such teleological reasoning attributes an occurrence to what it
achieves, not what has produced it. Gerard was correct within the context
he was working in, but new ways of doing science have provided a wider
context which embraces what Aristotle called “final causation.” Indeed,
today’s computer programs are constructed teleologically—to perform
what we aim to achieve. In a similar fashion, thermostats are “goal
directed”—their purpose is to keep us comfortable.

Much of current science, and especially the brain and behavioral
sciences, is wedded to discerning simple cause-effect relations. Thus,
experiments are performed within the constraints of testing hypotheses:
events are shown to depend on others that preceded them. As early as three
centuries ago, David Hume had already pointed out that this type of
causality can lead to false assumptions: his example was that just because
a cock crows before the sun rises does not mean that the cock’s crowing



“causes” the sun to rise. The correlation between the cock’s crowing and
the sun’s rising has to be understood within a larger context, a structure or
“thesis” such as the Copernican understanding of the rotation of the earth.
The correlation (crowing = rising) can be examined as a “hypo-thesis,” but
this examination does not, of itself, test the direction of “efficient
causation” in the relationship. Questions such as whether a particular
musical performance is “caused” by a brain process, an inherited talent, by
early exposure to classical European music, or by the availability of a
musical instrument, appear rather silly when stated this way. Rather, the
question that must be posed is: What are the step-by-step transformations
that allow learning and instrumentation to modify innate brain processes
to make possible the performance and appreciation of a concerto? The
question is ubiquitous in biology: How does an acorn grow into an oak
tree? How does the blastula made up of apparently uniform,
undifferentiated cells mature into the person you fall in love with?

My aim in The Form Within has been to broaden the scope of
explanation in the brain and behavioral sciences by selecting, in addition
to efficient and final causation, still another arrow in Aristotle’s quiver of
causalities: formal causation. As noted in Chapter 20, in the early part of
the 20th century, Claude Lévi-Strauss, the famous French anthropologist,
pointed out that if a system is simple enough, we can analyze it into its
components and try to determine which component is causally responsible
for an effect. But, when a system becomes complex—as, for example,
language—we need to look at the formal, the formative, relations between
the processes that compose the system.

Archetypes
It has not always been this way. Over the centuries, form as shape and

form as pattern have usually been juxtaposed, perhaps with one or the
other the focus of discourse. As described in the Preface of this book,
historically, form as pattern began to be seriously considered by
Pythagoras who became concerned with archetypes. To review briefly:
Archetypes originated as tepos, the Greek word for “patting, tapping,
beating.” Tapping makes sounds. As early as 500 BC, Pythagoras began a
scientific investigation noting the differences in sound made by a
blacksmith beating on metal bars of different lengths. Pythagoras and the



Pythagoreans augmented this observation by plucking strings of different
lengths. The Pythagoreans visually observed the oscillations of the
different lengths of strings and related them to perceived sounds.

Doubling the length of an oscillating string is heard as a sound that is
an octave lower. The octave was so named because the change in sound
could be related to ratios among the lengths of the strings. The frequency
of oscillation of the plucked string was found to be proportional to the
length of the string. For instance, if the lengths of the strings have a ratio
of 3 to 2, the shorter one will be heard as being a fifth of an octave above
the note of the longer. Eight notes were identified, the eighth being a
repeat, an octave higher or lower than the first. Each note could be
represented by a ratio of numbers.

Initially, the Pythagoreans were enchanted by the beauty and ubiquity
of the patterns they had uncovered. But, as with so many regularities we
find in nature, imprecisions arise when the range of observation is
extended. In the case of the musical fifth, when it is repeated twelve times
in ascending sequence, such a deviation occurs. It seems reasonable to
expect that the sound heard would return to the original note, only eight
octaves higher. Instead, when we repeat the fifth seven times, we find that
we are off by an eighth of a note—forming an acoustical spiral. The
discrepancy is called the “Pythagorean comma.” An eighth of a note
cannot be represented by a whole number, so the result is no longer a
simple “ratio” and could therefore be described only as an “irrational”
number.

The necessity for an irrational description of such a repetition was
disturbing to the Pythagoreans. Nature could no longer be described in
terms of ratios of integers and therefore seemed no longer rational or
integral. But perhaps, by assigning the loss of integrity to the observer, the
integrity of the rest of nature could be saved. In his book The Myth of
Invariance, the mathematician Ernest McClain suggests that this
attribution of loss of integrity to ourselves, as observers, formed the
background for the ready acceptance of the concept of original sin.

Later, the acoustical spiral— a shape—was found to be generated by
an archetypical self-organizing pattern: Fibonacci numbers are composed
by adding each number to the one preceding: 1+1 = 2; 2+1 = 3; 3+2 = 5;
5+3 = 8; and so on. Botanists and painters have called the origin of such
patterns the “golden mean” because they found them in plants and



animals, such as the chamber of a nautilus shell. Fibonacci ratios describe
all these natural forms.

80.

81. Fibonacci Spirals as found in Nature

In assigning numbers to shapes and to the results of observations,
Pythagoras initiated the application of mathematics to give meaning to the
results of observations. Currently we are witnessing a similar attempt to
organize a comprehensive view of the cosmos according to string theory—
the idea that minute subatomic vibrating strings are the elements that
make up the universe. Pythagoras was the first to apply such scientific
methods to relate our views, our observations, to physical measurements.
These methods raised issues that had to be faced, issues that even today we
tend to gloss over rather than to probe our profound ignorance.



Viewpoints
In the 16th century, René Descartes based his division of matter from

mind on the distinction between types of form: the shape of matter is
geometric; thought is inherently patterned. But he did more: He invented a
method by which geometry could be brought into correspondence with
patterns—by letters representing numbers, that is, by algebra. He devised
“Cartesian” coordinates that display the values of the relationships among
numbers along their axes (ordinates and abscissas) and thus could plot
these relationships within the field delineated by the axes as a line or a
curve. Inherent relationships could now be outlined.

During the 17th century, Isaac Newton and Gottfried von Leibniz used
Descartes’s recently invented method of outlining inherent relationships
among algebraic representations of numbers that we use today in
analyzing the results of making electrical recordings from the brain.
Newton and Leibniz showed that any curved shape, any surface, could
serve to locate, in time and space, the observation being measured:
location was defined as pattern—a point on the surface in its relation to its
neighbors. In turn, the accuracy of the measurement was defined by the
between-point-distance which can be varied according to the needs of the
task: the smaller the distance, the more accurate the measurement. Points
may be infinitesimally small. And when we sum up all the points and their
relationships, we obtain a measure of the area or volume that is enclosed
by the surface. Areas and volumes may be infinitely large.

82. Coordinates

Leibniz wrote a book called The Monodology wherein he describes a
world-view that centers on inherent relationships that are self-contained



shapes, much like those Russian dolls that contain, within each, another
doll and, within that one, another and another. For Leibniz, the most
encompassing shape that contains “all” is Unity, that is, God.

When I first read The Monodology in my teens, it seemed to me that
the aging Leibniz was simply making his peace with God, the ultimate
monad. It was not until my own work led me to holography that I realized
that Leibniz’s monads were derived from his viewpoint that also generated
his mathematics. Leibniz based his calculus on accurately measuring
shapes; monads were conceived by him as transformations, shapes that
encompassed each other to compose the world that we perceive. Every
monad is a world apart that reflects the whole macrocosm; monads are
“windowless,” one contained within another, much as those Russian dolls.
Today there are proposals concerning multiple universes that are
essentially monadic in character.

83. Derivative: Locating a Point on a Surface

84. Integration: Determining the Area of a Surface



Change “windowless” to “lens-less,” and the structuring of the world
we perceive is transformed not into shapes that contain one another but
rather as holographic processes formed of interpenetrating patterns.

Newton had to choose between explanations of his observations in
terms of shapes and explanations in terms of patterns. In discussing his
experiments with light, he chose shapes—points, particles—to describe his
results, contrary to his contemporary, the Dutch physicist and astronomer,
Christiaan Huygens, who described light in terms of patterns of
oscillations. However, much of Newton’s activities centered on alchemy,
on patterns of relationships and their power to transform, which had
initially been based on the oscillations that produce sound. Thus, when it
came to analyzing patterns of color, he divided the spectrum into seven
colors, explicitly in analogy to the seven tones of the dia-tonic scale.

Action at a Distance
An essential issue where we need to distinguish pattern from shape

concerns communication: How can a pattern generated in one location at
one moment in time be sensed at another location at another time? For the
Pythagoreans the question was: How do the patterns generated by
oscillations, the vibrations of strings, reach the ear of the listener? Their
answer, to which we still subscribe today, was that the air provides the
medium for transmission, much as water serves as a medium for
transmitting the energy that produces waves at a shore. But when we look
closely at waves, they turn out to be rather mysterious: Swimming at the
beach, one is bowled over by an incoming wave (unless we bodysurf or
ride on a surfboard) that clearly moves the water. But get out beyond the
breakers on a raft and all that happens to the raft is a bobbing up and
down, an oscillation. The energy of tsunamis (more generally called
“solitons”) traverses the entire Pacific Ocean practically undetected, only
to raise huge eighty-foot-high devastating moving walls of water on
reaching the far shore.

Quantum physics has struggled for almost a century with the
question: Do subatomic processes occur as particles or as waves? When
measured at an aperture, a slit, the energy activates the measuring
instrument discontinuously; when the energy hits the wall behind the slit,
interference patterns produced by waves are demonstrated. So the question



is: Does subatomic energy form things, particles, or does it come in
waves? Or both? Or neither? If the energy is like that of a tsunami, it
exists as oscillations until there is a shore, a measuring device to constrain
it. The attributes of the shore, the context within which a measurement is
made, configures the subatomic energy as much as does the energy itself.
(See Appendix A.)

Patterns that produce change, measured as energy, can be transmitted
across space and time. Ordinarily, transmission needs a medium within
which to operate. When we cannot identify such a medium, we are puzzled
and give the observed phenomenon of “action at a distance” a name.
Newton used the name “gravitational force” for one such recurring action.
Einstein suggested that the medium within which gravitational and other
forces operate might be the shape of space-time itself. Over a prolonged
period of usage, names appear to make palpable the observations they
were invented to explain. But from time to time, on the basis of new
observations, a re-examination of the ideas that gave rise to the names
occurs. Such a re-examination has taken place in atomic physics and in
cosmology over the past century. Currently (as suggested by Einstein in
the quotation at the beginning of the next chapter) cosmologists are
looking at pattern (e.g., oscillations of strings) rather than the shape of
space-time as the medium within which actions and energy transmission
can occur.

The Power of Names
Newton could not formulate his science without resolving the

transformative issues that alchemists had also faced. Initially, in his
equations involving velocity, momentum and acceleration he successfully
followed Galileo’s hands-on way of proceeding. But in order to unify the
Galilean observations with the observations of Tycho Brahe and Johannes
Keppler on the motions of the planets, he unsuccessfully tried various
explanations in terms of ethers of various densities. Much to his dismay,
Newton had finally to resort to Keppler’s invention of a “gravitational
force,” a Pythagorean “relationship” between centers of shapes, to account
for action at a distance that could not be directly explored.

The gravitational relation is a pattern that gives form to our
observations. Gravity is not a “thing,” a shape that exists apart from our



powers of observation. Newton had to ask, as do we, “Is the pattern of the
relationship that is gravity due to the way the human sensory apparatus
works?” And why today do we tend to “reify” gravity to be a shape, a thing
rather than a relationship? A part of the answer lies in the human
propensity for naming, thus assigning an existence as a thing, an entity, to
that which has been named. This shift in meaning has consequences, as we
saw in Chapter 8, when we tried to describe what we experience as a
muscular force in terms of Newton’s gravitational relationship.

An anecdote illuminates the issue. I’ve been puzzled by a question
friends often ask me when I mention someone I’ve recently heard from or
worked with in the past. My friends always persist by asking “Who was
it?”—a question I could only answer to their satisfaction by coming up
with a name—a demand rarely fulfilled, because I’m bad at recalling
names. As noted in Chapter 20, I was discussing, with an undergraduate
student majoring in philosophy, the fact that much of the central area of
the human brain cortex of the left hemisphere is involved in naming, and I
was wondering why this should be so important. The young lady promptly
explained: “If something doesn’t have a name, it doesn’t exist.”
Conversely, if it does have a name, does “it” exist? From her comment, I
suspect that Newton was upset when he realized that using the word
“gravity” for a phenomenon that was, for him, an intuition he was naming,
something he could not “lay his hands on” but which existed only by
inference from measurements made of the patterns of relationship between
shapes: planets and the sun; the sun, and its galaxy.

Today, Newton’s gravity, an attraction between material shapes, has
become familiar because we have learned from an early age that the
named “it” exists. Now some additional observations in quantum physics
and cosmology demand explanations of action at a distance, relationships
for which we had to coin new names. “Fields,” “forces” are the name most
often applied to the patterns that relate material shapes into larger units in
physics and cosmology. “Valence” is used in chemistry, as in H2O, where
the two atoms of hydrogen have two negative valences and the atom of
oxygen has two positive valences, thus allowing them to bond, to combine.
In biology and psychology, we use the term “memory” to describe the
bonds that bind patterns, bonds that describe processes occurring at one
time and place with those occurring at another time and place. Naming is
the most popular and useful way to identify observations, provided that we



do not lose sight of the fact that when we name a pattern we should not
confuse that pattern with a shape just because it has a name.

The Birthing of Psychology as a Science
Names have played a critical role in forming the science of

psychology as well as forming ideas in the natural sciences. B. F. Skinner
was adamant in his insistence that behavioral science could never be based
on the use of natural language. Noam Chomsky’s devastating attack on
Skinner’s views did not change this. I heard that when Skinner read my
paper “On the Neurology of Thinking,” he was very upset, exclaiming:
“Such beautiful data couched in such horrible language.” But when it came
to the relation between brain processing and behavior, Skinner made it
clear that they were different: “There are two unavoidable gaps in the
behavioral account, one between the inciting stimulus and the organism’s
response and the other between that response and its effect on further
behavior. Only brain science can fill these gaps and in doing so it
completes the account; it does not give a different account of the same
thing.”

Freud, also interested in creating a science of psychology,
distinguished, on the one hand, between his clinical theory derived from
his interviews with patients, and on the other, his metapsychology, which
had two components: brain processes and cultural contingencies.

The critical role played by names in birthing the science of
psychology is described in a superb book by Kurt Danziger Naming the
Mind: How Psychology Found Its Language (1997). Danziger shows that
many of the ideas, and therefore the names we use in psychological
science, originated in sociology (and culture) and were modified on the
basis of bias and experimental result to form the language by which we
communicate today in psychology.

Remembering
As described in the previous chapter, our remembrances, even more

than our perceptions, undergo transformations as our brains process them
whenever there is a change in the conditions of processing—such as a
view from a different person’s perspective or from the vantage of a new
job or a new relationship. An often-used example is the story of



Rashomon, a Japanese movie from 1950 in which the same story is shown
from numerous perspectives. Another, perhaps my favorite example, is
Lawrence Durrell’s 1957–1960 The Alexandria Quartet, a set of novels
about the same group of people, each book narrated by a different member
of the group. People exposed to the same situations “remember” them
differently as each person views his group from an egocentric set of
coordinates. These egocentric views are different from the view of the
group from outside the group, a view in which the coordinates have a
“heliocentric” origin.

Sir Frederic Bartlett of the University of Cambridge, England, called
attention to the fallibility of memory in his memorable 1932 book
Remembering. At a conference held at Cambridge University in the mid-
1950s, I presented my ideas on “The Neurology of Thinking”: I indicated
that the brain’s transactions with sensory input organized our attention;
that the brain’s transactions with its motor output organized our
intentions, and that the brain’s transactions with its memory organized our
thinking. I emphasized that the transactions entailed transformations,
transfer functions. But I did not, as yet, have clearly in mind the
conception that correlations were inadequate to handle these transactions
— that a change in coordinates, “coordination,” characterized the
transactions. Nonetheless, Sir Frederic expressed his approval: “Best thing
I’ve heard since Lashley.”

Furthermore, it would still be a decade before I could suggest how
our sensory input, motor output and “thinking” might coordinate with our
brain’s memory processing. As described in earlier chapters, Leith’s
implementation of Gabor’s holographic process finally indicated how the
brain’s memory could become distributed (dis-membered) before being
assembled (re-membered) on any particular occasion. As noted in Chapter
2, on the basis of Leith’s implementation, I was able to propose that two
separate brain processes are involved in the organization of the memory
process: a deep distributed structure that is made up of fine-fibered
branches of brain cells along with their connections—a web of neuro-
nodes; and a surface structure made up of neuronal circuits that is
composed by large nerve trunks. Memory becomes distributed in the deep
structure, where it can be influenced by local chemistry and/or new inputs
from other parts of our brain as well as input from our senses and the
response of those senses to our actions. Remembrances address the



“content” of this distributed store, formulate and reformulate the retrieval
according to the demands of the moment.

The Roads Not Taken
The story of how I gradually arrived at my ideas about how memory

becomes processed in the brain is worth telling because, for students
especially, it shows how understanding is often so slowly achieved. To
paraphrase William James, one must continually bring back a wandering
attention to the issue at hand. Our journey is rarely straightforward.

The main obstacle to my understanding of how our brain organizes
our memory was that I shared the commonly held conception that memory
processing consisted of storage and retrieval. This obstacle was removed
only when I realized what should have been obvious to me: that retrieval
itself is a stored memory process—a memory process that is separate
from, but addresses, the distributed memory processes in the deep
structure of our brain. In the analogy I presented in Chapter 19, not only
the performers in an orchestra, with the help of their musical scores, need
to remember the music, but so also does the conductor, if he or she is to
elicit the appropriate musical response from the orchestra. Memory
“retrieval,” though separate from what I have called deep processing, is
nonetheless a stored brain process that must become activated to be
effective. Here is the story of my journey into “memory,” much as it
happened.

More on Lashley
As so often, my journey’s starting point involved my involvement

with Karl Lashley. Lashley had always been deeply concerned with trying
to decipher the form memory might take in our brain. In his research, he
had injured large regions of the brains of rats, such as their visual cortex,
without eliminating the rat’s visually guided behavior. Lashley had
therefore concluded that our brain had within it great reserves for storing
the results of learning, reserves that are mobilized when needed for
remembering to occur. Indeed, as far as Lashley had been able to discern,
memory appeared to be distributed widely over a mass of brain tissue.

Lashley had supported his conclusion with additional research, again
using rats. He had trained them to carry out tasks such as running a maze.



He then endeavored to find the area of the rat’s brain that “contained” the
memory of what the rat had learned. He destroyed different areas of their
brains, but no matter which part of the brain he did away with, as long as
he left a small portion, the rat still retained the memory of what it had
learned. Its skills might be seriously affected—the rat might stumble
through the maze, for instance—but it still remembered the route it had
previously learned in order to obtain a reward.

Lashley concluded that there was no specific location in the brain that
held a specific memory, that each memory must become distributed
throughout an entire region of the brain. Lashley summarized his results in
his now-famous paper, “The Search for the Engram” (an engram is a
memory trace). “I sometimes feel,” he said, in reviewing the evidence on
the localization of the memory trace, “that the necessary conclusion is that
learning is not possible. Nevertheless, in spite of such evidence against it,
learning does sometimes occur.”

For the most part, psychologists were pleased to accept Lashley’s
views. Gary Boring, the Harvard historian of experimental psychology,
commented that, in view of Lashley’s findings, psychologists need not
trouble themselves about brain anatomy or physiology since, in Lash-ley’s
terms, all parts of the brain have equal potentiality to learn. Of course, this
was an extreme statement and over-simplification that Lashley himself did
not believe: he had frequently stated to me that limited regions of the
brain might be essential for learning or retention of a particular activity,
but the parts within such regions appeared to him to be functionally
equivalent. As he told me on several occasions, “It’s important sometimes
to state things in black and white or no one will listen. One can always
retract in the next publication.” Though he was indeed heard, I believe he
frequently did his own ideas a disservice by overstating them.

On another occasion, after Lashley had given a fascinating talk on
instinct—that is, inherited memory—he took a different tack. I asked him
to define “instinct.” My query brought forth more examples and stories,
but no definition. When I kept pressing for a definition, Lashley finally
said, “Dr. Pribram, in life one has a choice of being vague or being wrong.
On this occasion I choose to be vague.”

(We were Dr. Pribram and Dr. Lashley until his wife died in 1956. On
that occasion, he wrote a lovely letter saying that I was the closest he had



come to having a son and that we should address each other as Karl from
then on.)

Lashley’s answer was maddening to someone like me who, as I like to
quote Clerk Maxwell, wants “to know the particular go of it.” How can one
know the how of a process with only anecdotes and a name to go on? In his
over-statement of the case for a distributed memory and his under-
statement of his views on instinct, Lashley fell heir to the Pythagorean
propensity for relishing the mystical.

A Different Road
My own clinical experience regarding the role of the brain in

organizing memory differed from Lashley’s. My view of memory storage
came from my residency with Paul Bucy, who wrote an excellent chapter
in Roy Grinker’s 1943 text Neurology on how to find the location of “brain
tumors” on the basis of their effects on memory. This view of the
relationship between brain organization and memory was based on our
findings in patients and was shared by most neurologists, neurosurgeons
and neuroscientists at the time, and still is. For instance, deficiencies in
remembering faces occur when there has been damage to one brain area,
while deficiencies in remembering words occur with damage to a different
area. According to this view of memory, our memories are stored in
particular locations, or “modules” of the brain.

In addition, I had been taught during my internship with Bucy that
electrical stimulation of certain places in the brain can produce
remembered stories of the patient’s experiences. I was so intrigued by
these results that I was drawn into the practice of neurosurgery, only to be
sorely disappointed: nothing of the sort ever happened in my surgery.
Electrically stimulating the temporal lobes of my awake patient’s brains
(when this could be done without causing damage) produced no awareness
of anything, no stories or even words.

Some years later I had the opportunity to ask Wilder Penfield, whose
work had inspired me to try this with my patients, about my failure, and he
replied, “How many epileptics do you operate on?” I replied, “None. I
send them all to you as I don’t have sufficient facilities to assure that the
brain is kept moist during the long course of searching for an epileptic
focus.” (Drying of the brain would cause more scars and seizures.)



Penfield, surprised by my answer, told me that eliciting any memories by
electrical stimulation works only in epileptics. He was unaware that
everyone didn’t know this, adding: “I wouldn’t be operating on normal
people, or stimulating their brains, would I?” I urged him to point this out
in his papers, one of which I was about to publish. He did so and thereafter
he always made it clear that his results had been obtained exclusively with
patients who were suffering from epileptic seizures.

Some ten years later, at Yale, Penfield’s experiments were repeated,
using electrodes implanted in the area of the brain that Penfield had found
electrically excitable so that the same stimulation could be repeated on
several occasions. A surprising result was obtained: when the patient was
primed by talking about his or her childhood, and then the brain was
electrically stimulated, the patient would produce childhood memories;
when the patient was primed with talks about current situations such as
family relations, electrical stimulation of the brain would instead produce
memories of current events. The location of the electrodes and the
parameters of electrical stimulation were identical in each case, but the
contents of the patient’s recalled memories were different and depended
on the context provided by the different contents of the priming sessions.
Penfield’s, Lashley’s and Bucy’s review of the observations showed that
ordinary neurological and neurosurgical practice on the relationship
between brain and memory needed sorting out —and my conversations
with the protagonists of these various approaches to memory failed to be
illuminating.

My Interpretation
My explanation of Penfield’s results focuses on the brain scar that

produces the epilepsy. The scar tissue itself actually acts as a conduit for
the electric current to assemble— to re-member, or re-collect—a
distributed, dis-membered store. In the intact, non-epileptic brain, a
similar localized process (a temporary dominant focus of activity) using
the brain’s neuronal circuitry would normally assemble a memory from its
distributed store of neuro-nodes in the dendritic deep structure of the
brain. Both Penfield’s and Lashley’s results are readily explained, as noted
earlier, by the realization that memory is processed at two levels of brain
anatomy and physiology: a surface structure made up of separate



localizable neural circuits and a deep structure made up of a distributed
web of neuro-nodes in the fine-fiber branches of brain cells within the
circuits. Neither the surface nor the deep processing of memory “looks
like” what we experience in remembering.

In Summary
To return to the beginning of this chapter: I have outlined how mind-

talk and brain-talk become related by way of transformations. Linguistic
philosophers have stated that psychological processes and brain processes
are simply two ways of conceptualizing a single process: we have “mind-
talk” and “brain-talk.” Around 1970, standing on the shoulders of their
insight, I began to ask what that process might be. I started by suggesting
that whatever that process is, it becomes embodied in our psychological
experience and behavior and in the way the brain works. We are not just
talking; the patterns that the language describes reflect the way in which
the process actually works. I used music as an example. The patterns we
appreciate as music can be performed by an orchestra, embedded in the
grooves of a compact disc, or sung in the shower in early morning. The
patterns are not just talk; they become embodied in a variety of media.

The embodiment does not have a spatial form. Rather, it is an
embodiment of pattern. In this chapter I briefly related the history of the
discovery of pattern: pattagein. The Pythagoreans showed that relations
between sounds could be heard when strings of different lengths were
tapped. The history of the development of mathematical relations among
patterns has led to the current formulation of how complex forms are
generated. This formulation allows a fresh view of the relationship
between psychological and brain processes. Rather than rely on
correlations that demand emergence of psychological properties from
brain properties (and an attempt at reduction of the psychological to the
brain properties) we search for a change in the coordinates (a search for
the transfer functions) that describe the relationship. The reliance of
efficient causation or final causation does not work. What does work is
Aristotle’s formal, formative causation that, in this instance, is formed by
transformation.



Applications



Chapter 22
Talk and Thought

Wherein the view of language as communication is supplemented by its
even more ubiquitous use as a form of expression—and the view that
thought is a form of virtual expression.

“I cannot do it. Yet I’ll hammer it out.
 My brain I’ll prove the female to my soul, 

 My soul the father, and these two beget 
 A generation of still breathing thoughts, . . .

—William Shakespeare; Richard II, Act V., Scene 5

But of all other stupendous inventions what sublimity of mind
must have been his who conceived how to communicate his most
secrete thoughts to any other person, though very far distant
either in time or place, speaking with those who are in the
Indies, speaking with those who are not yet born, nor shall be
this thousand or ten thousand years? And with no greater
difficulty than the various arrangement of two dozen little signs
upon paper? Let this be the seal of all the admirable inventions
of men.

—Galileo Galilei



Of Monkeys, Gibbons and Great Apes
I set out to understand the manner in which the human brain regulates

the body and its behavior. Language is central to such an understanding
because our initial encounter with the importance of our brains comes
from what brain-injured patients tell us. Despite this, most of my research
was performed with monkeys—primates who don’t talk. The reason that
non-human primates were essential to this research is that brain injuries in
humans are too messy to be of use in enabling us to systematize the
relationship between brain damage and the behavior of the person who has
sustained such damage. This situation has changed considerably since the
advent of PET and fMRI imaging techniques.

I was successful in my work with monkeys because I love animals
and they usually love me. But to participate in my research, the monkeys
had to spend a part of their time in cages. Monkeys do not thrive when
caged singly, so I saw to it that there were always at least two animals in a
cage. When they were not being tested daily, they lived in a large
compound with plenty of available outdoor space. Also, I introduced
human surgical techniques and other caretaking innovations to provide
them maximum cleanliness and comfort. I’d rather have just hugged the
creatures, but rhesus monkeys are not very huggable due to their feisty
nature, so attachments were not all that deep.

By contrast, gibbons (minor apes) are delightful. I once visited an
island within Bermuda where a colleague, José Delgado, formerly at Yale
and then at the University of Madrid, had gathered a large group of
gibbons. A beautiful young gibbon lady took me by the hand and,
chattering continuously, showed me around the island. Though we’d been
together only a few hours, parting was traumatic for both of us—she threw
her arms around me, would not let go, screamed and wailed. Jose was not
pleased and chided me: experimenters should remain aloof.

I seriously considered purchasing some gibbons for pets. Instead, I
purchased two of them—a blond female and a black male—for my
Stanford laboratory. My graduate student Penny Patterson and I tested
them in a variety of environments including a mirror test, in which they
failed to recognize themselves, which according to the accepted view
means that they do not have a sense of self. I don’t believe this but have no
evidence to support my disbelief. Gibbons, living in a mirrored
environment, occupied our research interests until I purchased Koko, a



baby gorilla from the San Francisco Zoo, for Penny to raise and to teach
“language” skills. Her research with apes, which continues today, helped a
bit to fill the gap left in my studies with non-speaking monkeys.

Teaching “language” to apes was attempted repeatedly at the Yerkes
Laboratory of Primate Biology. I had become intimately involved when I
succeeded Lashley as director. Cathy and Keith Hayes had raised a
chimpanzee, Vicky, in their home and had tried to get her to vocalize with
no real success. The few vocalizations—“food,” “toilet,” “shoes,” which
Cathy thought she could differentiate—were not sufficiently distinct for
the rest of us to acknowledge.

Vicky was not the brightest of animals. A caged male chimpanzee
used as a control on various tests routinely did better than Vicky. This may
have been due to the fact that Vicky’s mother had bashed the infant’s skull
against the cement wall of their cage whenever an attempt was made to
separate Vicky from her mother. Lashley, who had little faith in the
“language experiment,” thought we should not waste our time using Vicky
in more promising experiments and (rather gleefully) assigned Vicky to
the Hayeses. To this day, I don’t know how the Hayeses failed to find out
about this early history of their ward.

By the time I took over the Laboratory, Vicky was at puberty and
ready to be paired with a mate. I was going to refurbish a small cottage
(that had been used by the Laboratory’s caretakers) for a proper “home”
for the pair. But fate intervened: About a week after Vicky and I were
sharing a drink, each of us sipping simultaneously from the same glass
with our own straws, Vicky suddenly came down with a high fever and
died. Cathy was very upset, almost hysterical, as might be expected. But
Vicky’s death solved a lot of administrative problems that may have made
Cathy feel that perhaps I was responsible for Vicky’s death. Interestingly,
no one at the Laboratory worried or inquired about my health—after all
I’d been sharing a drink with Vicky a week earlier and she was suddenly
dead from what appeared to be a brain infection!!

The first chimpanzee who was successfully taught to communicate
using signs was Washoe at the University of Nevada. The brilliant
breakthrough was made by Allen and Beatrix Gardner of the University of
Nevada. (Bea had been a roommate at Yale of one of my graduate
students.) The Gardners used the gestures of American Sign Language,
thus bypassing the vocal chords. The Gardners raised Washoe in their



home in Washoe County. I repeatedly visited Washoe during the 1960s.
Washoe mastered about 300 signs.

By now, several major apes, like chimpanzees and gorillas, have been
taught to use American Sign Language. They all can effectively use about
300 signs. They can string together three signs and use the ordering of
those signs in conveying differences in meaning: “Penny hug Me” vs. “Me
hug Penny.”

They can use words to lie; and then laugh when the experimenter
finds out that she’s been had: Penny had lost her watch and asked Koko if
she knew where it was. Koko signed “no” repeatedly during the day, only
to hand Penny the watch at dinnertime, laughing uproariously.

Apes as well as porpoises, whales and sea otters— and grey parrots—
can go to find an object near some other object that has been flagged by an
experimenter using sign language.

Primates, including apes, have been taught to recognize numbers and
to attach numbers to how many objects are in front of them. They can also
push symbols displayed on a computer screen, resulting in the delivery of
the symbol in an attached cup. The ape then accumulates the symbols in
order to trade them for bananas, apples, oranges, peanuts, etc.

The most impressive of these results has come from Kanzi, a male
bonobo (pygmy) chimpanzee at Georgia Tech. An unsuccessful attempt
had been made to teach Kanzi’s mother to respond to spoken English by
Duane and Sue Savage Rumbaugh. Kanzi grew up listening and observing
the simultaneous presentations of words in sign and vocal language. One
day, Kanzi took over and carried out the requests that the experimenter had
spoken to his mother. Kanzi’s skills have been nurtured, and he is
proficient in using a machine that takes tokens to deliver his daily rations
of food.

I was leaving after a visit with Duane and Sue Rumbaugh and wanted
something to take home with me, so I asked Kanzi for a “souvenir”—a
word he had not heard before. I explained to him: “I am leaving, going
home.” “Can Kanzi give me something to take home?” After a few
repetitions, Kanzi went to a rock pile, took a rock in his right hand and
another, larger, in his left. He placed the second rock on a fairly flat
surface and smashed the first rock down on it. He gathered the pieces,
examined them and threw them into a trash heap. He did this again, and on
the third try, he straightened up, chest out, and came over to hand me



beautifully formed flint-shaped stones! We hugged and I departed,
gratefully.

There have been serious and rather vicious disagreements as to
whether apes can be taught language. I have tried to arbitrate among my
friends and colleagues—and in the process lost the upper half of the
middle finger of my right hand! I was in Oklahoma, visiting Washoe. I was
trying to persuade the University of Oklahoma administration to change
Washoe’s caging: the cage was made of extruded metal, which has very
sharp edges. Perhaps Washoe was trying to show me how sharp those
edges were, when she suddenly came from the rear of the cage while I was
feeding her adopted infant (she had lost her own baby due to an infection
that followed an injury to its finger that had been cut on the sharp caging)
and slammed my hand down, cutting my finger to the bone. (Don Hebb
had written a paper demonstrating that males always give a signal before
they attack—while females never do.)

An artery spurted blood, and I made nasty remarks about trusting
females. Washoe signed “sorry, sorry, sorry.” My blood pressure fell to
zero but I felt fine as we traveled into Oklahoma City to get to a hand
surgeon who was too busy with office patients to deal with me. I was taken
to the hospital and the surgeon operated on the hand beginning around
9:00 p.m.—some ten hours after the injury. The surgery was performed too
late; gangrene set in after ten days and it took three further operations
back at Stanford to eventually save the bottom half of my finger and
rearrange tendons so my hand could function.

While I was recovering from surgery, I was able to dedicate the
addition to the hospital—the purpose for which I had been invited to
Oklahoma—with bottles of intravenous medication dripping into both my
arms. I was able to attest to the excellent care I was receiving. Only an
emergency message coming over the paging system for my host
interrupted the smooth flow of my intravenous assisted presentation: his
wife had suddenly gone into the last stage of labor and was delivering in
the OB ward upstairs.

Neither Washoe’s nor my ability to sign with our hands was impaired
by the incident.

Is It Language?



The battle over ape language was not the first to align those who wish
to conceive of humans as distinct from other animals against those who
believe in our more gradual evolution from primate ancestors. In the latter
part of the 19th century, the French Academy of Sciences had to forbid
discussion of the topic of the uniqueness of humans, because of the
physical assaults that the Academicians lobbed against one another.

With regard to language, the issue is not that diffi-cult to resolve.
Those who are raising apes to communicate with signs and symbols are
doing just that: the apes use signs and symbols to communicate. However,
this does not mean that apes use “language” as defined by the criteria of
linguists: sentence construction, imbedding of phrases; flexibility in
rearranging phrases, and the like. Apes don’t communicate the way human
Chinese, Arabs, Germans or Americans talk. Apes can be taught to use
“words” to communicate, but concatenations of words do not meet the
linguist’s definition of language. And when apes feel that there are better
ways to communicate, they don’t bother with the words.

Penny Patterson, Koko the gorilla, my daughter Cynthia, and I were
strolling together on the Stanford campus one morning. Flowers were in
bloom. Penny wanted to show off Koko’s talents: she signed to Koko to
sign to Cynthia that she should smell the flowers. Koko understood and
grabbed Cynthia by the nape of her neck and shoved her face into the
nearby flowerbed. Instant communication. Who needs language or even
words? Deeds are so much more direct.

Complexity
Language is a prime example of a self-organizing system. Language

begins as a proposition, a proposal by one human to another, a proposal for
a possible action. In the few instances where a human has been raised in
isolation very little emerges that can be recognized as language. Both
during development and culturally, the language facility grows in
complexity, not only as it is used to communicate with others but in self-
communication.

But when we focus on language as the centerpiece of the difference
between apes and humans, we are apt to miss the larger picture. In the
opening chapter of this book, I noted the enormous difference between
cave paintings and those made by chimpanzees. Nor do chimpanzees



compose musical phrases. Nor do they pluck strings of different length
and relate the changes in sound to octaves and their harmonics. No one has
ever been able to train an ape to perform a finger-tapping sequence
starting with the thumb, going to the index, then the middle, ring, and little
fingers.

In brief, we can state that cortical Homo sapiens sapiens is more
capable than any other animal of expressing and comprehending
complexity. Complexity, as Lévi-Strauss noted, requires a formal rather
than a reductive efficient-causal analysis.

For several years I criticized the use of the term “complexity” in
scientific and philosophical discourse, because I could not understand the
“how of it”—How can we measure it? The reason for my failure to
understand was that I was thinking only in terms of shape: diversity and
distinctiveness provide part, but only a part, of what makes a scene or a
behavior complex. The French mathematician, Benoît Mandelbrot, has
shown that progressive magnifications and “minifications” of a coastline
produce as much diversity at each level as could be charted at the previous
level. This is called “self-similarity” and is certainly not a measure of
“non-similarity”—that is, of complexity, though it is usually referred to as
such.

As described in Chapter 16, the answer to my quest to measure
complexity came when I had to deal with complexity in the laboratory.
Complexity can be measured in terms of pattern. I’ll briefly review how
the answer came about: For analyzing EEG recordings, I was provided the
following recipe by Paul Rapp. Take each occurrence and give it a symbol.
Then look for recurrence of groups of symbols and give each of those
groups a new symbol. Continue the process until there are no more
recurrences. The greater the number of levels of processing before there
are no more recurrences of patterns, the greater the complexity, the greater
the refinement of the process.

In Chapter 14, I dealt with the difference between information as a
reduction of uncertainty and what I had called “novelty,” the potential for
increase in uncertainty. This insight had been attained while coming to
terms with the function of the amygdala: processing the novelty of our
experience is a change in what has become familiar; novelty refines the
familiar. Novelty produces an increase in the complexity of processing our
experience—novelty is not a process of acquiring information in the sense



of Shannon’s reduction of uncertainty. Novelty is a function of amount of
structure in redundancy, of recurrences, of repetitions.

Now I had the opportunity to define complexity in terms of
hierarchies of redundancy. It worked for EEGs: As we get older, our
behavior slows down, but the patterns of our EEGs become more complex.
The complexity, the refinement of our experience and behavior in all of
their splendor is obvious. The relationship between our cultural
complexity and the complexity of our brain’s cortical processes is
currently being documented daily by cognitive neuroscientists.

When it comes to the question of which brain processes make
possible complex behavior, there are some answers that come easily and
some that we have difficulty even in articulating. I’ll take the easy issues
first:

There is a large surface of cortex in the middle of the brain that we
know has something to do with language. The particular method used to
establish the relationship between this part of the cortex and language is to
use brain electrical stimulation: A patient is asked to speak, the surgeon
begins the electrical stimulation, and those brain sites where speech
becomes interrupted are considered to be critical for speech. These
electrical stimulations may also disrupt certain other movements,
especially those involving the patient’s face and fingers.

A more recent method of finding the areas of cortex involved in the
production of language is to record from single cells or small groups of
cells while the patient is speaking. Changes in the response of the cell
indicate that the cell’s activity is related to speaking. There is a good deal
of congruity when we compare the results of the disruption of speech and
the effects of its production on single cell recordings.

Currently, many laboratories use brain-imaging techniques to explore
different aspects of understanding and producing language. Again, the
parts of the brain that are activated lie in its mid-central region. These
reports suggest that the cortical areas involved in language have a patchy
(rather than a dense) involvement in any particular language function.



85.

The results of all of the experimental studies, together with reports of
brain injuries that have affected speaking (aphasia), provide us with a
general picture: in ordinary right-handed persons, involvement within the
anterior regions of the left hemisphere relates to linguistic expression; the
posterior regions are more involved in comprehension. The “typical” case
of an expressive aphasia is a staccato of “content” words such as nouns
and verbs, with very few “function” words such as “the,” “and,” “better,”
“less,” and so forth. By contrast, the “typical” patient with a
comprehension deficit will fluently rattle off sentence after sentence —
performing so well that it may take the listener a few minutes to realize
that the sentences are totally meaningless. It is this part of the brain that
deals with the semantics of language.

Injuries near the “hand representation” in the cortex can lead to
agraphia, an inability to write; injuries near the visual cortex in the back of
the head can lead to alexia, the inability to read. These disruptions of
language are often attributed to disruption of the large, long-fiber
connections between cortical areas. In monkey experiments, I found that
such long tracts are mostly inhibitory. Inhibition allows the local patterned
processes in the neuro-nodal web to operate. No inhibition, no patterned
processing.

Another observation about brain and language has been made: Injury
to the right hemisphere of the brain in right-handed patients can lead to an
inability to produce or to sense the intonations that are ordinarily a part of
speaking. These changes in “prosody,” as it is called, leave the patient
speaking in monotone—or, if the damage is toward the back of the brain,



unable to hear the nuances, the intonations that are often essential to
grasping the meaning of a communication. When the damage is toward the
front of the brain, the disability is in expressing that meaning in rhetoric,
the pragmatics of language.

For years I worried about how pragmatic processing in the brain
could be got together with those processing semantics, the referential
“dictionary” use of language. My conclusion in several published papers
was that the brain was not directly involved; that the person’s culture
formed the bridge between the meaning of an utterance and the effect it
has on a listener. This conclusion is consonant with the way continental
Europeans most often use the concept we call “behavior.” In French
comportment and in German verhaltung mean how one holds oneself. How
one holds oneself with respect to—? With respect to the culture within
which one is embedded.

But finally I came to realize that our motor cortex, in processing
“images of achievement,” does just such a “cultural” job, and the
discovery of “mirror neurons” supports this conclusion. In language
especially, as so often otherwise, the brain serves temporarily as the
culture we navigate.

All the observations that I’ve briefly reviewed provide correlations;
they give us only a first step towards understanding “how” language
influences the operations of our brain, or how the brain’s operations help
to organize our language. Even at a gross level there has been an argument
about Noam Chomsky’s famous claim that we must have a separate
“language organ” in the brain, an organ apart from the brain’s more
general cognitive facilitation. Of course Chomsky is right: there are
individuals who are cognitively feeble minded, but whose speech is
unimpaired. And vice-versa. But the “organ” is not a single lump of tissue:
it is most likely distributed among other cognitive “organs,” much as the
Islands of Langerhans that control sugar metabolism are distributed
throughout the pancreas.

Correlations, though a useful first step in helping us come to grips
with a complex topic, do not address language in terms of the central
theme of this book: How does the form, the pattern, of a language become
transformed into a brain process—and how does the form, the pattern of a
brain process, become transformed into a language?



Neural Networks and Neuro-Nodal Webs
Considerable progress has been made in studying this issue in the

computational sciences. Massively connected artificial neural networks
have been taught to produce language. The process has been slow,
requiring large computers and huge amounts of processing time—
sometimes weeks—to produce understandable words and phrases. But that
is faster than it takes a human baby to learn a language.

The reward of pursuing this exercise is that the course of
development of neural network language acquisition has been shown to be
similar to the course of development of language acquisition shown by
babies: babbling, grouping of phonemes, construction of syllables, etc. All
this is achieved without making up any symbolic representations during
the course of language acquisition.

The success of using artificial neural networks in language
acquisition points to an often-ignored aspect of the relationship between
brain processes and speaking. Those of us who speak more than one
language do not have to translate from one language to another when we
think, write or talk. Each language addresses some deeper process that is
“language neutral”—and this neutrality is not given in the form of visual
pictures. Rather, this ultra-deep structure is likely to be akin to the
dendritic fine-fibered web of neuro-nodes in our brain, the very web upon
which the architecture of artificial neural networks has been based.

But of course, human languages do use signs and symbols—and most
computer programming is based on symbolic processing. There is still
somewhat of a gap in our understanding of the operations of artificial
neural networks and our understanding of the “intelligence” of ordinary
symbol-based computer programming.



86.

Gary Marcus, in his book, The Algebraic Mind, provides a method
through which we can begin to fill this gap between pre-symbolic and
symbolic processes. Symbolic processes are hierarchically complex
structures in the sense that I defined above. Human language is
characterized by a hierarchy of tree-structures such as those my co-
authors and I described in Plans and the Structure of Behavior. But the
neuro-nodal web, like the brain’s dendritic neuro-nodal deep structure, is
massively parallel. Marcus suggests that the transformations between
these parallel and hierarchical structures may be performed by way of
“treelets”: hierarchies that are limited in depth, thus less complex than a
fully formed symbolic process.

These treelets, distributed in parallel over the processing medium, are
composed of “registers” that store “values of variables.” In Plans and the



Structure of Behavior and again in Languages of the Brain, I called such
registers “tests” which registered the degree of match of the pattern of a
sensory input to a familiar pattern. When the registers become sensitive to
a “bias,” that is, a parallel way to alter the register, the bias establishes a
value against which the tests are run. What I am suggesting is a parallel
structure even within tree-lets as well as between them. Treelets become
considerably enriched by this addition.

Marcus proposes that the variables stored in treelets provide the
meanings of communications: propositions and sentences. In Chapter 16 I
developed the proposal that we value an act on the basis of its utility in
satisfying our desires. The utility of a speech act is its meaning. I’ll have
more to say about the structure of meanings shortly, and again in Chapter
25.

The meanings of spoken utterances, their value, are often enhanced
by the inflections and gestures we use in parallel to the spoken word.
Inflections emphasize a particular expression; gestures reflect its novelty
or familiarity. The procedures developed in the practice of neurolinguistics
provide a technique to assess the sensory modality used to provide such
enhancement: We record the number of modality- specific words a speaker
uses. Does the speaker mostly use visual similes, kinesthetic (muscle-
related) words and gestures, or those derived from music or sound
(auditory)? When I was tested using this technique, I was surprised to find
out that I was using the kinesthetic mode much more often than I was
using the visual; until then, I had thought of myself as a visual thinker.



87. Treelet showing numeric registers

88. Treelet showing names in registers (From The Algebraic Mind)

The Unfathomable
For many of the foregoing observations, we are able to ask questions

and, in some cases, we can also answer them. But what follows are some



observations that, so far, have eluded even letting us frame them into
questions: A young professor of psychology has a reading problem
following an accident. He can write but can’t read his own writing. He can
read numbers, even when they are interposed in “unreadable” sentences,
but he cannot identify a letter when it is interposed among numbers.

Even more fascinating are the performances of the specially gifted: I
mentioned before that I had the experience of watching a six-year-old girl
listen to a piano sonata—once. She then played the sonata from memory—
beautifully, with feeling. A classmate of mine at the University of Chicago
was able to visualize any page from our 800-page textbook of medicine—
at will—and “read” it to us (very helpful on exams). Thomas Edison is
said to have been able to memorize an entire book in this fashion. Some
cognitively-impaired persons can do multiplications of three- or four-digit
numbers by other three- or four-digit numbers faster than a computer can
kick out the answers.

People with these abilities are called “savants.” Sometimes, when
their general cognitive abilities are severely restricted, they are “idiot
savants.” More often they are normal cognitively and in every other
respect. We know from our own everyday experience that talents are not
equally distributed among us. We are taught to “Know Thyself”—find out
what we are good at, or even better at than most people. But right now, in
the brain sciences, we do not yet know where or how to look for what
provides humans with our talents. Is it chemistry, brain circuitry, density
of connections and/or very, very early experience?

89. Valuing information (From Pribram, Languages of the Brain, 1971)



How We Use Language
Most obviously, language serves us in communicating. But the

importance of naming in constructing “things”—recall my student’s
comment “if it doesn’t have a name, it doesn’t exist”—indicates that
language must have other important uses. Language is a cultural creation
and as such (as with other cultural creations) is self-organizing: plant a
kernel, it branches and flowers. Dialects develop that help organize and
locate social groups. This happens not only in native cultures but also
among teen-agers and even within experimental laboratories: we develop a
jargon, a shorthand way of talking about our observations, a jargon that
has to be decoded before submitting a manuscript for possible publication.

In trans-disciplinary studies such as mine, difficul-ties are
encountered unless the results are formulated; that is, given in the form
required by one particular discipline. For example, I used brain recordings
obtained while a rat’s whiskers were stimulated to show what the range of
Gabor-like cortical maps looked like. The community of “whisker-
stimulaters” repeatedly rejected my manuscript because the problems they
addressed were different, and they had never encountered Gabor functions.
Only when a new journal was founded for the purpose of publishing such
trans-disciplinary studies did the paper finally come to publication. I had
sent it to the editor to find out only whether the journal might be interested
—and was surprised by having it scheduled as the first paper in the first
volume of the journal.

At a conference in Prague, my then Stanford colleague Bill Estes and
I were asked to summarize the contributions made by psychologists in a
variety of disciplines ranging from learning theorists to developmental to
social and school psychologists. To Bill and me it seemed, at first, to be a
welter of unrelated presentations—until we hit upon a way of organizing
them. The presenters were all psychologists, so we asked ourselves: What
were the questions each was addressing? It turned out that only about a
half-dozen questions had been addressed, and we found that we could
organize all the presentations around these questions. Each presentation
had been formulated in the jargon of the subfield of psychology that was
re-presented. By asking what the underlying questions were, we could re-
formulate these earlier representations.

Language formulates our experience within the values of what we are
familiar with. In order to more broadly communicate, we had to re-



formulate the essences of the presentations into a less refined, less
complex set of tree-lets—and we discovered that framing questions within
the broader, less specialized context of “psychology” served this purpose.

Another aspect of language is that it allows us personal expression or,
as one of my colleagues described talks that were being presented at
conferences, “station identification.” This aspect of our speaking can
sometimes get out of hand, especially when the presentation is read from a
written manuscript. Unless crafted by a novelist or author of an epic, the
style of the written word is often more amenable to providing a recorded
memory of a set of transactions than to serving as a vehicle for
communication.

Alexander Romanovich Luria was chairing a session on frontal lobe
function at the 1968 International Psychological Conference in Moscow.
One of his students presented some of the work going on in Luria’s
laboratory. We had carefully rehearsed her talk so that she could finish her
presentation in the allotted twenty minutes. Of course, when it comes to
the actual presentation, it almost always takes longer than planned. At the
appropriate moment, Luria held up a large placard meant to give his
student a five-minute warning to signal that her time was almost up. She
ignored this signal and went on with her planned presentation. Luria then
gave her a similar two-minute signal and another when her time was up.
She kept right on with her presentation. After another few minutes, Luria
went up to the podium and took her arm, but she continued talking. Luria
led her off the podium, down to her seat—where she blithely continued
until she had reached the end of what she had prepared. During our
subsequent meetings, she good-humoredly laughed at what had happened
and told me that she no longer has such difficulties in presenting her work.

This retreat into the cocoon of one’s personal, rather than inter-
personal use of language occurs especially under conditions of stress in
facing an audience. That is why most people at conferences read a
prepared speech and/or heavily depend on visual presentations of what
they have written. Early on in my career, I had to face similar stressful
situations and used a variety of ploys to pull myself out of such a non-
communicating use of language. On one occasion I had written my speech
—something I rarely do—because I felt intimidated by the audience I was
to face. I went up to the podium to make the presentation and found that I
couldn’t read what I had written; my sight was blurred. I simply told the



audience what was happening which served as my station identification,
and went on to verbally communicate the results of my experiments.

The self-expressive use of language is much more common than we
usually realize. This use of language helps us to sharpen and refine our
thinking. The people who are the recipients of our verbosity serve as a
whetstone against which we can hone the self-organizing aspects of our
existence. Teachers do this. The presenters at conferences do this. Talkers
on telephones do this. Dinner conversational-ists do this. Cocktail party
guests do this. Sometimes these self-revelations are exciting and
interesting to the listeners; sometimes they are boring. It depends on the
interest the listener has in the speaker and the subject of that speaker’s
interests. In speaking, language is much more often used in this fashion
than for the purpose of communication—and that is one of the important
reasons why human language differs so profoundly from animal
communication.

Writing and Reading
As indicated in the opening quotation from Galileo, it is in the

invention of writing and the reading of writings that the full measure of
the communicative aspects of language come to the fore. The role of
language in the development of meaning, by exploring contexts, that is, by
reducing uncertainty and enhancing novelty, comes into its own by way of
writing and reading. But even while reading, expression of one’s thoughts
comes into play: one imagines oneself in a participatory role as actor or
listener while writing and reading.

Thinking
By what process can thoughts influence a brain? Thoughts do not do

so immediately. Rather, for a time at least, conscious thinking appears to
be what philosophers call an epiphenomenon. Conscious thought is
coincident with the activity of the brain but ordinarily does not influence
the brain or the rest of our material universe directly. “They” can’t put us
in jail for what “we” think as long as we don’t express those thoughts
verbally or otherwise.

However, that same activity of the brain that produced the thought
can also influence other activities in the brain if the activity lasts long



enough. Such influences, such “temporary dominant foci of activity” as
they are called, were shown to produce changes in Pavlovian conditional
behavior: A dog was conditioned to raise his right foreleg whenever a
certain tone was sounded. Then the motor region in the right hemisphere
of the dog’s brain was exposed and the region of the brain controlling his
left hindleg was chemically stimulated. Now, whenever the tone was
sounded, the dog raised his left hindleg. After the chemical stimulation
had worn off, sounding the tone again elicited the raising of the dog’s right
foreleg.

A thought, conscious or unconscious, occurs and, coincident with it,
there occur many other changes in the organization of the rest of the brain
produced by changes in body functions. These changes in brain
organization are not immediately related to the occurrence of the thought.
They may be due to digesting the meal you had for dinner or the movie
you have just viewed. As long as the two processes, thought and incidental
brain organization, last long enough together, a correlation can become
established. I have called this process a “temporal hold.” The incidental
change of brain organization can now further influence the body and its
sensory receptors and concurrent actions. In turn, these changes in the
body can change the brain, its memory. The new configuration can produce
and be coincident with new thoughts, perceptions and attitudes though
they may have only a tenuous connection to the original thought.
Whenever we attend or intend, we use our body responses to transform the
thought “into practical relations with the world we navigate.” The
transformation may or may not be an appropriate one!

Anyone who has tried to write can attest to the difficulty in putting
what we think we know into a communicable form. The thought process is
not imaging or verbalizing. Rather, thinking is an uncertain groping
toward a more-orless well-defined target. Most of our thinking is,
therefore, more unconscious and preconscious than conscious. Once we
consciously formulate the thought into an image or verbal statement, the
temporary dominant focus has had its effect and we test our “vision” or
our “proposition” in the world we navigate—with the resultant
confirmation or change in the thought. I am doing this right now: looking
on the computer screen at the typed version of my thinking —confirming
and correcting what I have written by way of ordinary perceptions and
actions. But tonight at midnight I may wake up, realizing that I had left



out something, worrying the thought, finally composing a paragraph—
none of which has a direct effect other than by way of changes that have
occurred in my brain’s dominant focus, which most likely take place
during rapid eye movement (REM) sleep. Thus the more encompassing
reorganization of my brain occurs through its regulation of my body.

Some Experiments
During the 1990s, John Chapin and Miguel Nicolelis at Duke

University carried the “temporary dominant focus” type of experiment
much further. First with rats and then with monkeys, they trained the
animals to press a lever to obtain a drink of water. Meanwhile, they were
monitoring cortical activity with electrodes implanted in the animals’
brains. As reported in the February 2004 issue of Popular Science,

. . . what they [Chapin and Nicolelis] discovered instantly
challenged the conventional wisdom on the way neurons send
their messages. What they found was that the commands for even
the simplest movements—required far more than just a tiny
cluster of neurons. In fact a whole orchestra of neurons
scattered across the brain—behaved like an orchestra.
Beethoven’s Fifth Symphony and Gershwin’s Rhapsody in Blue
sound nothing alike even if many of the same musicians are
playing both pieces, on many of the same instruments, using the
same notes. Likewise many of the same neurons, it turned out,
participated in generating different kinds of body movements.

Discovery often rewards the prepared mind. Chap-in, Nicolelis and I
had discussed some of their earlier experimental results and mine: we
were always “on the same wavelength.” Their finding of “an orchestra of
neurons scattered across the brain”—neurons that were involved in an act
being analogous to the way music is produced—fits the findings I
described in Chapter 8 on “Means to Action.”

Chapin and Nicolelis took their experiments a most significant step
further. They disconnected the lever the animal had pressed to obtain a
drink and provided a drink of water to the monkey whenever his brain
pattern signaled his intention to make the movement. The monkey
continued to produce the same brain pattern as when he had actually



pressed the lever, despite the lack of any correlation between external
movement of his arm and receiving a drink. The cortical brain pattern
represented the monkey’s “image of achievement,” the “target” of his
action, not any specific movement to achieve that target.

In another step, they used human subjects who had lost an arm. The
subjects watched a display on a computer screen that was hooked up to a
lever in such a way that certain patterns on the screen were coordinated
with movements of the lever. The experimenters recorded the brain
electrical patterns from the subjects while they were learning which
patterns were correlated with the movements of the lever. The lever was
then disconnected from the display. The display was now hooked up to
another lever, a prosthetic arm. The display controlled the prosthesis while
the subjects continued to learn which display patterns were correlated with
what movements of the prosthesis. Finally, the computer display was
turned off. With some considerable effort, the subjects learned to control
some simple movements of the prosthesis with brain signals only. The
visual input from the computer screen had elicited a brain pattern that was
coordinate with “thinking of moving the prosthesis” and this was enough
to accomplish the task. Trying to move a phantom arm and looking at the
prosthesis became unnecessary: the brain pattern was sufficient to control
the prosthesis.

The procedures Chapin and Nicolelis developed are akin to the
research of those earlier experiments that established a temporary
dominant focus of electrical excitation in an animal’s cortex. The brain
focus of electrical excitation takes control of the behavior of a paw or a
prosthesis. In place of the chemical stimulus used in the earlier
experiments, in the current situation the visual input from the computer
screen serves to establish a much more refined, complex brain pattern
through learning. As described in the quotation above, the brain pattern
itself then “orchestrates” the operation of the prosthesis. The analogy with
the production of music is well taken.

Brain Electrical Patterns: The EEG
In fact, Hellmuth Petsche and Susan Etlinger of the University of

Vienna have made electrical recordings (EEGs) from the scalps of humans
while they were listening to music. Appropriately, these experiments were



carried out in Vienna, for centuries a most famous music capital of the
Western world. The patterns shown in each of these recordings differ
according to the music the person is listening to: Mozart, Bach,
Beethoven, Schoenberg and jazz each elicited recognizable differences in
the recorded patterns.

Petsche and Etlinger then went on to ask whether such differences
also appeared in trained musicians while they were reading a musical
score, while they were remembering the music, or while composing. In
each case, the differences between the brain patterns reflected the type of
music being experienced or thought about. Recognizably different brain
patterns accompany imagining, and thinking about, different types of
music.

90. Listening to Bach (a) vs. listening to Beethoven (b). (From Petsche and Etlinger)



91. Listening to Schoenberg (a) vs. listening to Jazz (b). (From Petsche and Etlinger)

The fascinating results of these experiments remind me that
Beethoven created his last quartets and his Ninth Symphony after he had
become totally deaf. Would that we had records of his brain electrical
activity during those creative moments!

Petsche and Etlinger also tested persons who spoke several languages.
These experiments revealed that speaking different languages is also
coordinate with different brain patterns. In addition, they showed that
translating from one language to another produces different patterns from
those that accompany translating from the second language to the first.
Finally, they showed differences in EEG coherence patterns when a person
is imagining an object from when she is creating a text—both with eyes
closed.



92. Imagining an abstract object (a) vs. creating a text. (From Petsche and Etlinger)

In short, these investigations have identified differences in brain
patterns that accompany differences in thinking. One can understand how
scientists and philosophers might be tempted to declare, “If we knew what
every neuron is doing, we could dispense with studying behavior and
experience.” Or, in a somewhat more sophisticated approach, brain
patterns and “thinking,” currently loosely described as “information
processing,” are in some sense “identical.” Certainly, in everyday speech,
we refer to a musical score as “the music”—as in “Where did I put that
music?” So we tend to blur the distinction between a blueprint and the
building it represents.

But, of course, closer examination shows that the patterns embodied
in the EEG, or the patterns embodied in a musical score, or the patterns
embodied in speech are not identical. The patterns can be correlated with
one another, but if we want to know how they are related we need to know
how one pattern can become trans-formed into the other: Transformations
between patterns go beyond correlation in the sense that transformation



poses the ”how” question: What are the transformations, the transfer
functions, that enable such correlations to occur? Chapter 20 is devoted to
explaining in more detail what is involved in the process of
transformation.

Deep Thought
Language is a great aid to thinking, but language or music or

pictures are not, in themselves, thought. We are often aided in thinking by
language, music and pictures, but these aids are not the essence of
thinking per se. As in the case of language, there is a surface and a deep
structure to thinking. The surface structure leads directly to language and
to other forms of experience, communication and expression of a thought.
The deeper form of the thought process is like the ultra-deep form of
language. As noted, multi-language speakers and mathematicians address
this ultra-deep structure that becomes transformed during overt thinking
and communicating.

The experiments that led to our understanding of this deep structure
of thinking were performed in the mid- 19th century in Würtzberg,
Germany. The experimenters posed a well-articulated question to a group
of subjects. The experimenters assured themselves that each person in the
group had thoroughly grasped the meaning of the problem in question. The
subjects were sent away with the instruction that they were to record the
thought process by which they solved the problem that had been posed by
the question. When the subjects returned to the laboratory, they had all
solved the problem and had recorded the thought process by which they
had arrived at the problem’s solution.

The experimenters were able to reach two clear conclusions from the
results: 1) If a problem is thoroughly understood, the answer is already
given in the question! All of the subjects had reached exactly the same
answer to the question; 2) The course of thinking the problem through to
an answer to the question was radically different for each person! Some of
the subjects went to bed after thinking for a while and awakened with a
well-formed answer. Others recorded a step-by-step procedure in coming
to a solution. The thinking process of most of the subjects had taken some
haphazard, but individually unique, course in between these two extremes.



Psychologists and philosophers were intrigued by these results. The
most prominent among them were Harvard’s William James and the
University of Vienna’s Franz Brentano. Brentano was Freud’s teacher.
Freud developed a technique to unveil the hidden differences in thinking-
through a problem. He called the technique “free association.” His patients
came to him with a problem. By asking the patient to ruminate about
whatever came to mind, Freud felt that he could retrieve the articulation of
the question that formed the root of the problem. The brain’s memory-
motive and memory-emotive structures would make such “retrofitting”
and “reverse engineering” of conscious from unconscious processes
possible.

The form of the fine-fibered distributed web of the brain makes it
likely that the form of the ultra-deep structure of thinking is a
holographic-like distributed process. Dreams attest to such a possibility.
Remembered dreams were called “compromise processes” by Freud
because they partake of some of the structure of the waking state. But even
in remembered dreams, uncles and aunts and children and places and
things are connected up in a haphazard fashion, as if they had been
entangled in another state during sleep. According to the reasoning
proposed in this book, the potential of the ultra-deep structure of thinking
is realized when the holographic distributed process becomes transformed
by way of the memory-motive and memory-emotive processes into
cortically engendered complex verbal and nonverbal language and
language-like cultural forms.

Highlights in Forming Talk and Thought
I’ll recapitulate some of the more important and often overlooked

insights reviewed in this chapter:

1. Fluent multilingual speakers do not translate from one language to
another, nor do they have recourse to pictures when they speak. This
indicates that there is an ultra-deep structure which underlies
language, a structure that can become expressed in the variety of
culturally formed specific languages. This ultra-deep structure is
most likely processed in the neuro-nodal fine-fibered web of our
brains.



2. The structure of language is made up of hierarchical treelets that
operate in parallel. The treelets themselves function as biased
homeostats that register the set-points attained from parallel inputs.
Treelets account for what linguists refer to as a surface and deep
structure of language.

3. Language is a self-organizing system that continually increases in
complexity —though there are occasions when language is pruned in
order to reach clarity of expression.

4. Language is as often used to hone self-expression as it is used for
communication. Listeners should be aware of this and not lose
patience.

5. The invention of writing and the reading of writing have extended the
reach of language beyond the here and now and beyond small cultural
groups —the first successful attempt at globalization.

6. The brain processes underlying thinking are akin to the ultra-deep
structure of language but encompass other cultural expressions, such
as those that give rise to music, painting and the creation of artifacts.

7. Thinking influences brain processes by way of a “temporal hold”
during which temporary dominant foci occur in the brain that alter
functions of parts of the body (including the sense organs), alterations
that, in turn, change brain processes.



Chapter 23
Consciousness

Wherein I explore our brain’s role in forming the multifaceted aspects of
the impressions that we experience—and their unconscious underpinnings.

. . . [A]ll original discoveries and inventions and musical and
poetical compositions are the result of proleptic thought—the
anticipation, by means of a suspension of time, of a result that
could not have been arrived at by inductive reasoning—and
what may be called analeptic thought, the recovery of lost events
by the same suspension

—Robert Graves, The White Goddess, 1966



Begetting Conscious Awareness
In the previous chapter I described a “temporal hold” during which

thoughts can initiate a “temporary dominant focus” that alters brain
activity. The thought process itself is likely to be unconscious. But
thinking is not the only way a temporary dominant focus is formed:
Thinking addresses the memory store; alternatively, attention addresses
the sensory input and intention addresses the motor output during the
temporal hold. When we consciously experience “what is it?” we are
paying attention to a sensory input. When we consciously format “what to
do?” we actively construct an intention regarding a motor output. Our
brain activity is thus shaped by a variety of processes that may or may not
be relevant to what we are seeking. William James described the situation
beautifully: In order to deal with an issue, we must repeatedly bring our
wandering attention (thought and intention) back to focus on the issue we
are trying to address. Only when a temporary dominant focus has been
formed does observable behavior change, which allows us the opportunity
for conscious experiencing. We have all had the experience of “knowing”
something and sitting down to write about that something and finding out
that what we write has little semblance to what we “know.”

Privacy and Primacy
Today, philosophers of science are concerned with what they call two

difficult problems regarding consciousness: the privacy argument that
highlights the difficulty in understanding our own and other peoples’
consciousness; and the relationship of our conscious experience to brain
processes. As a scientist, I find neither of these problems unusually
difficult as long as we understand the limitations of what scientific
exploration achieves.

The problem of the privacy of our conscious experience is no more
difficult to address than the privacy of an atom. Physicists devise tools to
ask questions of atoms; psychologists devise tools to ask questions of
people and animals. The answers we obtain are, in the first instance, not
always clear. Also, such answers may be complementary, as, for example,
when a psychologist interviews husband and wife and finds it hard to
believe that they inhabit the same household—or when the physicist
probes an atom to find a constituent to be both a particle and a wave. In



fact, I feel that the answers the psychologist obtains are often more
transparent than those that constitute current quantum physics. Also, I
continue to be amazed at what the physicist does to penetrate the secret
life of an atom: he uses ultra-high amounts of energy to smash atoms, and
he looks at an oscilloscope for traces, marks, that indicate to him that
among thousands of traces the one he is looking for is “actually” there.
The task is certainly as daunting as asking someone how he feels today or
how he voted as he emerges from the voting booth. At least the
psychologist does not have to smash his subject into little pieces in order
to make his inquiry.

In other ways, the privacy argument that someone else’s conscious
experience is inaccessible is wrong. Actually, conscious experience is
unusually accessible to inquiry: we ordinarily just have to ask someone
what he has experienced or is experiencing. We can take the answers at
face value, or we can decide to probe further. This does not mean that the
issue that has been identified by philosophers should be ignored; just that
it has been mislabeled. When one of my children chronically complains of
pain, I have a difficult time dealing with his private experiencing of pain;
this is not because I don’t believe him but because of what I know about
the generation of painful experiences. I know from animal experiments
performed at Yale and innumerable human surgical procedures that have
been performed for the relief of intractable pain (as for instance, pain
resulting from a phantom limb) that the longer pain from a bodily injury
persists, the more of the spinal cord and brain become involved in
maintaining the duration of that pain. How does a parent communicate to
a child that his pain is really cerebral by now, and no longer originates just
in his muscles, as in neuro-myalgia, without being interpreted as saying
the pain is “merely psychological?” In such an instance, trying to put forth
an interpretation in terms of an efficient causal relation between our brain
function and our subjective experience becomes a hindrance.

While teaching medical students at Yale, I shifted the issue of
diagnosis and treatment of what we called psychosomatic illnesses, such
as stomach ulcers and high blood pressure, from Aristotle’s efficient to
final causation: Once no obvious current efficient cause for a stomach
ulcer has been found, it no longer matters whether the ulcer is diagnosed
as having been produced psychologically or biologically. The focus must
now be on what can be done. It matters little whether the ulcer was



“caused” by psychological stress or by organic excess acidity or both. In
fact it is unlikely that these “efficient causes” can even be separated! The
medical question that must be answered is: What needs to be done right
now? Is the ulcer about to perforate? If so, proceed with surgery. If not,
will an antacid medication work or can the patient rearrange his lifestyle
and receive supportive therapy? Ulcer medicine is “organic;” lifestyle is
“psychological.” As the discomfort and pain of an ulcer becomes
increasingly corticalized, the distinction between matter and mind
becomes irrelevant, except in terms of what can be done to alleviate the
patient’s condition.

The privacy of our experience of pain matters because of what can be
done about that pain. The privacy of our perception of red doesn’t much
matter to anyone, except philosophers. Nonetheless, there is a way to
tackle the privacy issue: that is, by giving primacy to our own experiences
as we begin our inquiries into them. I experience the color red. I ask you
what the color of a swatch of cloth is and, if you have been raised in the
same culture as I was, you will most likely say, “That’s red”—unless you
happen to be a girl, and you have learned to distinguish red from hot pink
or cerise, which is then the color of the cloth for you. If the person you ask
is a male who is red-green color-blind, his answer may be “green,” and I
proceed to test him for color blindness by administering the polka-dot
Ishihara test for color blindness to prove that he is indeed color-blind.
Furthermore, should the person I ask be located in Somalia, he would
probably say that the cloth is black—unless he is a tailor who has traded in
imported cloths of many colors. There is no term in the Somalian language
for “red,” but there are tens of terms for green, which is the color of
vegetables upon which the Somalis depend for subsistence.

Two questions immediately present themselves: First, do girls
actually see color differently from boys? The answer is yes: to some
extent, those retinal pigments that absorb the spectrum of radiation that
enable us to see are different in males and females. Further, red-green
blindness is almost unheard of in girls. The second question is whether we
see colors for which we have no experience and for which our society has
no language. The obvious answer seems to be “of course,” but the obvious
answer may not be the correct one. We do know that when it comes to
whether we have the ability to make differentiations, thus to be conscious



of finer-grain distinctions such as the difference between red and hot pink,
the answer turns out to be that it is unlikely.

A personal experience attests to what is involved in making
progressive differentiations of finer-grain distinctions. In my case, the
differentiation depended on seeing patterns among what initially appeared
to be a random distribution of dots. My colleagues at the Yerkes
Laboratories in Florida and I were attempting to classify the cellular
architecture, the texture, of the nerve cell bodies that make up the
thalamus, the halfway house of pathways that extend from our sensory
receptors to our cortex. We initially could not see most of the distinctive
patterns of cells shown in texts. All we saw at the time looked to us like a
bunch of randomly distributed polka dots. After much peering through the
microscope and comparing what we saw with texts, we finally, after a year
or so, began not only to see what the texts were describing, but also to
make innovations of our own which we later proceeded to name and
publish. When we persistently attend to our sensory input, we are able to
progressively differentiate the composition of the world we navigate; that
is, we become more and more conscious of that world and how to navigate
it.

To Summarize
Of course the privacy of our conscious experience is a problem, but it

is no different from the problem of the privacy of an atom or molecule. By
putting the primacy of our own conscious experience first, we can explore
the patterns, the forms that contribute to making up our experience. If that
experience involves the experiences of others, communication offers us
one ready step in breaching the privacy of our own conscious experience
and the conscious experience of others. An alternate step is hands-on:
measuring and analyzing what we experience. This is the usual path of
scientific exploration.

Modes of Awareness
When we begin to examine our awareness scientifically, we find that

it comes in at least two “flavors”: our monitoring the world we navigate;
and the assessment, the evaluation of the monitoring. For example, you are
driving down a highway with a friend. You are having an interesting



conversation. The car you are driving maintains its pace on the road (even
though not on automatic drive) and stays in its lane. You may even pass an
obstructing slow driver. Suddenly you are jarred out of your conversation
—out of the corner of your eye you noticed a police car. You intentionally
slow a bit, not too noticeably—and in a minute or two resume the
discussion: “Ah, where were we?” As the driver of the car, you must have
been “aware” all along of the road in front of you (and behind you) and
how to navigate it. At the same time you were engrossed “consciously” in
the conversation.

Since the 1960s, I have called attention to the difference between the
surface and the deep structure of brain activity. The surface structure is
composed of brain circuits that can rapidly lead to the execution of
automatic behavioral actions. When something out of the ordinary occurs,
it activates the deep structure of fine fibers in our brains. Thus, the more
automatic the processing, the less of that fine-fibered web of our brain
becomes engaged. Circuits are used when we activate our well-established
habitual input-output, sensory-motor reactions. Only when an event occurs
in a non-routine, relatively unusual, or “novel” circumstance, calling for
directed attention and the evaluation of the situation—a circumstance such
as spotting the police car—does deep processing become involved.

Initially I had thought that monitoring per se necessitated the
involvement of the frontal and related systems. I had expected, therefore,
that these systems of the brain would be critical, in some fashion, to
enhance deep processing. And, to some extent, this is correct: When we
are delaying, while making a complex choice, cells in the systems of the
back of our brain reflect that delay. When the prefrontal cortex of an
animal is anesthetized, these cells are no longer active during the delay
period, and the animal fails to execute the correct choice.

Blind-Sight
My focus on the frontal cortex was disrupted when Lawrence

Weiskrantz, retired head of the Department of Experimental Psychology at
Oxford, performed experiments on patients with occipital lobe damage.
These patients were able to navigate their world but were unable to “see”
what they were navigating. Weiskrantz named this condition “blind-sight.”
I had thought that our frontal lobes were the critical parts of the brain that



made human awareness possible; here we have patients who lose their
awareness with injuries to the opposite end of the brain! But on rethinking
the problem I came to see that the frontal systems are invoked when
assessment and evaluation of a situation demands directed attention, and
that blind-sight dealt with monitoring the navigation of our world, not
evaluating it.

Blind-sight patients do not see half of the world that is in front of
them. The half that is blind is on the side opposite the side where their
brain injury is located: for instance, if the injury is on the right side, they
are blind to the left side of what lies before them. When shown an object
such as a box or a ball, they cannot see the object. But when asked to guess
whether there is an object in front of them and whether the object is a box
or a ball, they guess correctly on about 80% to 90% of the trials. When
this and similar tasks are repeated, and the patients are asked how they
managed to guess so well, they are unable to give an answer.

A patient’s ability to navigate his world unconsciously after a brain
injury is not limited to vision. A more common form occurs after injury to
the right parietal lobe: later in the chapter we will encounter a patient who
no longer felt that her limb was a part of her body, but her limb worked
well while bringing a cup of coffee to her mouth (often to her surprise).

Another group of patients are “mind-blind” in that they cannot
consciously “read” the nonverbal gestures of others, but may nonetheless
communicate well verbally. We have already discussed a patient who had
the opposite disability: she could no longer “read” her own body language;
she stuffed herself but felt no hunger.

All of these instances fit the general concept that we use at least two
types of conscious processes in navigating our world. One type monitors
our navigating, the other assesses the monitoring and directs our attention
to where it is needed.

The Objective Me
A patient’s inability to be consciously aware of how he navigates in

the presence of reasonable navigational skills comes in several categories:
One, of which blind-sight and the lack of body awareness are examples, is
produced by injuries of the posterior parts of the brain. The second kind of
disability follows from injuries to the fronto-limbic parts of the brain.



These injuries can result in loss of feelings of hunger and thirst as well as
of familiarization. But I’ll begin with the story of a most articulate student
in one of my classes who had had a stroke in the right parietal lobe of the
posterior part of her brain.

Mrs. C. provided an excellent example of an inability to call her own
arm into consciousness. She was seated to my right during a weekly
seminar, and I noted that occasionally her left arm and hand would move
in a strange fashion. While pointing to her left arm, I asked Mrs. C., if she
was all right. She replied, “Oh, that’s just Alice; she doesn’t live here
anymore.” Mrs C. wrote her subsequent term paper describing her
experience. Here are some excerpts from this most informative paper:

I was doing laundry about mid-morning when I had a
migraine. I felt a sharp pain in my left temple and my left arm
felt funny. I finished my laundry towards mid-afternoon and
called my neurologist. He told me to go to the emergency room. I
packed a few things and drove about 85 miles to the hospital
where he is on staff (the nearest was 15 minutes away). In the E.
R. the same thing happened again. And again, the next morning
after I was hospitalized, only it was worse. The diagnosis of a
stroke came as a complete surprise to me because I felt fine, and
I didn’t notice anything different about myself. I remember
having no emotional response to the news. I felt annoyed and
more concerned about getting home, because I was in the
process of moving.

Not until several days later while I was in rehabilitation did
I notice strange things happening to me. I was not frightened,
angry or annoyed. I didn’t feel anything– nothing at all.
Fourteen days after I was admitted to the hospital, I became
extremely dizzy, and I felt I was falling out of my wheelchair. The
floor was tilting to my left and the wheelchair was sliding off the
floor. Any stimulus on my left side or repetitive movement with
my left arm caused a disturbance in my relationship with my
environment. For instance, the room would tilt down to the left,
and I felt my wheelchair sliding downhill off the floor, and I was
falling out of my chair. I would become disoriented, could hardly
speak, and my whole being seemed to enter a new dimension.



When my left side was placed next to a wall or away from any
stimuli, this disturbance would gradually disappear. During this
period, the left hand would contract, and the arm would draw up
next to my body. It didn’t feel or look like it belonged to me.
Harrison (my physician) moved the left arm repeatedly with the
same movement, and a similar behavior occurred, except I
started crying. He asked me what was I feeling, and I said anger.
In another test he started giving me a hard time until the same
episode began to occur, and I began to cry. He asked me what I
was feeling, and I said anger. Actually I didn’t feel the anger
inside but in my head when I began to cry. Not until I went back
to school did I become aware of having no internal physical
feelings.

I call that arm Alice—Alice doesn’t live here anymore—the
arm I don’t like. It doesn’t look like my arm and doesn’t feel like
my arm. I think it’s ugly, and I wish it would go away. Whenever
things go wrong, I’ll slap it and say, “Bad Alice” or “It’s Alice’s
fault.” I never know what it’s doing or where it is in space unless
I am looking at it. I can use it, but I never do consciously
because I’m unaware of having a left arm. I don’t neglect my left
side, just Alice. Whatever it does, it does on its own, and most of
the time, I don’t know it’s doing it. I’ll be doing homework and
then I’ll take a sip of coffee. The cup will be empty. I was
drinking coffee using that hand and didn’t know it. Yet I take
classical guitar lessons. I don’t feel the strings or frets. I don’t
know where my fingers are nor what they are doing, but still I
play.

How do I live with an illness I’m not aware of having? How
do I function when I’m not aware that I have deficits? How do I
stay safe when I’m not aware of being in danger?

My student’s egocentric integrity had become disrupted: the form of
her tactile and kinesthetic space-time had changed radically, as evidenced
by her feelings of tilting and falling. Patients who are blind-sighted suffer
from a disruption of their allocentric, that is, other-centered— visual and
auditory—organization. Egocentric and allocentric integrity form a



patient’s mode of conscious awareness within which he regards both the
self and the other as objects.

The Narrative I
The above situation in which Mrs. C’s arm had become an alien Alice

contrasts sharply with the following observations made by Chuck Ahern,
whose PhD thesis I supervised. Ahern based his study on observations
made on an eight-year-old boy while he taught him how to read:

TJ had an agenesis [lack of development] of the corpus callosum [the
large nerve fibers that connect the hemispheres of the brain] with a
midline cyst at birth. During the first six months of his life, two surgical
procedures were carried out to drain the cyst. Recently performed
Magnetic Resonance Imaging (MRI) showed considerable enlargement of
the frontal horns of the lateral ventricle— somewhat more pronounced on
the right. The orbital part of the frontal lobes appeared shrunken as did the
medial surface of the temporal pole.

93. fMRI of Limbic Cortex Lesion

TJ lost the ability to process episodes of experience. When
TJ returned from a trip to the Bahamas he did recall that he had
been on the trip; however, the details he could recount about the
trip numbered fewer than five. His estimates of how long it had
been since his trip were typical in that they were inaccurate and
wildly inconsistent on repeated trials. Also, the first five times
back at tutoring he stated that he had not been at tutoring since
his trip. It appears that he is unable to place in sequence those



few past events that he can recall. Nonetheless, he can answer
questions correctly based on his application of general
knowledge about development, e.g., he knows he was a baby
before he could talk because “everyone starts as a baby.” But,
one day he asked his tutor if he knew him when he was a kid,
indicating, I think, his incomprehension of the duration of each
of these developmental periods and his unawareness of what
events constituted such a period for him.

Furthermore, TJ appears to have no ability for quantifying
the passage of the duration of an experience [what Henri
Bergson (1922–1965) called durée] and no experiential
appreciation of the meaning of units of the duration of an
episode. For example, a few minutes after tutoring begins, he
cannot say—even remotely— how long it has been since the
session started. He is as apt to answer this question in years as
in minutes. He does always use one of seven terms of time
quantification (seconds, minutes, hours, days, weeks, months or
years) when asked to estimate the duration of an episode but
uses them randomly. He can put these terms in order, but does
not have any sense of their meaning or their numerical
relationships to one another.

TJ is aware that he has a past, that events have happened to
him but he cannot recollect those events. He also spontaneously
speaks of events in his future such as driving an automobile and
dating and growing a beard. He has play-acted on separate
occasions his own old age and death. TJ is capable of excitement
about the immediate future. On the very day that he was going to
the Bahamas he was very excited as he exclaimed repeatedly:
“I’m going to the Bahamas.” But when his tutor asked him when,
he said blankly: “I don’t know.” He also displayed keen
anticipation when one day he saw a helicopter preparing to take
off from the hospital. The helicopter engines revved
approximately 13 minutes before it took off and TJ become
increasingly more vocal and motorically active, laughing as he
repeated “When’s it going to take off?” He also anticipates
future punishment when he is “bad.” He is aware, on some level,



of the immediate future in his constant question “what’s next”
which he asks his mother at the end of each activity.

There are a variety of other occasions on which he
demonstrated this capacity regarding tempo (as opposed to
evaluating the duration of an experience.) There have been
several breaks in his usual thrice-weekly tutoring schedule. Each
of four times this schedule has been interrupted, he has run to
meet his tutor when he approached rather than waiting inside as
he usually does. Also, on these occasions he has typically asked
if his tutor missed him. However he states he does not know how
long it has been since his last session, and there was no evidence
that he knew it had been longer than usual.

TJ compares who walks faster or who draws faster. He has
at least a basic sense of sequencing as when he says “I’ll take a
turn and then you take a turn.” He also uses terms like “soon”
and “quick” correctly in conversation. For example, when he
wanted to do a drawing at the beginning of a session, and his
tutor said that we needed to begin to work, he countered “this
will be quick.” Unsurprisingly, he finished his drawing at his
normal pace. He somehow seems to use such terms correctly
without any experiential appreciation of them. (From Chuck
Ahern)

These two case histories illuminate two very important dimensions of
self. One dimension, portrayed by Mrs. C., locates our body’s configural
integrity as an objective “me” in the space-time world we navigate. The
other dimension, highlighted by TJ, narrates meaningful episodes of
circumscribed duration of our experience. Without such narrating, the
events comprising the experience fail to become relevant and evaluated
with respect to an autobiographical self, a narrative “I.”

The locational dimension includes clock-time, what the Greeks called
chronos. Chronos is the “time” Minkowski and Einstein related to space.
Location for a moving organism is always in space-time.

Narrating entails not only our experience of duration but also Robert
Graves’s “proleptic thought,” the decisive moment; what the Greeks called
kairos.



We exist in two very different worlds: a space-time world we
navigate, and a narrative world we inhabit.

(An excellent review of the history of differentiating the objective
“me” from the hermeneutic “I” can be found in Hermans, Kempen and van
Loon’s article called “The Dialogical Self: Beyond Individualism and
Rationalism.” American Psychologist, 1992.)

The Importance of TJ
TJ was of additional interest to me for two reasons: First, despite

having sustained his brain impairment before birth, he could learn to read
and to communicate. Therefore, organization of our “Objective Me” does
not depend on our ability to process the episodes that constitute our
narrative self.

Second, TJ is also of special interest because, in spite of devastating
damage to his medial frontal and temporal lobes which left him with very
little of what we often call the “limbic” system, essentially he has
remained emotionally and motivationally intact. He can express “worries”
and “interests”; his appetites are normal as are his satiety processes.
Hence, Harvard’s Walter Cannon’s theory, based on his experimental
results obtained in the 1920s and 1930s, is most likely correct: the
thalamus and hypothalamus (and even the brain stem) are more essential
to the intensive dimension of emotional experience and expression than
are the limbic parts of our forebrain. The limbic amygdala and the non-
limbic upper basal ganglia, as well as the hippocampal system, modulate
thalamic, hypothalamic and brain stem processes. In turn, the pre-frontal
cortex makes it possible to formulate more precisely these narrative
aspects of our conscious experience.

The Other
The “Narrative I” has a companion in a “Caring Other.” Brain-

imaging techniques and microelectrode recordings of the responses of
single brain cells, have shown that the front parts of the right hemisphere
of the human brain become activated not only when we accomplish an
action, but also when we watch someone else accomplish that action. This
finding has been interpreted to indicate that a brain process is involved in
imitating the actions of others.



Another part of our frontal cortex becomes activated not only when
we experience discomfort, but when we watch someone suffer. Empathy
with another involves an identifiable brain process when we consciously
experience it.

On the basis of my research and that of others, I established for
myself the difference between an “Objective Me” and a “Narrative I.” But
given the finding that our brains are involved when we watch another
perform a task and when we emotionally empathize with another’s
pleasure or pain, this view of two possible “selves” is not enough. The
“Narrative I” depends on stringing episodes together into a story. Otto
Rossler, a pioneer in the creation of the discipline of non-linear dynamics,
has more generally referred to the process of storytelling as pathway
optimization. We tell stories and we use language to tell them. Language,
pathway optimization, is a left hemisphere function in most cultures. But
our right hemisphere aggregates episodes, weaving them together in a
more subtle fashion. Thus, caring, interest, joy and sorrow, and other
intensive dimensions of our experience become conscious.

Just as the “Narrative I” is processed mainly by one hemisphere—the
front part of the left hemisphere—so also does the “Objective Me”—as
exemplified by the brain-injured, “blind-sighted” and “Alice doesn’t live
here any more”—involve mainly one hemisphere, the back part of the
right hemisphere. What does the corresponding part, the back of the left
hemisphere of the brain do? It enables the semantic, dictionary aspects of
language and the concepts upon which languages are based. Concepts,
when shorn of their complexity, finally boil down to our “pointing at”
what is being spoken about. Rossler has called this aspect of the dynamics
of a complex process such as language a “directional optimizer.” This is
the allocentric, or other-centered, aspect of our conscious processing, the
semantic “Other” that complements the egocentric “Me.”

Paradoxically, we experience the “Narrative I” in a mode that
philosophers would refer to as a third-person mode: for example, in
remembering that when I was a six-year-old boy I had set up a weather
station at my school in Switzerland, I tell a story about “that boy” even
though it is “I” myself who happens to be the boy. By contrast, “Imitation”
and the “Caring Other” are experienced in the first person: I personally am
imitating others and share their joy or suffering.



Just as, paradoxically, we experience the egocentric “Objective Me”
in the first-person mode. After Mrs C. had suffered her brain injury, “Alice
didn’t live here any more.” But before the injury, Mrs. C’s arm did live
with her. In another example, it is intimately “my” arm that itches from a
mosquito bite. Furthermore, our allocentric “seeing” is also experienced in
the first-person mode; the blind-sighted person has lost a part of his ability
to subjectively experience, to “see” the world he navigates.

To Summarize
Left hemisphere language-based processes—the Narrative I and the

Semantic Other—become experienced in a third-person mode. Right
hemisphere processes—Imitating, Caring and, paradoxically, the Objective
Me and its Navigable World—become experienced in a first-person mode.
It is our brain cortex that allows our consciousness to become so rich.

Unconscious Processing
A great deal of our behavior is habitual, automatic and unconscious.

In normal individuals, with no brain damage, switching from unconscious
to conscious behavior occurs readily when a situation changes suddenly
and unexpectedly. At other times, when circumstances change more
gradually, we have to exert considerable effort in “tacking,” as a sailor
would put it, to take advantage of changes in the direction of the wind.
What might be the difference in how the brain operates during these two
modes of changing?

Freud considered modes of unconscious and conscious operation in
terms of several “levels:” conscious, preconscious and unconscious,
suggesting that there is a “horizontal” barrier between our conscious,
preconscious and unconscious layers of processing. Freud further proposed
that “repression” operates to push our memories into deeper layers where
they can no longer access awareness. In the Project, Freud treats memories
as memory-motive structures—what today we would call neural
“programs”—located in the basal ganglia, proposing that our awareness is
enabled through the connections of these programs with the cortex. In
Freud’s view, the connections to and from our cortex determine (via
attentional processes) whether a memory-motivated wish comes to our
consciousness and thus becomes available to us for reality testing.



The Hidden Navigator
My interpretation of the relationship between conscious and

unconscious processing is different from Freud’s and was initiated by
observations made on hypnotized subjects. On the basis of experimental
observations made on hypnotized subjects at Stanford, Jack Hilgard
described what he called “a hidden observer.” A hidden observer
participates even when the subject is unaware of what is motivating his
own behavior. It is as if the hypnotized subject knows, in some way, that
what is guiding his behavior is different from his ordinary experience. In
my laboratory, Helen Crawford, professor at the Virginia Technical
University, and I showed that those subjects who demonstrated the hidden
observer phenomenon were those in whom we could not induce an
experience totally away from the experimental setting. However, if the
subject could immerse himself or herself in being on a beach in the
Caribbean with his or her girl- or boyfriend, no hidden observer
accompanied them.

These observations have led me to suggest a modification of the
picture that Freud presented of “levels” where “unconscious wishes” are
suppressed and repressed. My view is inspired not only by the observation
of a hidden observer, but also by my post–World War II approach to
analyzing brain function: from inside out: An alternative model of the
relationship between conscious and unconscious processes, I suggest, is
that of language. A skilled multi-linguist “knows” several languages—but
uses only one in a particular context, a particular situation. He may find
the language inadequate to his purpose and knows that he could express
himself better in another, momentarily hidden tongue, but how to
accomplish this in the current context seems very difficult. As my
colleague Georg von Békésy once remarked to me, German is so much
better in expressing nuances— how frustrating it is to write papers in
English!

Sometimes, when we are in the presence of others who can also speak
several languages, our ability to switch between them comes easily. At
other times, a language, which may have been useful to us at an earlier
age, has been “suppressed.” For instance, my mother’s native language
was Dutch, but she learned English, French and German as a young child.
In Chicago, as I was growing up, she spoke an almost flawless English
when we were out shopping or when she was entertaining guests. At these



times there was no groping for a Dutch phrase or other hesitancy that
would accompany an effort at translation from one language to another.
She was consciously using English, and her other languages had become
“repressed” into unconsciousness. But within our family, in a more
familiar context, she spoke in whatever language best fitted what she
wanted to express, sometimes multilingually in a single sentence.

We learn to process our navigable world, and to communicate, at a
particular age and in a particular “language.” This language may become
inappropriate at another period of our life, but we may still use it, albeit
inappropriately, in certain circumstances much as when a person with
Tourette’s syndrome makes embarrassing statements during “tics” due to
damage to his middle basal ganglion (the globus pallidus.) Psychotherapy
for ordinary patients can help point out when their “hidden navigator” is
impeding a current communication.

As I learned from Piaget, the paths of our cognitive and
emotional/motivational development do have similarities. Language offers
a powerful model for understanding the difference between our current
conscious awareness of how we feel and the patterns of experiencing and
behaving that are no longer used, or useful, but become inappropriately
activated in “familiar” situations. One form of psychotherapy is to access
these “hidden” patterns by attempting to re-establish the familiar
interpersonal context, en famille, that existed when the earlier “language”
modes were used, and to bring them into consciousness in a more current
context. This mode of therapy is called “transference;” most often the
therapist temporarily becomes the patient’s parent “figure.”

How Unconscious Is Unconscious?
The Argentine philosopher Matte Blanco, in his 1975 book The

Unconscious as Infinite Sets, proposed that we should define
consciousness as our ability to make clear distinctions, to identify (that is,
to categorize) alternatives. Making clear distinctions would include being
able to tell personal egocentric from extra-personal allocentric experience;
to distinguish one episode from another in a narrative or an imitation from
a self-generated action; or an empathic from a self-pitying response to a
situation.



By contrast, unconscious processes would, according to Blanco, be
composed of infinite sets “where paradox reigns and opposites merge into
sameness.” When infinities are being computed, the ordinary rules of logic
do not hold. As Lewis Carroll was fond of pointing out, dividing a line of
infinite length results in two lines of infinite length: paradoxically, one =
two. When we are deeply involved with another human being, our capacity
for love and ecstasy is evoked but, paradoxically, so also is our capacity
for suffering and anger to occur.

My interpretation of the conscious-unconscious distinction agrees
with Matte Blanco’s. To bring our well-springs of unconscious behavior
and experience into consciousness means making distinctions,
categorizing and becoming informed—in the sense of reducing
uncertainty, the number of possible alternatives in making choices.

Within the scientific community, clarity regarding the details of how
such distinctions are achieved did not gel until the late 1960s, when
several theorists began to point out the difference between feedback, or
homeostatic processes, on the one hand, and programs—which are
feedfor-ward, homeorhetic processes—on the other.

Feedback mechanisms, though they are dependent upon error
processing, may eventually become ultra-stable. By contrast, feedforward
processes are hierarchical “complex” organizations that can readily be
reprogrammed by changing the way they are chunked. The difference
between feedback and feedforward processing turned out to be the same as
the distinction that had been made by Freud between primary (automatic)
and secondary (voluntary, reality testing) processes.

When these interpretations are considered seriously, an important
change in views becomes necessary: our unconscious processes, as defined
by Matte Blanco, are not completely “submerged” and unavailable to our
conscious experience. Rather, our unconscious processes produce feelings
which are difficult for us to localize in time or in space and difficult for us
to identify correctly. Indeed, our unconscious processes serve to construct
those emotional dispositions and motivational contexts within which we
are then able to construct our extra-personal and personal realities. Our
feelings are to a large extent undifferentiated, and we tend to cognize and
label them within a context: that is, according to the circumstances in
which those feelings become manifested.



It is in this undifferentiated sense that our behavior is unconsciously
motivated. When I burst out in anger, I am certainly aware that I have
done so and of the effects of my anger upon others. I may or may not have
attended the build-up of feeling prior to the blow-up. I may have projected
that build-up onto others or introjected it from them. I might even have
been made aware that all these subliminal processes can occur through the
guidance of a friend or a therapist, and still have found myself intensely
upset. Only when we can clearly separate the events leading to the upset
into alternative or harmoniously related distinctions such as anger, fear,
hate, or a similar bevy of positive feelings, is unconscious control
converted into conscious control.

During the 1930s, Willhelm Reich devised a technique called
“process psychotherapy” by which complex unconscious processes are
brought to consciousness: Reich started by pointing out some of a person’s
nonverbal behaviors such as facial expressions and gestures which the
patient uses routinely and unconsciously. Additionally, Reich called
attention to the effect those facial expressions and gestures have on others.
Gradually the therapy proceeds to verbal language and strips the subject of
other aspects of what Reich called the person’s “character armor.” The
process of stripping is a process of clearly differentiating one expression
or gesture from another—and the effect that such an expression can have
on the experience and behavior of others in one or more clearly
differentiated situations. Once these distinctions are noticed, conscious
experience has supplanted unconscious processing and, as a result, the
interpersonal world the person navigates changes dramatically.

As an example, some years ago, I realized that I rarely made eye
contact during a conversation. I was paying attention to the other person’s
lips, lip-reading in order to enhance my understanding of what the person
was saying. Since then, I have made a conscious effort to make eye contact
during conversation. As a result, people have commented on my attentive
listening and what an interesting conversationalist I am. This despite the
fact that I now understand considerably less of what the other person is
saying! My nonverbal behavior signals to them that they are being
attended, where before, my nonverbal behavior signaled that I was
engrossed in my own process of deciphering a conversation.



In Summary
“Unconscious” processes are actually more diffi-cult to describe than

conscious processes. However, Matte Blanco’s definition is a good one:
consciousness depends on the ability to differentiate; the undifferentiated
compose unconscious processes. This definition fits my proposal that
unconscious processes are like currently unused language systems of
persons who are fluent in several languages. The unused languages are
“stored” in their ultra-deep processing form until activated. But such
hidden languages can still exert an influence on current behavior, such as
the complexity of articulation, or the relevance of the behavior to the
current social context. Psychotherapy addresses maladaptive processing by
bringing to current consciousness these hidden, unconscious language-like
processes.



Chapter 24
Mind and Matter

This chapter is what the rest of the book is all about.

The book of nature is written in the language of mathematics.

—Galileo Galilei, 1623

Mathematics brings together phenomena the most diverse, and
discovers the hidden analogies which unite them . . . it follows
the same course in the study of all phenomena; it interprets them
by the same language, as if to attest the unity and simplicity of
the plan of the universe, and to make still more evident that
unchangeable order which presides over all natural causes.

—Jean Baptiste Joseph Fourier

The Fourier theorem is probably the most fundamental in
physics.

—Richard Feynman, The Feynman Lectures on Physics, 1963



The formulations of Gabor and Fourier provided the formal tools I
needed to comprehend not only the theoretical concepts described in
previous chapters, but also the experimental results we were producing in
my laboratory. My aim from the outset of my research had always been to
explore the relationship of our brain to psychology—that is, to our
perception, our behavior, our thoughts, motivations and emotions.
Initially, my aim had been to eliminate “mentalism”—the idea of a
mystical mental principle— from psychology just as biologists of the 19th
century had eliminated “vitalism”—the idea of a mystical vital principle
—from biology. This aim subsequently led me into a series of unexpected
adventures that proved to be a true odyssey. In the 1950s, I was
temporarily sustained by the storm of behaviorism—a storm that provided
me the skills to steer a clear course between the Scylla of mentalism and
the Charybdis of materialism.

My “growing up” from behaviorism—as Lashley had ironically put it
—began in earnest when, in testing patients in clinical situations, I found
that a patient’s behavior frequently did not reflect what he told me he was
feeling. Thus it was necessary for me to take note of the patient’s “inner
life” in order to obtain a full profile of his psychological life. This
limitation of the behaviorist approach to psychology should not have come
as a surprise: after all, every day, we see all around us people (politicians?)
who say one thing and do another. But, in my work with monkeys, I had
been concentrating on nonverbal behavior, to the exclusion of what verbal
behavior has to tell us.

From Behaviorism to Phenomenology
I reported my observations of the discrepancy between a patient’s

behavior and her verbal expressions in Sigmund Koch’s multivolume
Psychology: A Study of a Science (1962). I described how we could begin
to relate verbal and nonverbal behaviors and what this relationship might
signify for our future study of the brain. I began to explore the
consequences of what I had learned from this disparity in such venues as
my 1973 paper, “Operant Behaviorism: Fad, Fact-ory and Fantasy?” I
made my final step to “adulthood” in my 1979 paper “Behaviorism,
phenomenology and holism in psychology.” In this paper I observed that
now that we had achieved a science of behavior—we needed to develop a



science of psychology. I suggested that such a science of psychology begin
our scientific explorations with the phenomena (including behaviors) we
observe and experience. I also noted that phenomenology lacked formal
structure and that we needed to provide that structure with experimental
results that would of necessity include studies ranging from those on the
brain to those of society. Skinner, who rarely asked anyone for a copy of a
paper, asked me for one. I was pleased.

In a series of papers, as well as in the final chapters of my 1971
Languages of the Brain: Experimental Paradoxes and Principles in
Neuropsychology, I formulated such a structure for a phenomenological
psychology on the basis of the experiments reviewed in that book. My
formulation began, though did not end, with what is called the “dual
aspect:” that is, a dual view of the relationship between brain and mind:
that is, we have mind talk and we have brain talk. But I noted that the
“talk” has to be about some common something—and what this something
might be was not addressed in the dual aspect approach to the mind-brain
relationship. I also had no answer for what this “something” might be, but
I did propose a crucial change: for “talk” I substituted structure; that is,
form as “pattern.” We have patterns that compose psychological processes
and patterns that compose brain processes. This shift meant a shift to a
view in which patterns were embodied in a variety of ways that included
language and culture.

I used music as an example. As I’ve noted in earlier chapters, music
can be expressed in a score on a sheet of paper, in a performance, or on the
roller of a player piano —the great granddaddy of the audiotape. And, as I
also noted, we are in a position to determine the transformations that form
the relationship among patterns, say between a visual score, a visual
process in the brain, a motor performance, an instrument’s “output”
patterns, and those of the auditory sensory and brain process.

Unfortunately, the importance of this distinction between “view” and
“embodiment” has not as yet penetrated the philosophical establishment.
Thus Harald Atmanspacher, the editor-in-chief of the journal Mind and
Matter, introduces the 2007 issue on “Many Faces of Dual Aspects” with
an excellently phrased summary of current views: “Dual-aspect
approaches (or double-aspect approaches) consider mental and material
domains of reality as aspects, or manifestations, of one underlying,
unseparated reality.”



Atmanspacher, a good friend, fails to emphasize or address the
difference between “aspects” and “manifestations,” though he discusses
the importance epistemic and ontic issues. In my presidential address to
the society of Theoretical and Philosophical Psychology, published in 1986
as “The Cognitive Revolution and Mind-Brain Issues” in The American
Psychologist, I had gone a step further in beginning to identify that ontic
reality as “energy.” This was a step in the right direction, but it would take
another fifteen years before I would be able to spell out the details of that
commonality.

The detail of my proposal rests on Gabor wavelets (or similar “quanta
of information”) that characterize both 1) communication, a mental
process, and 2) processing in the receptive fields in the sensory cortex of
our material brains. The Gabor functions (constrained Fourier transforms)
thus implement both communication and brain processes.

Descartes
The story that emerges from my studies is consonant to a

considerable extent with what Descartes had actually proposed, not what
most current philosophers and scientists are interpreting him to have said.
My espousal of Descartes’s proposal is contrary to the current views of
most brain scientists and many philosophers of science: Thought can be
defined technically in terms of Shannon’s measure of information; brain is
made of the matter that physicists have been studying for centuries.

Philosophers and brain scientists have focused on Descartes’s
distinction between mind and matter and have, in many instances, come to
feel that the distinction is, in the words of John Searle, “the worst
catastrophe” ever to hit science and philosophy. (Searle suggests that
“mind” is a secretion of the brain much as bile is a secretion of the liver.
Searle limits his analysis to efficient causation—and I have covered the
restrictions of efficient causality for the mind/ brain issue throughout this
book. Furthermore, liver and bile are both material, whereas brain and
mind are different in kind.)

Descartes was fully aware of the arguments that Searle now uses.
Descartes’s views are illustrated in his correspondence with Princess
Elisabeth of Bohemia where he addresses the issue of the union of mind
and body. In a passage that comes as a reply to the Princess, Descartes



writes: “But it is by means of ordinary life and conversation and in
abstaining from meditation and from studying things that exercise the
imagination, that one learns to conceive the union of soul and body.” (The
Essential Descartes, 1969, emphasis mine)

The Princess’s arguments for a unitary approach to mind and body,
including specifically the operations of the brain, are so lucid and up-to-
date that John Searle could well have written them. In fact, the
October/November 2006 issue of the Journal of Consciousness Studies is
devoted to Galen Strawson’s current interpretations of Descartes and his
correspondence with Princess Elisabeth that are in many respects similar,
but in some other respects rather different from the view that I had
developed and am describing here: Strawson heroically wraps Descartes’s
proposal in current philosophical physicalist and pan-psychic garments. I
have instead chosen to look to science for a deeper understanding.

By contrast to the focus by philosophers on Descartes’s analytic
distinctions, mathematicians have been sustained by Descartes’s
innovative insights into coordination: Cartesian coordinates. The two sides
of Descartes’s thinking are not in conflict: distinctions lead us to
conscious awareness; coordination leads us to meaningful valuation of
those distinctions. What is really new in the results of our research
performed over the last half-century is that we have found that, to some
extent, distinctions between thought and matter are processed by the
posterior regions of the brain while union between thought and matter is
forged by fronto-limbic systems. Descartes had an intuition that attitudes
in the form of emotions and motivations played a special role different
from thinking. In the following quotation note that, for Descartes, passions
help to fortify, preserve and conserve thoughts (in today’s thinking, by way
of reconsolidations of memory).

The utility of all passions consists alone in their fortifying
and perpetuating thoughts which it is good to preserve, and
which without that might easily be effaced . . . And again, all the
harm which they can cause consists in the fact that they fortify
and conserve others on which it is not good to dwell. (René
Descartes, Treatise on the Passions of the Soul, 1649)



I am especially intrigued by Descartes’s indication that the passions
(feelings) fortify thoughts. In a sense, as proposed by Stevan Harnad, the
founding editor of the Journal of Behavioral and Brain Sciences, this
accounts for the perennially unexplained aspect of our sensations that we
call “qualia,” the “feeling” of a color or sound or taste. Also, intriguing is
the idea that “feelings” help to establish and preserve thoughts, an idea
encapsulated by Olga Vinogradova, a student of Eugene Sokolov: “To
establish a memory, an ounce of emotion is worth a pound of repetition.”

My opinion is that, taken as a whole, Descartes’s insights are
supported by the scientific evidence (especially that pertaining to
processing by the frontolimbic forebrain) that has accumulated over the
past two centuries. From the viewpoint of understanding the “particular
go” of the relationship between brain and psychological processing,
Descartes’s contributions provide a most helpful beginning.

Fourier
My story of the development of a solid scientific basis for developing

Descartes’s proposal regarding the mind/brain relationship begins with
Joseph Fourier. I have already discussed Fourier and some of the uses to
which his mathematical formulations have been put during the past two
centuries. Fourier lived at the turn of the 18th to the 19th century. French
mathematics was, at the time, the best in Europe, but the Arabs had a
longer tradition of excellence, which Napoleon wanted the French
Academy to mine, so he put Fourier in charge of the scientific aspects of
the Egyptian military expedition. Fourier returned to France from Egypt
after two years and presented to the Academy what we now call the
Fourier theorem. Fourier had developed a method for making a
transformation from the space-time domain within which we navigate,
into the spectral domain for the purpose of measuring heat—and vice-
versa. Transforming into the spectral domain is the very technique that
Gabor employed to develop his holographic process. In Fourier’s hands,
his method allowed him to measure the spectral properties of radiant
energy such as heat and light. Fourier was immediately elected to the
French Academy on the basis of his invention.

As noted in Chapter 5, within a month, Laplace, the senior
mathematician at the time, showed that Fourier failed to provide a tight



mathematical proof of his theorem. Laplace was not successful in ousting
Fourier from the Academy, but he did manage to prevent Fourier from
publishing in any major mathematical journal for a decade. Fourier
published his work in privately owned journals, and toward the end of this
decade-long siege, received a major award because of the usefulness both
to mathematicians and physicists of his contribution. Laplace finally
began to see the merit of Fourier’s approach and the two eventually
became friends.

Of course, Laplace was right. Even today, there is argument as to
whether there is a really tight mathematical “proof” of Fourier’s theorem.
Nevertheless, his procedure works. And it works in a very fundamental
way, as the Richard Feynman quotation at the beginning of this chapter
indicates. Mathematics is about ideas. The proofs necessary to test these
ideas may come along much later, once new methods of thinking have
been developed.

Once, while I was attending one of David Bohm’s classes, he wrote a
set of some six equations on the blackboard to make a point. I don’t recall
the equations he wrote or even what his point was, but I do remember
Basil Hiley, Bohm’s mathematician-collaborator over the previous
decades, nudging me with his elbow: with a few swear words he muttered,
“Great idea but it’ll take me ten years to provide a proof with additional
equations to fill the gaps between his.“

Waves and Their Interference Patterns
What then was Fourier’s great idea? As developed in Chapter 2,

“heat,” as well as “light,” has ordinarily been thought of as coming in
waves or particles. Fourier provided us with a way to sample a wave: He
shifted the wave onto itself so that the peak of the shifted wave was
superimposed somewhere other than at the peak of the initial wave. This
produced crossings of the waves. A number could now be assigned to the
amplitude (the vertical distance from the halfway point of the wave) of the
crossings. The shift had produced an interference pattern consisting of
reinforcements and cancellations where the wave intersects with itself.
The interference pattern constitutes a spectrum. Calculations of
interactions among spectra could now be made by way of their
coefficients.



Fourier knew that a problem still remained: Just how far should the
wave be shifted onto itself? Fourier’s answer was half a wavelength or 90
degrees. Waves are often designated as sine waves and cosine waves.
Being a non-mathematician, it took me years to try to understand how
waves could be designated in terms of their sines and cosines. Finally it
dawned on me: waves can be represented as a continuous progression over
space, or, as on an analogue clock face, a circular motion over time. (The
clock represents the diurnal wave, the cycle of day and night.) When
considered as a circle, any point on the wave can be triangulated and thus
its sine and cosine stipulated. I was proud of myself for figuring this out. I
was even happier when I read Fourier’s original manuscripts that showed
how he had labored before coming to this insight in much the way that I
had. (Neither of us had thought about the Pythagoreans!)

The Fourier Diagram
Fourier’s theorem allows us to transform between spectra and the

space-time dimensions of the world we navigate. This was to have many
far-reaching practical applications, such as in analyzing EEGs; in image
processing, as in PET scans and fMRI; as well as in statistics, where the
Fast Fourier Transform (FFT) is used to correlate various data sets.

Even more intriguing are some of the insights quantum physicists
have derived from their application of the Fourier Theorem. I have
recently extended their insights to come to an understanding of the
brain/mind issue.

The Fourier diagram provides my summary of these insights. The
diagram is based on a presentation at Berkeley made during the early
1980s by the Berkeley physicist Jeff Chew at a conference sponsored by a
Buddhist enclave. I had known about the Fourier transformation in terms
of its role in the analysis of brain electrical activity and in holography, but
I had never appreciated the Fourier-based fundamental conceptualizations
spawned by concepts in quantum physics, as portrayed in the diagram. I
asked Chew where I might learn more about these relationships, and he
noted that he’d gotten them from his colleague Henry Stapp, who in turn
had obtained his view from Jacques Dirac. As early as 1929, in a paper
about “The Quantum of Action and the Description of Nature,” Niels Bohr
had noted that “the analysis of our sense impressions discloses a



remarkable independence of . . . the concepts of space and time on the one
hand, and the conceptions of energy and momentum . . . on the other.”

I had had monthly meetings with Chew and Stapp for almost a
decade, and I am indebted to them, to David Bohm and Basil Hiley, and to
Eloise Carlton, a mathematician working with me, for guiding me through
the labyrinth of quantum mathematical thinking. As I am not a
mathematician, I had to get over the aversion many of us have to trying to
decipher formulas and equations. I found that in mathematics there is
usually an abbreviated explanation of the ciphers used in the text
preceding and following the equation. This worked well for me when
physicist Kunio Yasue and anesthesiologist Mari Jibu wrote the
mathematical appendices for my 1991 book, Brain and Perception.

The diagram summarizes a description of the relation of brain to
mind in a concise fashion:

Some modern philosophers have recognized the distinctions
portrayed in my Fourier diagram as Cartesian and Heisenbergian “cuts.” In
keeping with the spirit of Cartesian co-ordination, I prefer to characterize
the relationships between the distinctions as dependencies rather than cuts.
As one of my undergraduate students eloquently put it: “The mind-body
relationship is much less of a ‘problem’ than it is a joyous ‘harmony.’”

The Cartesian, up-down relationship in the diagram describes a
dependency between communication and the physics of matter: A material
medium is required for a communication to take place; and the physics of
matter is described in mathematical, mental terms. The Heisenberg left-
right dependency describes uncertainties in precisely specifying
transformations between the spectral and space-time domains.



94.

Thus the diagram has two axes, an up-down and a right-left. Each axis
describes a transformation, a change in coordinates. The left-right
distinction in my Fourier diagram—between a potential world of spectral
flux and the space-time world as we experience it—as noted, is called the
“Heisenberg dependency” because Werner Heisenberg showed that there is
a limit to how accurately we can simultaneously measure the momentum
and the location of a mass. The physics community, which was
accustomed to accurately measuring whatever was of interest, had to
realize that there can be a residual uncertainty in some measurements in
some situations. Gabor showed a similar limit in simultaneously
measuring the pitch aspect of a communication and its duration. But as we
shall shortly see, the diagram shows that we need to seriously consider not
only the reduction of uncertainty measured as information but also the
pre-existing uncertainty upon which the reduction is based. I’ll have more
to say on this important topic in the next chapter.

When Matter Doesn’t Matter
The definitions and measurements of energy and entropy were

formulated during the 19th century when steam engines were invented and
it was essential to be able to measure how much work the heat produced
by an engine could do and how much of that work was dissipated (as by
friction or vaporization). The science of measuring and understanding
energy and entropy is known as “thermodynamics.”



The up-down dependency shown in my Fourier diagram distinguishes
change from inertia. The upper part of the diagram focuses on change as it
occurs in both energy and entropy, defined as follows: Energy is
commonly measured as the numerical amount of actual (kinetic) or
potential work necessary to change a process. Entropy is a numerical
measure of how efficiently that change is brought about.

During the 20th century, with the advent of telephones and computers,
it became important to measure the energy requirements of systems of
communication and computation. As with the steam engine, the issue of
efficiency of operation was considered critical. How much of a message is
lost in communication? How “efficient” is a computer program: that is,
how much computer time and space does it take to make the required
computation?

Claude Shannon, working at the Bell Laboratories in the late1940s,
formulated a numerical answer to these questions in terms of switches:
How many of the switches in a telephone exchange or in a computer’s
hardware are in an “on” position and how many are in an “off” position?
Accordingly, the unit of measurement was made in terms of the number of
“on and/or off” positions and became the “bit” (binary digit).

Surprisingly, Shannon’s equations had the same form as those that had
been used to describe entropy in thermodynamics. Shannon therefore
suggested that the measure of an amount of uncertainty in a
communication is formally the same as the amount of entropy in a
thermodynamic process. Reducing that uncertainty provides the
information in the communication. The amount of reduction of uncertainty
provides the measure of the amount of information that has been
communicated. Shannon’s measure of the reduction of uncertainty was
shown by Gabor to be related to his measure of minimum uncertainty,
which he based on what he called “selective entropy.” Although there have
been arguments about the validity of making comparisons between
thermodynamic and communication theory, here we are discussing the
patterns, the form or formulations of the theories, not their applications.

Shannon’s formulation allows us to define precisely Descartes’s
“cogito,” thought, in terms of communication theory. John Dewey, the
educator and pragmatist, noted in The Natural History of Thinking: “The
man in the street, when asked what he thinks about a certain matter, often
replies that he does not think, he knows. The suggestion is that thinking is



a case of active uncertainty set over against conviction or unquestioning
assurance.” Shannon’s definition of information as the “reduction of
uncertainty” thus provides us with at least one scientific measure of the
thought process.

Note that in these descriptions there is no mention of mass. In
thermodynamics and in communication one might say: Matter doesn’t
matter—or, as we shall see shortly, it matters only as a medium for the
manifestation of energy and the medium for communication.

On the Other Hand, in Physics . . .
Contrast this to the lower part of the Fourier diagram: the inertia of a

mass is defined as its momentum, the force that resists change. When the
comfort of the constant momentum of your car is suddenly altered in a
crash, the potential force of the momentum becomes activated. Just as
energy has a potential and a kinetic form, so does momentum.

Position involves mass: The location of a shape, a mass (such as
yourself) can be mapped onto a system of coordinates. For example, you
are sitting in your car listening to the CD version of this book, happily
unaware of the speed of rotation of the earth around its axis. Despite these
gyrations, you are able to locate yourself with a Global Positioning
System.

Flux and Space-Time
The left-right dependency on my diagram serves to distinguish

between observations made of the spectral domain and those made in
space and time. The spectral domain is characteristically a flux, composed
of oscillations, fluctuations, where interference patterns among waves
intersect to reinforce or cancel. Holograms are examples of spectra, that
is, of a holo-flux.

As described in Chapter 2, flux can be visualized by removing the
lens from a projector showing a slide—and using ordinary eyeglasses to
demonstrate the spread of the “information” in the slide over the whole
illuminated field. Flux describes potential energy and momentum. Since
flux is potential, we cannot measure it—until the energy becomes kinetic.
The potential energy and momentum in a mountain lake are not realized



until its water flows out to form a waterfall. Then its efficiency and mass
of the waterfall can be measured.

Only through their manifestations in space and time do we get to
know of the existence of potentials such as those of energy, of momentum,
and of holographic processes. We can know the potential domain only by
way of realization in space and time—a dependency between the potential
and the world we navigate. Both in physics (Heisenberg) and in
thermodynamics (in the concept of free energy) some uncertainty arises
when this dependency is subjected to precise measurement.

In the communication sciences, a similar relationship has been
established. Gabor called his wavelets “quanta of information.” He related
Shannon’s measure of uncertainty to the Heisenberg formulation of
indeterminacy for the limit of simultaneous measurements of momentum
and mass. Gabor showed such a minimum to exist for communication: the
minimum uncertainty that can characterize a message—that is, the
maximum compressibility that a message can achieve and still be
interpretable. In communication, therefore, as in thermodynamics and in
the physics of matter, the form of the Fourier relation holds.

Communication and Matter
The up-down dependency in the diagram illuminates a different

aspect of our understanding: We have come to realize that physics and
psychology are inter- dependent sciences. To communicate (minding) we
must embody the communication. Likewise, we cannot understand the
physics of matter without recourse to communication, to minding.

To illuminate this important point, let me begin with a story: Once,
Eugene Wigner remarked that in quantum physics we no longer have
observables, only observations. Tongue in cheek, I asked whether that
meant that quantum physics is really psychology? Instead of the gruff
reply that I’d expected, Wigner beamed me a happy smile of
understanding and replied, “Yes, yes, that’s exactly correct.”

If indeed one wants to take a reductive path in science, one ends up
with observations, with psychology, not with observables such as particles
which remain unchanged, that is invariant, over many different ways to
observe them. In fact, it is mathematics, a psychological process, that



describes the relationships that organize matter: current physics is thus
rooted in both matter and mind.

The converse story to the one that describes physics is the situation in
communication. Communication ordinarily occurs by way of a material
medium such as air, water, or wire. Bertrand Russell noted that the form of
a medium is largely irrelevant to the form of the communication that is
mediated by that medium. With respect to the relation among
brain/behavior/experience, the medium is not the message. In terms of
what philosophers and computer scientists call functionalism, it is the
communicated form of a pattern that is of concern, not the shape of the
material that conveys the pattern. The shape of the medium can be as
disparate as that of a cell-phone, a computer, or a brain in a human body.
But the functionalist story is incomplete: not to be ignored is the fact that
communication depends on being embodied in some sort of material
medium and that embodiment demands a particular know-how, particular
sets of transformations to accomplish the embodiment.

One way we may interpret the dependencies between communication
and matter is that mass is an “ex- formation,” an externalized (extruded,
palpable, concentrated) form of flux. In the same vein, communication
(minding) is an “internalized” forming of flux, its “in-formation.”

Bernard d’Espagnat, a theoretical physicist, in his treatise In Search
of Reality summarizes a similar view:

It is thus quite legitimate to perceive in the whole set of
consciousnesses, on the one hand, and in the whole set of
objects, on the other hand, two complementary aspects of reality.
This means that neither one of them exists in itself, but that each
one comes into existence through the other . . . Atoms contribute
to the creation of our eyes, but also our eyes contribute to the
creation of atoms . . . . [I]ndependent reality is situated beyond
the frames of space and time—empirical reality, that of particles
and fields, is, like consciousness [cogito] merely a reflection of
independent reality.

d’Espagnat’s “independent reality” is described by me as a holo-flux,
that is, as “potential reality; his “empirical reality” is characterized in my
approach as “experienced reality.”



Thus, taking Descartes’s total enterprise, the Cartesian distinction
between thought and matter and their union in mathematics and everyday
life, leads, to “complementary embodiments”: ”a joyous harmony” of the
meaning of the mind-brain relationship.

My experiments described in the next chapter show that Shannon’s
“information measurement as the reduction of uncertainty” provides us
only a beginning in our ability to understand the multiple embodiments of
mental processes. The patterns of pre-existing uncertainty need to be
specified and sampled as well. My suggestion is that such sampling can be
used as a measure of meaning.



Chapter 25
Meaning

Brain scientists need to be concerned with meaning, not just information
processing. Herein, I chart an experimental approach to brain processes as
they organize meaning.

One has the vague feeling that information and meaning may
prove to be something like a pair of canonically conjugate
variables in quantum theory, they being subject to some joint
restriction that condemns a person to the sacrifice of the one as
he insists on having the other.

—Claude Shannon and Warren Weaver, 1949



The Meaning of Information
The ability to quantitatively measure “information” was such a

breakthrough in our ability to understand communication and computation
that scientists felt that little else needed to be achieved. But we were
wrong. As we began to test the theory in live organisms instead of artifacts
such as telephones and computers, we found the theory woefully
inadequate. The problem was, and is, that in ordinary communication
between live organisms we have to specify what is meant by the “amount
of uncertainty” which needs to be reduced in order to achieve a measure of
an amount of information. And your uncertainty may not be mine.

Despite this, current neuroscientists continue to speak of
“information processing” in its more popular usage to encompass
“meaning.” This has the disadvantage that it fails to formulate a
neuroscience of meaning in its own right. As noted in the quotation from
Shannon that introduces this chapter, such a formulation may be achieved
by analogy to measurement in quantum physics: measuring meaning is
most likely to be complementary to measuring information as uncertainty
reduction.

A neuroscience of meaning can fruitfully begin, as noted in earlier
chapters, with Charles Peirce’s definition: meaning is what we mean to do
—to which I added that meaning is also what we mean to experience. The
current chapter describes the experiments and their results that lead me
from the theory of information processing to an equally rigorous theory of
meaning.

An Experiment
In the late 1950s, I designed an experiment, using monkeys, to test

information measurement theory. I planned to see which part of the brain
is involved in our ability to choose among alternatives—the number of
alternatives specified by the amount of “Shannon information” in each
choice. My plan was to set up a board that had twelve holes, each large
enough to hold a peanut. I went to the dime store and picked up twelve
junk objects just large enough to cover the holes. I wanted to test the
monkeys’ ability to find a peanut hidden under one of the objects
displayed, given a display of two, three, four, etc. objects from which to



choose. The idea was simple: the more objects (alternatives), the more
trials (or the longer) it would take the monkey to find the peanut.

No one had tried to work with monkeys using such a large number of
simultaneously displayed choices. On preliminary tests, I found that
untrained monkeys simply refused to work with large displays, given so
much “uncertainty,” such a paltry chance of finding a peanut. I had to train
the monkeys by starting with a choice between two objects and working up
to twelve. To keep the monkeys working, I kept the prior set of objects
intact, though in different placements on each trial. Two years of testing
twelve monkeys, four hours each day, in what came to be called the
“multiple-choice experiment,” provided some unexpected results: When
there were fewer than four cues to choose from, the monkeys behaved
differently than they did when there were more than four cues. The cut-off
point at four indicates that animals (and humans) can immediately tell
which alternatives are to be considered. This ability is called “subatizing.”
With more than four alternatives, a search becomes necessary.

Rather than searching randomly—as would have been ideal for my
experiment if I had been able to vary the number of cues that I presented
—the monkeys learned to search for the peanut through the array of
objects I had used on previous trials. Thus, for them, the ability to
remember which objects had been most recently rewarded was the issue—
not a choice among the several, up to twelve, cues. For the monkeys, the
problem had become something very different from the one that I had set
out to test.

95. Figuress 1. and 2. Diagram of the multiple object problem showing examples of the three and
the seven object situations. Food wells are indicated by dashed circles, each of which is assigned
a number. The placement of each object over a food well was shifted from trial to trial according
to a random number table. A record was kept of the object moved by the monkey on each trial,



only one move being allowed per trial. Trials were separated by lowering an opaque screen to
hide, from the monkey, the objects as they were repositioned. (From Pribram, “On the Neurology

of Thinking” Behavioral Science)

This experiment was set up to examine the effects of restricted
removals of different areas of the brain cortex on information-processing
ability when I made restricted removals of different areas of the brain
cortex. I used four monkeys for each area removed and found that removal
of one specific brain area, the inferior temporal cortex, and no others,
changed the way the monkeys searched for the peanut. I was puzzled by
the result: The control monkeys took progressively more search trials as
the experiment proceeded—but not in the way information measurement
theory had predicted. Even more puzzling, the monkeys with removals of
the inferior temporal cortex actually did better than the unoperated and
operated control monkeys during the early parts of the experiment, a result
which was opposite to any that I or anyone else had found before. As is my
custom when I can’t understand an experimental result, I presented these
findings (along with others that I did understand) in talks given on various
occasions—and asked the audience whether anyone did have an
explanation.

An Explanation
On one of these occasions, at Dartmouth, a young professor, Bert

Greene, made a suggestion which could be tested by reanalyzing the data:
Greene predicted that the animals with the brain operations differed in the
way they sampled the displayed cues. (Sampling meant that the monkeys
moved the cue to see whether there was a peanut in that well.) His
prediction was correct. Whereas normal monkeys tended to return to cues
that had been



96. Graph of the average of the number of repetitive errors made in the multiple-object
experiment during those search trials in each situation when the additional, i.e., the novel, cue is
first added. rewarded previously, the monkeys with the brain operations sampled the displayed

cues randomly. The brain surgery had removed a memory process that the control monkeys used
in making their choices.

I was able to show that mathematical sampling theory described
quantitatively what was happening. Sampling theory predicted the change
in behavior at the four-cue point in my experiment and fit the data
obtained throughout. I had started to test information measurement theory
and ended up testing mathematical sampling theory instead!

From this “multiple-choice experiment” I learned something that
other experimental psychologists were also learning at the time: if we are
to measure Information in terms of the reduction of uncertainty, we must
know the subject’s state of uncertainty. My monkeys responded to the
alternatives, the available choices presented by the display, not as a
straightforward series from 2 to 12 but as an array to be sampled in which
previous responses were remembered. As Ross Ashby noted, we learned
that information measurement theory was a superb instrument for framing
issues but not helpful when the subject in an experiment was working
within a context, the measure of uncertainty not accessible to the
experimenter.

What Is Being Sampled
In another, somewhat simpler experiment, I taught two groups of

monkeys (four in each group) to choose one of two objects: a tobacco tin
and an ashtray. One group of monkeys had received a removal of the



inferior temporal cortex in both hemispheres of their brains; the other
group served as control subjects who had not been operated upon.
Monkeys with the cortical removals took somewhat longer to learn to
make such a choice—for example, to choose the

97. Graph of the average of the percent of the total number of objects (cues) that are samples by
each of the groups in each of the situations. To sample, a monkey had to move an object until the

content or lack of content of the food well was clearly visible to the experimenter. As was
predicted (see text), during the first half of the experiment the curve representing the sampling

ratio of the posteriorly lesioned group differs significantly from the others at the .024 level
(according to the non-parametric Mann-Whitney U procedure, [Mann & Whitney, 1947].)

(From Pribram, “On the Neurology of Thinking’” Behavioral Science) ashtray—when compared
to the number of trials it takes normal animals to learn to make that choice.

After the monkeys had learned to make the choice, I changed the
situation in which the choice had to be made. Now, I placed either the
ashtray or the tobacco tin in a central place between two wells covered
with identical lids. The task for the monkeys was to find the peanut: the
peanut was always in the well on their right in the presence of an ashtray,
and in the well on their left when a tobacco tin was present. This was a
difficult task for a normal group of monkeys to learn—it took them around
500 trials to do so. But the monkeys who had the cortex of the inferior
temporal lobe removed failed to learn to make the correct choice in this
task in several thousand trials.

To assure myself that the monkeys who were failing were still able to
“tell the difference” between the ashtray and the tobacco tin, I inserted ten



trials of the original task where both ashtray and tobacco tin were present
during the opportunity for choice. Invariably, all monkeys made the choice
that they had learned earlier on all ten trials. The failure on the new and
difficult task was not in perceiving a difference between the stimuli but in
navigating the world of the new, more complex situation—the context—in
which the choice had to be made. The lesson for me was, once again, it’s
not the sensory information that is being processed but the meaning that is
given to that information by the context in which it is being processed.

The DADTA
Rewarding as the result of the multiple-choice experiment was, I

realized that no one in his or her right mind (including me) would ever
undertake to repeat such a multi-year-long, drawn-out, tedious testing
procedure. Laboratory-friendly computers were just coming on the scene
in 1958 as I was moving to Stanford, so I resolved to computerize my
laboratory at Stanford. I designed and had the Stanford Research Institute
build a special-purpose device that was based on telephone-switchboard-
controlled automated behavioral testing equipment. For the first apparatus,
peanuts were placed in a vertically mounted roulette wheel. The
mechanical driver of the wheel made such a racket that it scared not just
the monkeys who were to test in it but also the entire laboratory.

Nonetheless, I demonstrated the device at the convention of the
American Psychological Association that year with the help of the single
adult male macaque monkey who was not frightened by the noise. The
demonstration was a complete success; it attracted a large crowd, and my
stalwart simian subject and I got a kick out of watching the reactions of
the crowd.

Returning to Stanford, I arranged to have a more civilized apparatus
built, again by the Stanford Research Institute, and named it DADTA, an
acronym for Discrimination Apparatus for Discrete Trial Analysis. The
original special-purpose machine was soon replaced with a general-
purpose computer whose specifications I helped set up—I received the
third and seventeenth ever built. Within a decade, my students and I were
training 100 monkeys a day on three DADTA in a totally computerized
laboratory: the task displays and reinforcing contingencies were
programmed and the monkeys’ responses were recorded by the small



general-purpose computers which, by then, we also used to study brain
electrical activity recorded with microelectrodes.

Monkeys Can’t Think?
I wrote up my results under the title “On the Neurology of Thinking.”

The history of its route to publication is one of those sagas that keep
young investigators from innovating. I had been asked in 1958 to write a
“lead article” for publication by the journal Science and felt the findings in
my multiple-choice experiment to be sufficiently novel to be worthy of
this prestigious publication.

98. Brain and Perception

In the introduction I noted that the paper was about only one kind of
thinking, that is, problem solving. The paper presented the data (including
the mathematics in a footnote) and noted that there were two aspects to
problem solving: one, the amount of information, that is the amount of
uncertainty reduction; and two, the specification of uncertainty, the
context within which the reduction was taking place.

Despite the fact that I had been invited to write the paper, it was
summarily rejected. Reasons: “Everyone knows that monkeys can’t think;
the paper does not reflect established views.” My response: “I had stated
that I had limited my paper to that aspect of thinking known as problem
solving. The data showed that the monkeys were capable of problem
solving. The monkeys had demonstrated active uncertainty by tentative



responses—scratching their ears and stroking their chins and by the longer
time it took them to respond when there were more cues to choose from.” I
had started the paper with a quotation (noted in the previous chapter) from
the pragmatist, John Dewey, who wrote in The Natural History of
Thinking:

The man in the street, when asked what he thinks about a certain
matter, often replies that he doesn’t think at all; he knows. The
suggestion is that thinking is a case of active uncertainty set
over against conviction or unquestioning assurance.

It took two more decades before the “established views” reflected
what is common knowledge: the fact that animals do problem-solve and
show signs of being thoughtful while doing so. Have you ever tried to
place a bird feeder in a place inaccessible to squirrels?

In 1959, I sent the paper, “On the Neurology of Thinking” to a
relatively new journal, Behavioral Science. The editor and reviewers were
laudatory and the paper was accepted for publication within a few weeks.

Sensory Specificity in the “Association” Cortex
The results of the “multiple-choice experiment” fit with others that I

obtained in a long series of studies in which I removed areas of cortex
from the back of the monkeys’ brains. These areas were at that time called
“silent areas” because brain electrical stimulation had no effect, and
surgical removals in behavioral studies had not resulted in any consistent
changes in behavior. This cortex is also called the “association cortex,” so
named, in the latter part of the 19th century, by Viennese neurologists
(especially Flechsig and Meinert, teachers of Freud) who were
philosophically aligned with “British associationism.” These neurologists
suggested that the cortex that lies between the primary cortical sensory
receiving areas serves to combine, to “associate” visual, auditory and
tactile sensations with one another to produce higher-order cognitions.
They thus named this reach of cortex the “association cortex.” If it has a
name, it must exist as such!

Contrary to the accepted view at the time, research conducted in my
laboratory consistently showed that the silent cortex included systems that
are sensory specific. That is, surgical removal restricted to one or another



part of this cortex influences behavior in one sensory modality only; and
further, that higher-order processing in general includes several steps that
involved only one sense at a time. For instance, these experiments showed
that making choices among visually presented objects is altered when, and
only when, an area of cortex in the temporal lobe is removed. When an
area in the parietal lobe is removed, the perceptual difficulty is limited to
reaching. Another part of the temporal lobe is involved in making auditory
choices; and farther forward is the system that is involved in making
choices among tastes.

99. The upper diagrams represent the sum of the areas of resection of 15 subjects showing a
decrement in the performance of a pre-operatively learned visual discrimination task. The middle
diagrams represent the sums of the areas of resection of 25 subjects showing no such decrement.
The lower diagrams represent the intercept of the area shown in black in the upper and that not
checkerboarded in the middle diagram. This intersect represents the area invariably implicated

in visual choice behavior by these experiments. (From: Pribram, “Neocortical Function in
Behavior. Invited” Address to the Eastern Psychological Association, 1955.)

Such sensory-specific higher-order effects had been known to be
present in humans who have suffered damage in these brain areas. But it
was commonly thought that the specificity was due to intrusion of the
damage into adjacent primary sensory systems. My experiments were
designed to avoid such intrusions and showed them not to be responsible



for the sensory specificity. After many publications over many decades,
these results have become accepted as common knowledge.

In addition to these publications were some contributed by Mortimer
Mishkin (a former doctoral and postdoctoral student) and Leslie
Ungerleiter who, as a postdoctoral student, had published with me (as
discussed in Chapter 6 on objects and images) the results of the study that
showed size constancy to be impaired by resections of the peri-striate
cortex in monkeys. Their influential publications developed the theme that
visual processing became divided into two pathways, one through the
parietal lobe of the brain that computed the location of an object (its
“where”) and the other through the temporal lobe that specified the nature
of the object (its ”what”). Further research showed that the parietal
pathway is composed of a short ventral path that processes where an object
is located as well as a more extended dorsal branch that processes how an
object comes to be used.

I expressed (and published) several caveats that need to be kept in
mind: 1) Ungerleiter’s and my studies, as well as those of others, indicated
that the prestriate cortex was especially involved in object constancy—the
fact that objects are objects irrespective of the perspective from which
they are perceived; 2) that the entire posterior parietal and temporal cortex
are involved in the processing of spatial relations, as shown by Charlie
Gross; and 3) the pathways are not cortico-cortical pathways since
carefully controlled extensive removals of the cortex lying between the
primary visual cortex and the parietal and temporal cortex did not alter the
functions of the pathways. Rather, the connections dip down through the
thalamus and basal ganglia and back up again in various locations.

Once these “pathway” studies were published, a virtual cottage
industry developed to study the anatomy, physiology and behavioral
relationships of visual processing forward of the primary visual receiving
cortex. The results of this outpouring of results are elegantly reviewed and
summarized (in The Tell-Tale Brain: A Neuroscientist’s Quest for What
Makes Us Human)—as is their relevance to cases of human brain
pathologies. And most important: the author V. S. Ramachandran
emphasizes and shows how these parts of the brain deal with meaning.

Meaning depends on another caveat that these studies do not take into
account. I pointed out to various contributors to my 1974 section of the
Neurosciences Third Study Program meetings entitled “Central Processing



of Sensory Input” that all these “pathways” are reciprocal; that is, there is
as much “traffic” going from the higher-order parietal and temporal lobes
(and most densely from the peristriate cortex) to the primary visual cortex
as there is emanating from that cortex. It is these centrifugal influences
from higher-order systems onto the primary sensory and motor systems
that are responsible for generating meaning.

A corollary to this insight is that, as I teach my students, we live in
our sensory-motor systems. It is in these systems that the input from the
world we navigate and that from our central brain processes comingle. We
as often introject as project the result, especially in complex situations.

It is the job of psychotherapists to “locate” what is going on.

Two Meanings of Meaning
Further, as shown in humans by Martha Wilson, then professor of

psychology at the University of Connecticut, who had been one of my
students, comprehension of meaning is of two sorts: 1) choosing on the
basis of the use an object can be put to, which is related to the functions of
the left hemisphere of the brain; and 2) choosing, based on the physical
(perceptual) attributes of an object, which is related to the functions of the
right hemisphere.

Alex Martin has made a comprehensive review (Annual Review of
Psychology, 2007) of his own findings and those of others using the fMRI
technique of imaging the activity of ongoing brain processes in humans.
He has fully confirmed not only most of what is in this chapter but also
Martha Wilson’s localization of the two sorts of comprehending.

The two ways of comprehending reflect the two ways of attaining
meaning, noted earlier: I mean by “meaning” what I mean to do and what I
mean to experience. In both senses, meaning entails choice.

Comprehension of Meaning
What these studies showed is that not only the perception of objects,

but also comprehending their meaning is processed by sensory-specific
systems of the brain. But comprehension can be described by information
measurement theory only if the context (the uncertainty) in which the
description occurs is specified. The meaning of the information depends



on clearly describing a context (the uncertainty) and the sampling, an
active selecting, of forms within that context.

In an excellent interchange with the students of my 2008 autumn
semester class at Georgetown University, something became clarified that
I had not fully comprehended. Comprehension, as it depends on an active
selecting of forms within the context (the uncertainty) of information
processing, is dependent on “attitude.” Attitudes are formed by processing
novelty. Novelty deals with arrangements of patterns that can be described
in terms of the structure of redundancy. Thus, the context of information
processing is described in terms of the non-linear dynamics of complexity.

Information processing theory deals with the amount of information,
the number of distinctions processed by the immediate brain systems
involved with the input from the world we navigate; complexity theory
deals with the choices, initiated by our world within, that we use when we
navigate our world.

A summary of the results of these experiments can be stated
succinctly: The processes that occur by virtue of the brain systems beyond
the primary sensory and motor systems are best described by sampling
theory, that is, processing choices among the physical attributes of a
perceived object or the use to which that object can be put. These choices
are guided by the attitudes we bring to the way we navigate our world.

The time has arrived to acknowledge “meaning” and its basis in
active choice among perceptual and useful forms as a precisely definable
concept in the psychological and brain sciences. Such a science of
meaning would provide a venue for a rapprochement with the humanities
thus bridging the now existing gap between these “two worlds” of inquiry.



Appendix A
Minding Quanta and Cosmology

Karl H. Pribram, MD, PhD (Hon. Multi.)
  

Published in Zygon; Journal of Religion and Science
 Vol. 44 No. 2, June 2009 pages 451–466

Physical concepts are free creations of the human mind and are
not, however it may seem, uniquely determined by the external
world.

—Albert Einstein and Leopold Infeld, 1938



Minding Quanta and Cosmology
The revolution in science inaugurated by quantum physics made us

aware of the role of observation in the construction of data. Eugene
Wigner remarked that in quantum physics we no longer have observables
(invariants) but only observations. David Bohm pointed out that, were we
to look at the cosmos without the lenses of our telescopes, we would see a
hologram. I have extended Bohm’s insight to the lens in the optics of the
eye. The receptor processes of the ear and skin work in a similar fashion.
Without these lenses and lens-like operations all of our perceptions would
be entangled as in a hologram. Furthermore, the retina absorbs quanta of
radiation so that quantum physics uses the very perceptions that become
formed by it. In turn, higher-order brain systems send signals to the
sensory receptors so that what we perceive is often as much a result of
earlier rather than just immediate experience. This influence from “inside-
out” becomes especially relevant to our interpretation of how we
experience the contents and bounds of cosmology that come to us by way
of radiation.

Introduction
The revolution in science inaugurated by quantum physics made us

aware, as never before, of taking into consideration the role of observation
and measurement in the construction of data. A personal experience
illuminates the extent of this revolution. Eugene Wigner remarked that in
quantum physics we no longer have observables (invariants) but only
observations. Tongue in cheek, I asked whether that meant that quantum
physics is really psychology, expecting a gruff reply to my sassiness.
Instead, Wigner beamed a happy smile of understanding and replied: “Yes,
yes, that’s exactly correct.” In a sense, therefore, if one takes the reductive
path in science one ends up with psychology, not particles of matter.

Another clue to this “turning reductive science on its head” is the fact
that theoretical physics is, in some non-trivial sense, a set of esthetically
beautiful mathematical formulations that are looking for confirmation (see
George Chapline 1999, “Is theoretical physics the same as
mathematics?”).

At a somewhat more conservative level, Henry Stapp (1997/1972:
The Copenhagen Interpretation) has eloquently reviewed the history of



how the founders of quantum physics (e.g., Niels Bohr, Werner
Heisenberg, John von Neuman,) dealt with the then newly realized
importance of the “how” of our observations to an understanding of the
composition of matter. Stapp has added his own views on how these
innovations in thinking affect our understanding of the mind/ matter
interface.

Here I will pursue a different take on the issue: coming from brain
science, how can we better understand some of the puzzles that have
plagued quantum theory and observation to the point of “weirdness”? And
furthermore, how important are the prejudices of our Zeitgeist in
interpreting our cosmological views? My hope is that by pursuing the
course outlined here, weirdness and prejudice will, to a large extent,
become resolved.

Observing Quanta
To begin: David Bohm (1973) pointed out that were we to look at the

cosmos without the lenses of our telescopes, we would see a hologram.
Holograms were the mathematical invention of Dennis Gabor (1948), who
developed them in order to increase the resolving power of electron
microscopy. Emmet Leith (Leith and Upaticks 1965) developed the
hologram for laser light photography, a development that has, in
popularity, overshadowed the mathematical origin of the invention.
Holography is based on taking a space-time image and spreading it (the
transformation rule is called a spread function; the Fourier transformation
is the one used by Gabor) over the extent of the recording medium. Thus,
the parts of the image have become wholly enfolded with each other and
the whole has become totally enfolded in each part.



100. Eugene Wigner’s 80th Birthday

I have extended Bohm’s insight of the importance of lenses in
creating a space-time image to the lens in the optics of the eye (Pribram
1991). In fact, the receptor mechanisms of the ear, the skin and probably
even the nose and tongue work in a similar fashion. Without these lenses
and lens-like operations all of our perceptions would be enfolded as in a
hologram.

In optics, a very small aperture of the pupil produces the same
transformation as does a lens. When the pupil has been chemically dilated
as during an eye examination, focus is lost and the experienced vision
becomes blurred. However, if a pinhole or slit in a piece of cardboard is
placed in front of the dilated eye, ordinary vision is restored. One can
accomplish this in a crude fashion if one needs to read some directions by
tightly curling one’s index finger producing a slit. Also, in experiments
during which we map the receptive fields of cells in the brain, we drift
dots or slit-like lines and edges in front of a stationary eye. In my
laboratory we used dots, single lines, double lines and gratings and found
differences in the recorded receptive fields when more than one dot or line
was used. The differences were due to interactions produced in the visual
system of the brain when the stimulating dots or lines moved together
against a random background.

What I’m proposing is that the difference in the observation of
interference effects (an enfolded holographic record) in the two-slit
experiment vs. the observation of a granularity (particles) in the single-slit
experiment is due to the difference in the measurement apparatus. This, of
course, is not a new proposal: it is the essence of the argument made



initially by Bohr and accepted by quantum physicists for almost a century.
What I am adding is that the measuring apparatus, the slits, are mimicking
the biology of how we ordinarily observe the world we live in. There is no
weird quantum effect unique to that scale of observation.

101.

The Brain’s Role in the Making of Observations
In turn, the observations made in quantum physics are relevant to

how we perceive our world. The retina of the eye has been shown to absorb
a single quantum of photic energy: that is, the retina has a resolving power
such that it consists of pixels of single quantum dimension. Yakir
Aharonov has developed an experimental paradigm for quantum physics
that he calls “weak measurement.” Weak measurement does not disturb
what is being observed. Essentially, the technique consists of repeated
measurements composed of two vectors, a “history” vector determined by
past events and a “destiny” vector determined by events that occur in the
future of the time any single weak measurement is obtained. I noted that
this apparently startling procedure is similar to the one used in non-linear
dynamics (complexity theory) that traces the vectors that develop what
have been called “attractors” over repeated observations of paths toward
stabilities far from equilibrium. Point attractors and periodic attractors are
two simple examples of such stabilities.

Research in my laboratory established functional pathways that
connect higher-order cortical systems to the retina. Eight percent of the



fibers in the optic nerve are efferent to the retina, and these fibers are able
to change retinal processing about 20% of the time. The control of the
retina occurs within the time that retinal processing of optical input
occurs.

Thus, whenever there is a repetition of a sequence of optic inputs, a
second vector “anticipating” that input is operative. Just as in quantum
physics, “attractors,” contextual futures determine our visual perceptions
—and what is true of vision has also been shown to be true for hearing,
tactile and kinesthetic perceptions and the perception of flavors.

The laws of physics, especially the laws of quantum physics,
apparently have their complement in the laws of human perception: The
laws of quantum physics have been shown to be dependent on the
constraints imposed by the instruments of observation. The laws of human
perception have been shown to be dependent on the constraints imposed by
processes such as attention, intention and thought organized by the
observer’s brain. To complete the hermeneutic cycle, observations in
physics are made by humans whose observations are dependent on their
brains.

Meaning
Patrick Heelan discusses at length the transition of scientific,

philosophical and religious thought from a separation of our perceptions
from an “out there,” to an interpenetrating, intentional, view of a
meaningful reality. Heelan indicates that this transition comes by way of
the hermeneutic process that stems from individual encounters in the
world we navigate. This view is considerably more sophisticated than the
currently accepted way of describing the organization of brain function
and of communication in terms of “information processing.”

The popularity of “information processing” has two sources. One
source is that when speaking of “information” most people mean
meaningful information. The other source comes from communication
theory and its use in telephony and computer science. Claude Shannon
defined “information” as the “reduction of uncertainty” and sharply
separated this definition from the definition of “meaning.” The usefulness
of Shannon’s definition has given “information” an aura of respectability
that has been assimilated by the undefined use of the term “information



processing.” Actually, the more appropriate term would be the “processing
of meaning” but then we would need a scientifically useful, that is
testable, definition of “meaning.”

A good beginning can be made with Charles Saunders Peirce’s
definition: “What I mean by meaning is what I mean to do.” Coming from
one of the founders of pragmatism this is hardly surprising. I would add:
“What I mean by meaning is what I intend to do and what I intend to
experience.”

These are good beginnings but do not provide us with the useful
laboratory testable procedures that make the concept “meaning” as
transparent as Shannon’s (and Gabor’s) concept of “information.” In order
to provide such a transparent concept we need to take a detour to define a
context for Shannon’s definition of “information” and then show the
shortcomings of Shannon’s definition for human (and primate)
communication. Finally, we need to describe an experimental result that
provides at least one definition of “meaning.”

This detour is relevant to our interpretation of quanta and cosmology.
For decades, quantum physicists were divided as to the best representation
of quantum phenomena. Schrödinger, DeBroglie and Einstein opted for the
wave equation while Heisenberg, Bohr and his Copenhagen adherents
opted for a vector representation of quantum “reality.” I recently published
a paper in which the results of micro-electrode analysis of brain processes
was shown both in terms of wave functions and vectors. In that paper I
recapitulated the quantum physicists’ arguments: the wave representation
is more “physical” more “anschaulich”; the vector representation is more
abstract and therefore can be more easily applied over a range of
experimental results. What I am proposing is a way of conceptualizing the
brain/ mind relationship (or better stated, the person/experience
relationship) that is derived from, and in turn, motivates our understanding
of quantum physics.

The Holographic Process
The precision of our understanding is today best formulated in

mathematical concepts. The root problem in coming to grips with the
person/experience relationship, the brain/mind transaction, is that at first
blush, brain is material, matter, while what we experience is different. We



can eat brains but not our experience. The current way scientists deal with
experience is in terms of communication and computation, in terms of
information processing. But any more human approach to the issue finds
“information processing” barren. Additionally, as noted, the manner in
which scientists use “information processing” is itself largely unscientific.

These limitations of understanding brain and mind, person and
experience, need not be so. Careful attention to what philosophers have
had to offer since René Descartes, what the science of radiation (heat and
light) has shown, what psychologists and communication sciences have
developed, can provide a transparent set of concepts that go a long way
toward “resolving” this apparently intractable problem.

The formation of attractors during our experiencing the world we
navigate (and in performing experiments) is a complex dynamic process.
In order to examine aspects of this process in detail, sections (Poincaré
sections), slices, can be taken at any “moment” to display this complexity.
One such momentary display is the holographic experience.

Experiencing a holographic process at the macroscopic scale is just as
weird as any observation made in quantum physics: My classroom
demonstration always evokes disbelief. I take an ordinary slide projector
and show a slide (for example, a pastoral scene). I then take the lens of the
projector away, and as predicted by Bohm, all one sees is a fuzzy cone of
light containing no discernable image. Then I take a pair of reading
glasses and hold it in front of the projector at just the right distance. Voilà!
Wherever and whenever the lenses focus the light, the image on the slide
(the pastoral scene) appears. Taking two pairs of spectacles, one can
demonstrate four images—and continue to show images anywhere there is
light. In experiments performed in quantum physics, a pinhole or single
slit is the equivalent of the lens in the classroom experiment. At the
quantum scale, replace the pastoral scene with a particle. The ”particle’s”
holographic form (its complex conjugate) becomes exposed by means of
double or multiple slits (gratings). The “scenic” particle is now spread
everywhen and everywhere.

This holographic form of holism is not to be confused with the
hierarchical form in which the whole is greater than and different from the
part. Hierarchical relations are found everywhere in biology and in the
behavioral sciences. The holographic form of holism has come into
science fairly recently. The spectral aspects of quantum physics and the



procedures used in functional Magnetic Resonance Imaging (fMRI) and in
digital cameras are examples. However, in optics, interference effects
have been studied since Huygens, though their importance to our
understanding of brain and cosmos had to await the insights of the 20th
century.

The Brain’s Role in the Making of Theories
Brain science can contribute even more to our understanding of

quantum theory. Two observations are relevant:

1. The procedure of working from theory to experiment is what minding
quanta and cosmology is all about. Our brain is central to this
endeavor. Rodolfo Llinás in I of the Vortex (2001) develops the theme
that the whole purpose of having a brain is to anticipate a useful
choice on the basis of past experience—the essence of a well-
developed theory.

2. Brain dynamics allows conscious experiences (musings) to be
momentarily superfluous to making choices; due to this delay these
experiences can become esthetically elegant. Einstein’s oft-quoted
remark that theory must first be beautiful to be true (before its full
value can be experimentally fulfilled) is a case in point.

Henry Stapp (1997) encapsulates these two observations: “ . . .
body/brain processes generate possibilities that correspond to possible
experiences, and then [as we navigate our world] nature selects, in
accordance with the basic quantum statistical rule, one of these possible
experiences, and actualizes it, and its body/brain counterpart— this means
that our experiences are not only the basic realities of the theory and the
link to science . . . but also [that they] play a key role in specifying the ‘set
of allowable possibilities’ that . . . [compose] mind/brain events.” (Recall
the correspondence between statistical, used by Stapp, and spectral
representations to bring his comments into register with this essay).

Quantum Weirdness
The conceptualizations that have characterized quantum physics for

almost a century have struck scientists as bizarre and weird. When taken



within the framework of “Minding Quanta” as detailed in this essay, the
weirdness can be dispelled to a large extent. First, the hologram,
embodying the spectral domain at the classical scale, is just as weird as is
the entanglement observed at the quantum scale. (Probability amplitudes
remain specific to the quantum scale but are currently under attack by
Basil Hiley in an extension of David Bohm’s approach to quantum
phenomena.)

Second, because quantum phenomena are expressed in terms of a
Hilbert space defined by both spectral and space-time coordinates, verbal
interpretation often seesaws between these axes. Language has a tendency
to reify, make “things” out of processes. This can be useful as in such
disciplines as biochemistry when the juice squeezed out of the pituitary
gland has effects on most of the other endocrine glands: The juice is
assumed to be composed of a multiplicity of substances, things, each
having a specific target in one of the other glands. And indeed this is what
was found.

But reification can have drawbacks when the labels used to “thingify”
do not properly correspond to the process being labeled. My first
encounter with this issue was when we recorded a direct sensory input
from the sciatic nerve to the “motor” cortex of the brain. According to the
dictum of separation of input from output as in the reflex arc of the spinal
cord (known as the Law of Bell and Magendie), the “motor cortex” should
have no direct sensory input. I called two of the most active and respected
scientists working in the field who replied, “Yes, we’ve seen this strange
artifact over and over.” But it wasn’t an artifact as my students and I
showed. I removed all possible indirect sensory inputs (post central cortex
and cerebellar hemispheres) without disrupting the response evoked by the
sciatic stimulation. The designation “motor” had misled and the
reification of the Law of Bell and Magendie turned out to be erroneous
even at the spinal reflex level. (The nervous system works much more like
a thermostat with a control wheel to change a setting as developed in
Miller, Galanter and Pribram 1960, Pribram 1971.)

When an enfolded system with space-time constraints, a Hilbert
space, is being investigated, the temptation is overwhelming to reify the
process in terms of the space and time constraints within which we
ordinarily navigate. Take for instance the excellent book by George Green-
stein and Arthur Zajonc entitled The Quantum Challenge. They describe



what are considered to be bizarre quantum phenomena: 1) a particle can
pass through two slits at the same time; 2) measurements can never be
perfectly accurate but are beset by a fundamental uncertainty; and 3) the
very concept of cause and effect must be rethought.

Their first chapter tackles the two-slit issue. The authors carefully
describe matter waves and Louis DeBroglie’s description of a quantum
particle in terms of wave forms: “the quantum treatment deals primarily
with waves rather than particles. Indeed the very word ‘particle’ plays
little part in the discussion. The concept comes in only when psi is used as
a measure to discern the probability of finding ‘the particle’ at a given
point in space.” As noted above, the “wave” and statistical description are
to a large extent interchangeable. Here the “particle” is not a thing, not an
“it” but a statistical possibility that can occur in two spatially separated
slits at the same time. Equally important the “wave” in the above
quotation is really not a wave which occurs in space-time but a spectral
pattern created by interference among waves. David Bohm had to chastise
me on several occasions before I stopped thinking of waves and began to
think in terms of spectra in this context.

Greenstein and Zajonc come to the conclusion: “Clearly, if we take
quantum mechanics seriously as making statements about the real world,
then the demands on our conventional thinking are enormous.” At this
point recall my claim that “conventional thinking” is prejudiced by lenses
and the lense-like operations of our senses. Greenstein and Zajonc
continue: “Hidden behind the discrete and independent objects of the sense
world is an entangled realm in which the simple notions of identity and
locality no longer apply.”

Since the early 1960s most of us have experienced in our own sense
world the value of a method for attaining correlations—the Fast Fourier
Transformation (FFT); and the value of the image storing and restoring
powers of the holographic process. Examples of the use of “quantum
holography” in image processing are tomography (PET scans and fMRI—
functional Magnetic Resonance Imaging) and more recently in the
operations of digital cameras.

This mathematical and engineering triumph, though available to us in
the world we navigate, partakes of most of the “bizarre” attributes of
quantum physics. For instance, when space-time is Fourier transformed
into the spectral domain there can be no cause and effect in the usual



scientific sense. The Fourier transformation is a spread function that
disperses space-time events which therefore no longer exist as such.
Currently scientists ordinarily seek what they call “efficient” causation in
which effect follows cause. In the holographic, enfolded domain, space
and time disappear, so it is inappropriate to inquire as to where or when
an efficient causal relation exists. The transformation from space-time to
spectrum (and back again to space-time) is a change in form and thus falls
under Aristotle’s formal causation. It is in this respect that Greenstein and
Zajonc’s admonition that “the very concept of cause and effect must be
rethought” is honored.

A change in form, a trans-formation in itself suggests that some
uncertainty may inhere when an attempt is made to measure both spectral
and space-time forms simultaneously. The world looks different when
one’s pupils are dilated—a sort of neutral zone between having a good
pupil/ lens system and having none. A good deal of uncertainty is involved
when one tries to navigate the world in this condition. Greenstein and
Zajonc’s second bizarre phenomenon, that measurement can never be
completely accurate, actually does occur in the ordinary world of
communication as well, as developed by Gabor in his (1946) “quanta of
information.”

An argument has been made that transformations such as the Fourier
are simply conveniences to be applied as needed to describe a particular
phenomenon. This is not necessarily so. The transformations describe
“real world” measurements that cannot be arbitrarily assigned to one or
another situation. In measuring Gabor-like (Hilbert space) processes in
sensory receptive fields of the primate sensory cortex, my colleagues and I
showed that electrical stimulation of the posterior part of the brain would
shift the receptive field toward a space-time configuration, while
stimulation of the frontal part of the brain shifted the configuration toward
the spectral domain. These changes occurred in the experienced space-
time world we navigate, not in an arbitrary application of mathematical
whim.

In short, weirdness is not restricted to the quantum scale of
observation. Instantiation of the Fourier relationship in holography has
demonstrated practically identical “bizarre” characteristics. Bringing that
weirdness into our everyday experience makes it less weird as we become
familiar with it. Greenstein and Zajonc summarize the issue succinctly



with the statement that “hidden behind the discrete and independent
objects of the sense world is an entangled realm,” and at the scale in which
we navigate our world, is hidden a holographic universe in which are
embedded the objects we perceive with our senses and actions. The
enfolded realm spans all scales of inquiry from cosmic through brain
processing to quantum fields and accounts for much of the weirdness
encountered in attempted explanations of observations.

102. Logons, Gabor Elementary Functions: Quanta of Information.

103.



Cosmology
I began this essay with David Bohm’s observation that if we did not

have telescopes and other lens-like means of observation the Universe
would appear to us as a hologram. Thus the laws of optics such as the
Fourier relationship are relevant to these observations. Bohm’s insight
implemented by the Fourier relation brings clarification not only at the
quantum scale but also to cosmology. The medium that allows us to
observe the cosmos is radiation. Background radiation has been given
several names depending on the observed data-base upon which the name
is given. Bohm, called it a “quantum potential”; Puttoff calls it “zero point
energy.” In conversations with each of them they agreed that the structure
of this background radiation is holographic. Currently, the terms “Dark
Energy” and even “Dark Matter” have surfaced as having to be measured
and conceived in terms other than space and time. By analogy with
potential and kinetic energy, I conceive of both these “hidden” quantum
and cosmological constructs as referring to a “potential reality” which lies
behind the space-time “experienced reality” within which we ordinarily
navigate.

In a recent Smithsonian presentation, Roger Penrose revealed that by
using his famous techniques of conformal rescaling he has reformulated
what occurs at the horizons of our universe, both with respect to the “big
bang” and its presumed ever-accelerating expansion. Instead of a big hot
bang, he finds the metaphor of “a gentle rain falling upon a quiet lake,
each drop making ripples that spread to intersect with other ripples made
by other drops.” The patterns recur at the “expanding” future boundary of
the universe. These patterns are, of course, holographic. Penrose’s fertile
brain has made it possible for him to reformulate currently accepted
dogma with an alternative more compatible with Buddhist and Hindu
teachings than with the creationism of the Judeo-Christian-Islamic
tradition. Important here is not whether one or another view of the cosmos
is correct, but that Penrose could use an intellectual brain-formed tool,
“conformal rescaling,” to provide a cosmology totally different from a
currently mainstream scientific conception.

I am deeply grateful to Patrick Heelan for his encouragement, interest
and critique of earlier drafts.



References
Aharonov, Y., Rhorlich, D. (2005) Quantum Paradoxes. Weinheim;
Cambridge: Wiley-VCH

Bohm, D. (1973) Quantum Theory as an indication of a new order in
physics. Part B. Implicate and Explicate Order in physical law.
Foundations of Physics, 3, 139–168

Brillouin, Leon. (1962) Science and Information Theory, 2nd ed. New
York: Academic Press, Inc.

Chapline, G. (1999) Is theoretical physics the same thing as mathematics?
Physical Reports 315, 95–105

Chapline, G. (2002) Entangled states, holography and quantum surfaces, in
Chaos, Solitons and Fractals, 14, 809–816

Gabor, D. (1946) Theory of communication. Journal of the Institute of
Electrical Engineers, 93, 429–441

Gabor, D. (1948) A new microscopic principle. Nature, 161, 777–778

Greenstein and Zajonc (To be completed)

Leith, E. N. and Upatnicks, J. (1965) Photography by Laser. Scientific
American, 212: 24–35

Llinás, R. R. (2001) I of the Vortex: From Neurons to Self. Cambridge:
MIT Press

MacKay, Donald G. (1987) The Organization of Perception and Action: A
Theory for Language and Other Skills. New York: Springer-Verlag

Miller, G. A., Galanter, E. and Pribram, K. H. (1960) Plans and the
Structure of Behavior. New York: Henry Holt. (Russian trans; also in
Japanese, German, Spanish, Italian)



Pribram, K. H. (1991) Brain and Perception: Holonomy and Structure in
Figural Processing. Hillsdale, NJ: Lawrence Erlbaum Associates, Inc.

Puttoff, H. (To be completed)

Shannon, C. E. and Weaver, W. (1949) The Mathematical Theory of
Communications, 117. Urbana, IL: The University of Illinois Press

Stapp, H. P. (1997/1972) The Copenhagen Interpretation. American
Journal of Physics, 40: 8, 1098–1116



Appendix B
As Below, So Above

The Quest
There are more than 37,000 neuroscientists currently performing

experiments that address the topics that have formed my scientific life and
the chapters in this book. From them we can expect a torrent of research
results that bear on the health of our bodies and our brains. But there is
more we need to know. I am often asked what all this research is good for?
My answer is the one Faraday gave when asked this question regarding his
discoveries of the properties of electricity: “Someday you’ll be able to tax
it.”

My more serious answer addresses the very issue that lies at the heart
of today’s quandary: As a result of the industrial revolution and modern
scientific advances we have spawned a materialist society that fails to
address the spiritual quest that nurtures us. I hold that using the research
results I’ve reported in The Form Within and the interpretations I’ve
derived from them, we can chart a course that reverses this strictly
materialist image of ourselves.

It is time for us to consider how our scene has changed. How have the
experimental achievements of the last half-century—as recounted here—
altered our views? Most important, “in ancient times” we navigated our
world and discovered experiences in ourselves that reflected what we
observed in that world: We woke at sunrise and slept at sunset. We were
intimately connected at every level with the cycles of nature. This process
was disrupted by the Copernican revolution, by its aftermaths in biology—
even by our explorations of quantum physics and cosmology—and in the
resulting interpretations of our personal experiences. But today, once
again, we have rediscovered that it is we who observe our cosmos and are
aware that we observe; that it is we who observe our navigation of our
world and observe our own observations.

This overturning of the balance between the priority given to
observations of the world we navigate and the priority given to the
navigation process itself reverses a trend that began with Copernicus and



Galileo and was continued by Darwin and Freud. The oft-lamented shift in
perspective that moved the cortical human away from being the center of
his universe is being reversed. But there is also a change from the earlier
view: No longer is it “as above, so below;” now the cortical human is
making his navigations, his quest, the means to create his world: “as
below, so above.”

“Remembrances-of-things-future” takes on a special flavor when we
apply this concept to Homo sapiens sapiens. As Sapiens, we note that
others are mortal and, by way of remembrance-of-the-future, we are able
to extrapolate from such observations that we too may die. Our “Narrative
I” is made up of episodes that have a beginning and an ending. Our
beginning originates in our mother/mater/ matter. We can prolong our
narrative by including episodes experienced with or by our biological or
creative offspring and their descendants—as well as by documenting in
symbols the situations in which our personal episodes have occurred. Thus
by our creativity we are circumventing our ending in death.

When we humans find ourselves in situations that are difficult to
navigate, we seek to “formulate” the situation, that is to create form as a
basis for continuing our navigation: we create (hi)-stories or do science, or
both. The quest for form can be seen as a spiritual endeavor. As such, the
quest pursued by scientists is spiritual, that is, it is pursued with “a special
attitude or frame of mind characterizing and animating an individual or a
group” (Webster’s International Dictionary). Many of my most creative
colleagues have expressed awe and wonder at what they and others have
discovered. Scientific exploration is a spiritual process in which Homo
sapiens sapiens excels.

Spirit is ordinarily defined as being immaterial. As I recounted in
Chapter 24, thermodynamics is a scientific endeavor that is based on the
utilization of forms of energy, not matter. Also, descriptions of holoflux,
(the quantum potential, zero-point energy) and quantum holography are
devoid of any space-time structure. These endeavors are scientific and the
patterns they investigate describe the patterns ordinarily probed under the
rubric “spirit.” There was a time when brain processes were described as
mediated by “breath”—that is, spirit. Now we affirm that these “spirits”
are electrical, and in the small-fibered web, they form electrical fields.
And as we continue to explore the effects of magnetism and “soft



photons,” that is of heat, there is likely a great deal more we will discover
about brain processing that affirms our spiritual nature.

We know that Homo sapiens sapiens has experienced the spiritual
aspect of “being” since the dawn of our species. Science is a quest and, as
I have tried to show, there is no reason scientists should continue to
restrict that quest to be confined merely to the composition of matter.

I remember my colleagues’ and my excitement some 75 years ago
when, in Ralph Gerard’s laboratory at the University of Chicago, we
tapped a cat’s paw and instantly obtained a response from an electrode
implanted in the cat’s brain The electrode was hooked up to a loudspeaker
and every time we tapped the paw, the loudspeaker emitted a pulse. The
cat’s brain had registered an event that had occurred in the cat’s external
environment! For the first time we realized that it might be possible for us
to track how environmental stimulation could be remembered.

Initially, many of us believed that the shape of the environmental
stimulus would simply be “stamped” in: Plato had suggested that memory
worked something like a piece of wax upon which experience becomes
stamped. But as as I’ve explored through my research and explained in
these chapters, the formation of a memory has been revealed to be much,
much more complex and fascinating, far beyond the expectations any of us
had when we made those first observations.

I’ll recap here some of that complexity: Comprehension is enhanced
by the sampling operations of the posterior sensory-specific “association”
cortex. Our choices of what to sample are based on several things: in our
familiarity with painful and pleasant consequences of our prior choices
which are processed by our limbic basal ganglia— and in our estimate of
the likelihood that a current choice will successfully yield a pleasant
outcome, an estimate that involves the frontal cortex.

We can parse our experiences in many ways. In cosmology, we call
this a “many worlds” solution to “what is really out there.” By knowing
how our senses and our brains operate, we can reduce the number of
possibilities, of “worlds,” that our research is still uncovering. (See
Appendix A for specific details on how our speculations about the cosmos
are formed by our biology and the culture we live in.)

A problem arises in the brain/behavior/mind sciences when brought
to bear on our experience of right and wrong. One direct solution is to
declare that doing right leads to pleasure, doing wrong leads to pain. The



complexities that devolve from such a direct attempt at solution have been
argued for millennia. For the brain sciences per se, the issue is not what is
right and wrong, but that Sapiens has the ability to conceive of rightness
and wrongness. Conceiving right and wrong entails consciousness and
conscious choice. The word conscience in the French language makes no
distinction between (in English) “consciousness” and “conscience.”

The quest for how our brains operate to enable us to distinguish right
from wrong is a fascinating one. The first steps were taken in applying the
concepts of economic utility theory as I’ve described in Chapter 16. Later
experiments have revealed to us common errors that we humans routinely
make in our judgments. There is promise, as we find out more about
exactly how the brain works when we are making such errors, that we can
circumvent them.

Dreams, drugs and Zen meditation have in common their ability to
dissolve the boundaries that ordinarily define our ability to formulate right
and wrong, boundaries that we ordinarily use to navigate our world. Both
in science and in the humanities, we construct such boundaries to give
shape to our physical and social our worlds.

One winter morning at Stanford, I began my course in
neuropsychology by pointing out that I was making an assumption
fundamental to the way I was going to present the course material. The
assumption was that our skin serves as a boundary between our
environment and the internal workings of our body. This boundary is, to
some considerable extent, a fiction: Is the boundary to be placed inside our
socks or outside our shoes? Are our eyeglasses a part of the boundary? Or
are they a boundary only when they are contact lenses?

Boundaries determine what is “without” and what is “within.” Much
has been learned about what we experience about what is “out there” in
relation to what is “in here” from experiments consequent to fitting the
glasses with prisms that invert the visual world—which by navigating that
world it again is perceived as right-side up. The results of these
experiments showed that the boundary between our egocentric (in here)
and our allocentric (out there) space is placed at the end of our reach.

One morning,I described to my class these examples that raise the
issue as to whether the skin can really be considered a boundary between
our self and the outside world, an issue that needs to be kept in mind. But I



also stated that I was positing, for the course ahead, that the skin is most
conveniently conceived as a boundary.

That noon, at the Palo Alto Research Institute I heard a talk given by
Alan Watts. Watts had just returned from a sojourn in Japan where he had
undertaken to understand Zen Buddhism. The contents of his talk that
afternoon, which would later became the topic of his famous book
Psychotherapy East and West, was that Zen taught you that the skin is an
artificial boundary! When we hold onto the boundary, we either blame our
parents for our ills, or we blame ourselves. Neither is healthy. When we
insist on assigning responsibility, we make us less response-able.

Whatever happens happens, and we can try to find out what multiple
transactions and transformations are involved. We need not put all the
burden of “guilt” on ourselves, nor pass it off onto others.

Meditative exercises, drugs and dreams go further in dissolving these
boundaries. First, we experience patterns rather than shapes and, if taken
to the extreme, we can experience the loss of patterns as well: only a holo-
flux, “the white light” remains.

Similarly, the boundary between what we call “science” and what we
call “story” is only “skin deep” and not always as clear as is generally
held. During the 1960s, I used to remark that science and the scientific
method provide by far the best way humans have found to share our
experience of the world. The scientific method, with its constant probing
and reformulating, provides one excellent means toward refreshing
communication. But it is the communication that applies the skills of the
humanities that makes science’s stories sharable.

The Two Cultures
The methods of science and the methods of the humanities can, to

some extent, be distinguished. But is the quest that is engaged by the
different methods distinguishable? Sapiens sapiens, the cortical primate,
seeks knowledge and security. These come together in a quest for meaning.

Choosing always begets risks. The consequences of choosing can be
painful or pleasurable—or, more often than not, some of both. I had the
pleasure of seeing a drama in Bali that portrayed the following scenario:
The hero had taken a risk and made a choice that had painful consequences
for himself and his loved ones. The “devil” came and took him away—the



hero died. Mourning. A funeral procession. Doleful music. About halfway
through the course of the procession the motif of the music began to be
uplifting—as were the middle parts of the garments covering the corpse.
Before long a huge artificially rigged erection adorned the midsection of
the corpse and great rejoicing came over the crowd. But that was not the
end of it. A long sermon followed the uniting of the hero with his
sweetheart. The sermon, accompanied by pantomime and puppetry,
proclaimed that according to Balinese historical tradition “every choice
we make, every action we take, has good as well as bad consequences—
and furthermore, not choosing, not acting, is a choice, is an action.” The
hero and his bride had experienced the bad consequences and now had
been given the chance to act on the good consequences of what had
transpired: a lovely dramatization that embodies the wisdom of the
Balinese culture.

There have been many statements made recently that science has
banished mankind from the center of his universe and therefore
impoverished his spirituality. This has not been my experience as a
scientist—nor that of many of my colleagues. In fact as I read them, the
major contributors to moving us from an egocentric view of ourselves—
the scientists who have made the major contributions to the Copernican
revolution, to Darwinian evolution or to Freudian psychoanalysis—never
expressed any effacement of spirit or spirituality. Quite the contrary, they
all were profound “thinkers,” that is, doubters. Therein lies a paradox.

Without doubt, there is no belief. If there were no doubt, experiences
would just be. As in the case of materialism and mentalism, the one could
not be articulated without the other. If there is no up, there can be no down.
Doubt engenders quest and quest engenders belief. Belief comes in a range
of ambiguity, and thus offers us a range of assurance—and each of us
differs in our tolerance of ambiguity. Assurance can be centered on
oneself, or on the world we navigate.

During the 1960s, I sat on a variety of committees public and private,
that were determining the distribution of fellowships. I tried always to see
to it that the fellowships would include funds for the recipient’s family so
that the acceptance of the fellowship would not be disruptive. I placed
great faith in the fellowship programs to further science scholarship and
productivity.



A decade later, I began to realize that one could not rely on these
fellowship programs. They had their own agendas. One such agenda might
be to support start-up investigations. Another might be to fund research
that would have a high probability of visible payoff. In view of these
experiences, I began to train my students to rely on themselves, to decide
what research they wanted to accomplish, and where the best laboratories
were to accomplish their aims. They had to have the assurance that would
develop our science in innovative directions—one could not rely on
“institutions” to do this.

My experience in brain research has taken a course similar to the one
that I found to characterize the science support system. As a behaviorist, I
sought the formative influence on the brain and on behavior to be the
shaping of more or less initially random behavior by environment. But as I
have described, not only I, but the whole field of psychological science,
has found that the wellsprings of behavior lie in our navigating the world
we experience and that our navigations shape our brains and the
environment we experience.

We, Homo sapiens sapiens embrace among our experiences non-
material, spiritual qualities. Much of the research that I have reported here
has dealt with forms, with patterns, that are, in themselves, not matter per
se. These patterns can be considered the spiritual complements of matter.
In this sense, spirit and spirituality now have status as topics ripe for
scientific investigation.

So Above
One of my cherished examples of the wonder that is opened up to us

through scientific exploration, is the knowledge revealed in the action of
chlorophyll. We have all admired Einstein’s formulation of the relation
between energy and matter in the equation E = mc2. But chlorophyll
actually performs Eintein’s equation by transforming radiation from the
heavens to our planet into usable energy in plants that feed animals, and it
has done so for ages on end. Our scientific explorations of the origins and
evolution of chlorophyll, and the manner in which chlorophyll works,
make for an “awesome” and “wonderful” tale indeed.

The brain is to mind what chlorophyll is to life. The brain has made
possible a transformation from merely “behaving” in a rote fashion to



“minding,” that is, to thinking, intending, and attending. Our brain does
this by actively transforming our future from what it has collected and
processed from our past experience.

Viewed from the vantage of discoveries in the brain sciences that I
have surveyed in The Form Within, the world we navigate takes on a
renewed radiance. A technical definition of “radiance” given in Webster’s
dictionary is: “the flux density of radiant energy per unit solid [three-
dimensional] angle”: that is, a holoflux.

The radiance we experience in a rainbow is created by interference
patterns among a bounded bandwidth of wavelets. Experiencing radiance
is in keeping with the Pythagorean theme we’ve become familiar with in
The Form Within, wherein I have here recounted the story of one quest for
that elusory “pot of gold” heralded by the radiance of the rainbow.



A

Abductive reasoning, 387

Action(s), 168

contravariation in action patterns, 128, 170

fixed action patterns, 169–170

movements distinguished from, 161–163

scotoma of, 165, 166

Addiction, 199, 282–283

Adrenal gland, 262, 268, 294

Adrenaline, 256, 268, 294, 296

Adrian, Lord, 359

Aggression, 328–331, 333–341

cultural context and, 339–342

male, 329–330

modes of, 328–329

triune brain and, 331–334

Agnosia, 23

Ahern, Chuck, 470–471

Aharonov, Yakir, 393, 395, 517

The Alexandria Quartet (Durrell), 423
Algorithmic complexity, 322, 362

Allocentric integrity, 470

Allocortex, 215, 216, 218, 334

Allport, Gordon, 298

Altamira cave paintings, 10–11



Ambliopia ex anopsia, 20

American Sign Language, 436

Amygdala, 215, 217–219, 224–227

attention and, 256, 257

desirability and, 275

familiarization, habituation and, 256, 262–263

Amygdalectomy, 32, 224, 226, 229–233, 250, 251, 255, 256, 262, 265,
286, 320

preferences and, 277–278

sexual behavior and, 232–234, 335, 336

traumatic avoidance experiment, 261

Anand, Bal, 226–228

Anatomy of the brain after World War II, 188–191

frontolimbic forebrain, 217–218

before World War, 188

Anderson, Pier, 200–201

Anger-in and anger-out, 257–258

Anorexia, 199

Anxiety, 240

Aphasia, 23, 442

Broca’s, 310

Appetitive phase, 199, 203, 228, 266

Archetypes, 12, 415–417

Ashby, Ross, 158–159, 503

Association cortex, sensory specificity in, 507–509

Associations (associative view), 23



Atkinson, Richard, 368

Atmanspacher, Harald, 486

Attention, 256–258, 262, 265, 269, 270, 462, 467

Attitude(s), 285–300

structure of attitudinal patterns, 298–299

Attractors, 366, 395, 517, 520

point, 119, 129, 363, 395, 517

Autopoiesis, 365–367

Awareness, modes of, 466–467

B

Bagshaw, Muriel, 225, 226, 251, 255, 257

Bailey, Percival, 146, 180, 184–187, 189, 208, 212

Barlow, Horace, 62, 68–70

Bartlett, Sir Frederic, 423

Basal ganglia, 188, 217, 218

middle, 266–267, 478

upper, 257–260, 266, 270

Bateson, Patrick, 234, 303

Beach, Frank, 234, 335

Beck, Lloyd, 34

Behaviorism (behaviorist psychology), 152, 162, 164, 167, 224, 234,
238, 251, 341, 351, 352, 383, 386, 484, 485, 538

Békésy, Georg von, 110–114, 477

Belief, perception of objects and, 122–124

Bell, Charles, 151



Bell and Magendie law, 151, 522

Bergson, Henri, 471

Berkeley, George, 122–123

Bernstein, Leonard, 363–364

Bernstein, Nikolas, 167–168

Biological memory, 356, 359, 376, 403

Birds, 333, 334

Bishop, George, 54

Blanco, Matte, 60–61, 72, 478–481

Blind-sight, 467–468

Bohm, David, 75, 92, 101, 490, 491, 514–516, 520, 522, 523, 526

Bohr, Niels, 82, 393, 395, 396, 491, 515, 516, 519

Boltzmann, Ludwig, 243

Bonin, Gerhard von, 212

Boring, Gary, 21, 426

Bower, Gordon, 368

Bradley, Raymond Trevor, 57

Brain injuries, 408

Brain Mechanisms and Intelligence (Lashley), 21
Brain/mind (mind/body) relationship, 408–411

Descartes and, 486–488, 498

Fourier diagram and, 491–493

Brain processes, 4, 14, 401–402

coordination of, 402–403

in feelings, 224

in memory, 423–429



in perception, 39, 41, 52, 60, 81

in pleasure and pain, 182, 210

Brain stem, 217

Breathing, control over, 190

Brentano, Franz, 348, 458

Brightness (luminance), 65, 121, 382

British associationism, 507

Brobeck, John, 225–228

Broca, Paul, 218, 219

Broca’s aphasia, 310

Broca’s area, 172, 307, 310, 408

Brown, Graham, 169, 170

Brown, Roger, 158

Bruner, Jerome, 136, 180, 239

Bucy, Paul, 26, 30, 146, 180, 181, 184, 185, 187, 208, 214, 232, 311,
427, 428

Bulimia, 199

Bull, Nina, 298

Bullock, T. H., 47, 54

Burr, Chandler, 389–391

Byrd, Admiral Richard Evelyn, 181

C

Calcium ions, 370, 371

Campbell, Fergus, 61–62, 75, 76

Cannon, Walter, 154, 180, 224, 225, 296, 297, 474



Carlton, Eloise, 140–141, 491

Carroll, Lewis, 479

Cartesian coordinates, 9, 417, 487, 491

Causation, 413–415, 430

efficient (efficient causality), 383, 385, 386, 393, 394, 398, 400, 412,
414, 430, 487, 524

formal, formative, 5, 395, 396, 398, 400, 401, 412, 415, 430, 524

Cave paintings, 10–11

Chaos, order out of, 365–367

Chaos theory, 349, 362, 365, 366, 394, 395

Chapin, John, 452, 453

Chapline, George, 514

Chew, Jeffrey, 93, 491

Chimpanzees, 10, 435–437, 439

Choices multiple-choice experiment, 501, 503–505, 507

paradoxical, 278

Chomsky, Noam, 174, 422, 443

Chunking, 322–324

Cingulate cortex, 180, 211, 215, 216, 218, 219, 305, 309, 312, 318, 324

Circles, 13

Classical (Pavlovian) conditioning, 255, 261, 277, 450

Cochlea, 111, 112

Cognitive neuroscience, 124, 244, 245

Color perception of, 382–383, 464–465

spectral processing of, 45

Color-blindness, 464–465



Colors (color vision), 71–72

Communication matter and, 496–498

reduction of uncertainty and, 494, 496

Complementarity (complementary forms), 394, 396–397

Complexity (complexity theory), 320–322, 347, 349, 350, 361

formative causation and, 393–398

language and, 439–443

Comprehension, 442, 471

of meaning, 509–511

Compression of experienced memory, 362–363

Concave mirrors, 110, 138, 140, 141

Concilience, 278–280

Conditioned avoidance, 260–261

Conformal rescaling, 142, 526, 527

Connections, 375

Consciousness (conscious thought or experience), 398, 410, 411, 461–
481

blind-sight, 467–468

conscious-unconscious distinction, 479–481

egocentric integrity (objective “me”), 468–470, 474–476

hidden navigator (hidden observer), 477–481

modes of awareness, 466–467

narrative “I,” 470–476

privacy and, 462–466

unconscious processing, 476–481

Conscious thought, 450



Consensuality (consensual validation), 122–124, 403

Consolidation of memory, 368–369

Constancy, 139

Constraints, 265–266

Constructions, objects as, 121–122

Contour maps, receptive fields and and, 83, 84

Contravariation, 128, 170

Controllable thermostat, 152, 154, 156–160, 170, 171, 189, 190, 523

Coordinates, 380–383

Cartesian, 9, 491

Coordination, as basis for transformation, 402–403

Copernicus, 367, 532

Core-brain, 184–191, 200, 201

pattern sensors, 184–187

Corollary discharge, 159

Correlations, 15, 19–32, 172, 381–382

inaugural experiments, 22–23

Cortex, 334

intrinsic, 23–25

language and, 441

sexual behavior and, 335, 336

silent areas of, 22–24

Cosmology, 526–527

Covariation, 128, 170

Crawford, Helen, 477

Crick, Francis, 355, 410



Cultural context, 339–342

Cushing, Harvey, 184, 311

Cybernetics, 157

D

DADTA (Discrimination Apparatus for Discrete Trial Analysis), 504

Damasio, Antonio, 210

Danziger, Kurt, 423

Darwin, Charles, 347–351

DeBroglie, Louis, 392, 519, 523

Deductive reasoning, 386–388

Deep thought, 457–458

DeKruif, Paul, 181

Dendritic web, 49

mapping, 50–53

Descartes, René, 9, 327, 417

mind-body relationship and, 486–488, 498

Desirability, 275–278

D’Espagnat, Bernard, 407, 497–498

DeValois, Russell and Karen, 43, 45, 59, 65, 68–70, 75–78, 97

Dewey, John, 494, 506

Diabetes insipidus, 200

Diamond, Marian, 358

Dirac, Jacques, 491

Direct current (DC), 35–40

states of awareness and, 38–39



Dissociation, 132–134

Diversification, 348–349

Dominance hierarchy, 229–232

Dorsal thalamus, 150, 189

Dreams, 458, 535, 536

Drive stimuli, 269

Dual aspect approach, 485–486

Duration (of an experience), 471–472

of frustration, 308–309, 311

sense of, 277

Durrell, Lawrence, 423

E

Ear, 111

Eating, 190, 199, 225–228

amygdala and, 226, 232, 265, 266

Eating disorders, 199

Eccles, John, 42, 152, 202, 332, 397

Economo, Constantin von, 180, 181, 216

EEGs (EEG recordings), 36, 37, 75, 302–304, 321–323, 362, 363, 369,
440–441

complexity of, 362–363

developmental stages in, 302–304

music and language experiments, 454–457

Efficiency, 286–291, 294

effectiveness distinguished from, 318–319



Efficient causation (efficient causality), 383, 385, 386, 393, 394, 398,
400, 412, 414, 430, 487, 524

Egocentric spaces, 132–134, 423, 470, 475, 479, 535, 537

Eidetic memory, 404

Einstein, Albert, 83, 379, 420, 473, 513, 519, 521, 538

Electrical activity of the brain, 360–363

Electrical stimulation, 63, 67, 110, 135, 149, 154, 191, 196, 198, 202,
210, 211, 213, 215, 256, 258, 259, 269, 271, 297, 312, 314, 328, 369,
427, 428, 441, 507, 525

Elementary function, Gabor’s, 102, 104, 525

Eliminative materialism, 355

limitations of, 383–385

Emergence of psychological properties, 396, 408–411, 430

Emerson, Ralph Waldo, 7

Emotions, 269, 295. See also
Feelings as attractors, 269–270

definition of, 295

as “hangups,” 259–260

limbic brain and, 213–214

Empathy, 474

Endocrine responses, 262, 263, 269

Endorphins, 193, 194, 201–203

Energy, measurement of, 493–495

Entropy, 493, 494

Epilepsy, 427–429

temporal lobe, 231, 267–268

Epileptic seizures, 37, 38



Episodic memory, 291, 354

Estes, William (Bill), 320, 322, 448

Etlinger, Susan, 454–456

Euclid (Euclidian geometry), 12, 16, 35, 61, 65

Evolution complexity, self-organization, and, 349–352

Darwinian, 347–351

inherent design and, 345–352

Executive circuits, 55–57

Expected utility, 274

Experimentation, limitation of, 385–388

F

Faith, perception of objects and, 122–124

Familiarization, 255–256, 263, 264, 269, 270, 274, 297. See also
Habituation

Faraday, Michael, 389–390

Fast Fourier Transform (FFT), 97–99

Fear, 32, 213, 260–261

Feature detection approach, 60, 61, 70, 78, 81, 82

feature extraction vs., 64–68

Features, 60

Feelings. See also Emotions definition of, 295

Descartes on, 488

localized vs. hedonic processing of, 296

Festinger, Leon, 280

Feynman, Richard, 483, 489



Fields, R. Douglas, 55, 370, 371

Field theories, 392–393

“Fight and flight,” 224–225, 228, 297

Final causation, 395, 412, 414, 415, 430, 464

Fine-fibered branches, 42, 49–51, 54, 55, 82, 424, 444, 458, 459, 466

Fixed action patterns, 169–170

Flexible assemblies, 55–57

Flux, space-time and, 495–496

FMRIs, 25, 99, 110

“Folk psychology,” 355, 383, 384

Foramen magnum, 188, 217

Forebrain, 188–190, 194, 217

limbic, 214

electrical and chemical stimulation of, 210–213

Form, 12. See also Transformations concept of, 8–9
creating, 374–376

as shape and as pattern, 9, 12–14, 72–74

Formal, formative causation, 5, 395, 396, 398, 400, 401, 412, 415, 430,
524

complexity theory and, 393–398

The Form Within, 4, 5, 8–10, 13–16, 32, 52, 60, 348, 389, 414, 539
Förster, Heinz von, 366

Four Fs (Fighting, Fleeing, Feeding and Sex), 225, 238, 250, 251, 259,
278, 335

as basic instincts or dispositions, 234–235

Fourier, Jean Baptiste Joseph, 94–97, 483, 484, 488–490

Fourier diagram, 491–493, 495



Fourier theorem, 489, 491

Fourier transform, 54, 93, 127

fast (FFT), 97–99, 110

windowed, 103–106, 109, 364

Free association, 458

Freeman, Walter, 252–253, 313, 314

Free will, 398–401

Frequencies, 60

spatial, 62, 68, 69, 73, 74, 76, 77

Frequency approach to analyzing visual patterns, 76, 81, 82, 87

Freud, Sigmund, 23, 239–245, 258, 269, 270, 297, 298, 302, 348, 352,
423

“free association” technique, 458

Piagetian stages and, 302, 303

unconscious processing and, 476, 477, 479

Freud’s “Project” Reassessed (Pribram and Gill), 241, 244, 245, 338
Friedman, Danny, 238

Frontal lobes, pain and, 193

Frontolimbic forebrain, anatomical fundamentals, 217–218

Frustration, 257, 308–309, 324

Fulton, John, 165, 189, 198, 208–210, 214, 219, 311–313, 315

Fundamental tones, 75–76

G

Gabor, Dennis, 42–44, 54, 80, 92, 163, 263, 387, 493, 494, 496, 515,
518, 524

elementary function, 102, 104, 525



holographic process, 42–44

quanta of information, 80, 102, 106, 108, 109, 114, 263, 486, 496,
524, 525

windowed Fourier transform, 103–106, 109, 364

Gabor functions, 106, 107, 120, 128, 298, 447, 486

Gaffan, David, 291

Galanter, Eugene, 21, 154, 156, 158, 204, 251, 301, 360, 367, 523

Galileo Galilei, 421, 433, 450, 483

Gall, Francis J., 25–26, 408

Galton, Sir Francis, 351

Ganglion cells, 78, 79

Gardner, Allen and Beatrix, 436

Generative organization, 171

Geometric isomorphism, 35, 37, 401

Geometry Euclidean, 12, 16, 35, 61, 65

plane, 81

Pythagorean, 12, 13, 415–417, 419

Gerard, Ralph, 34, 35, 39, 47, 208, 413, 414, 533

Gerbrandt, Lauren, 155, 156

Gestalt psychology, 35

Gestures, 445, 446, 468, 480

Ghost in the machine, 125, 126

Gibbons, 434–435

Gibson, Eleanor, 71

Gibson, James, 68, 71, 99–101, 127, 166, 167

Gill, Merton, 241–245, 338



Goldscheider, A., 40

Grandmother cells, 70, 71

Graves, Robert, 353, 379, 461, 473

Greene, Bert, 502–503

Green hand cells, 70–71

Greenstein, George, 523–525

Gross, Charles, 71

Gross, Charlie, 508

Group theory, 128–130, 134

Grundfest, Harry, 147

Gumnit, Robert, 36, 37

H

Habituation, 255, 263, 288, 297. See also Familiarization
Hagen, Scott, 183

Harlow, Harry, 30

Harmonics (harmonic analysis), 75–76, 81, 83, 86, 104, 363, 384, 439

Harnad, Stevan, 488

Harré, Rom, 396

Harvard inductorium, 210–211

Hawking, Stephen, 92

Hayes, Cathy and Keith, 435

Hebb, Donald, 20, 29, 41, 136, 196, 437

Heelan, Patrick, 139–140, 518, 527

Heisenberg, Werner, 82, 103, 105, 141, 492, 493, 496, 519

Helmholtz, Hermann von, 97, 117, 120, 128, 130, 244, 399



Herschel, Sir John F. W., 349

Hidden navigator (hidden observer), 477–481

Hilbert space, 103, 141, 522, 523, 525

Hiley, Basil, 490, 491, 522

Hilgard, Ernest (Jack), 39–41, 42, 43, 239, 386, 477

Hindbrain, 188, 190, 217

Hippocampus (hippocampalcingulate system), 216–217, 219, 260, 267,
277, 286–288. See also Cingulate cortex

familiarity and, 288–291

holographic aspects of hippocampal processing, 292–293

Hodgkin, A. L., 49, 50

Hoffman, William, 129

Holism, holographic form of, 520–521

Holography (holographic process), 42–45, 92–93, 519–521

brain waves and, 74–75, 108–109

conformal rescaling and, 142

flux and space-time, 495–496

hippocampal processing, holographic aspects of, 292–293

quantum theory and, 515–516

retinal image and, 101–102

slide projector demonstration, 44–45

Homeostasis, 154, 204

Homeostatic control (or processes), 160, 189, 190, 202, 203

Homunculi, 146, 150, 315

Horizons, 139, 140, 142–143

of our universe, 142–143



Hubel, David, 40, 41, 61, 63, 64, 68–72

Hudspeth, William, 303, 304

Hull, Clark, 164, 386

Huxley, A. F., 49

Hyden, Holger, 369, 370

Hypothalamic region, 189, 191, 200, 202, 216, 226, 227, 259, 297, 337

Hypothesis testing, 386, 388–389

I

Images objects distinguished from, 120–121

remembering, 354

“Images of achievement,” 14, 162, 171–173, 317, 319, 374, 443

Imaginings, 173

Imitations, 173

Induction (inductors), 372–374

Inflections, 445

Information, 9

meaning of, 500

measurement of, 500–503

Information processing, 456, 500, 510, 518, 519

Information revolution, 8

Inherent design, in the evolution of mind, 345–352

Instincts, 234–235

Intention, 462

Interference patterns (interference effects), 41–44, 109, 241, 251, 293,
420, 490–491, 516, 520, 523, 539



Intrinsic cortex, 23–25

Introspection, 125, 167

Invariances, 170–171

Isomorphism, geometric, 35, 37, 401

J

Jacob, Pierre, 171

Jakobson, Roman, 174

James, William, 3, 91, 180, 210, 213, 253, 257, 258, 260, 262, 295, 296,
298, 328, 348, 424, 458, 462

James-Lange theory, 210

Jeannerod, Marc, 171

Jibu, Mari, 107, 183, 492

Johanssen, Gunnar, 127, 168, 170

K

Kafatos, Menas, 106

Kahneman, Danny, 279–280

Kairos, 277, 473

Kanzi (bonobo), 437

Kauffman, Stuart, 350

Kelso, J. A. Scott, 394

Kepler, Johannes, 137

“Kluges,” 346, 347

Klüver, Heinrich, 180–181, 232

Koch, Sigmund, 484



Koestler, Arthur, 331–333, 351–352, 367, 407

Köhler, Wolfgang, 35–37, 39–41, 135, 251, 274

Koko (gorilla), 435, 436, 439

Kretsch, David, 358

Kronos, 277, 473
Kruger, Lawrence, 147, 149, 150, 215, 315

Kuffler, Stephen, 50–52, 61, 152

L

Lakoff, George, 33

Language, 434–450. See also Speech apes and, 434–439

complexity and, 439–443

cortex and, 441

EEG patterns, 455

as memory system, 355–356

multi-lingual speakers, 455, 457, 459, 477–478

self-expressive use of, 448–450, 459

as self-organizing system, 439, 447, 449, 459

as speech acts, 174

Languages of the Brain (Pribram), 48, 61, 157, 158, 187, 282, 357, 369,
372, 374, 380, 445, 485

Laplace, Pierre-Simon, 96, 489

Lashley, Karl, 20–22, 32, 40–42, 166, 180, 208, 238, 239, 251, 252, 306,
312, 313, 424–429, 435, 484

interference patterns theory, 41–44, 109, 241, 251, 293, 420

Law of Bell and Magendie, 151, 522

Lawrence, Douglas, 280



Learning, as self-organization, 371–374

Left-handedness, 337–338

Leibniz, Gottfried Wilhelm, 417, 418

Leith, Emmet, 42, 43, 92, 93, 387, 424, 515

Leksell, L., 150

Lenses, 92

Leonardo da Vinci, 67, 91, 93, 95, 140, 142, 223

Lévi-Strauss, Claude, 398, 415, 439

Licklider, J. C. R., 102, 103–104

Lie, Sophus, 128–129

Limbic forebrain, 214

electrical and chemical stimulation of, 210–213

Limbic systems (limbic brain; olfactory brain), 4, 32, 198, 213–216,
218, 219

emotions and, 213–214

functions of, 215–217

prefrontal cortex and, 219–220

Lindsley, Donald, 188, 257

Line detectors, 69, 76, 77

Linnaeus, Carolus (Linnaean classification), 348

Llinás, Rudolfo R., 128, 145, 169, 170, 521

Lobotomy topectomy project and, 307–311

Yale lobotomy project, 312–314

Loeb, Jacques, 40

Lorenz, Konrad, 136, 162, 234

Luce, Duncan, 278



Luminance (brightness), 65, 121, 382

Luria, Alexander, 157, 163, 168, 252–254, 316, 317, 404, 448, 449

Lyerly, James, 307

M

McClain, Ernest, 416

McCulloch, Warren, 56, 150, 209, 212

Mace, George, 281

McGaugh, James, 256

McGuinness, Diane, 257, 302

Mach, Ernst, 141–142, 397

MacLean, Paul, 181, 212, 213, 219, 331–333

Magendie, François, 151, 522

Magoun, Horace (Tid), 188, 257

Malach, Rafael, 25

Malis, Leonard, 147, 150, 315

Mandelbrot, Benoît, 440

Mapping the brain, 25–32

correlations, 32

the neuro-nodal web, 50–53

in the 19th century, 25–26, 30

Marcus, Gary, 346, 374–376, 445

Marr, David, 31, 81

Marshall, Wade, 148, 149, 183

Martin, Alex, 509–510

Maslow, Abraham, 135



Masochism, 201–204

Mass, 495–497

Materialism (materialist approach), 9, 12

eliminative, 355, 383–385

Mathematics. See also
Geometry early Greek, 12

Matter, 9

communication and, 496–498

Meaning, 9, 146, 499–511

comprehension of, 509–511

information processing and, 456, 500, 510, 518, 519

two meanings of, 509–511

Means-ends reversal, 281–282

Melzack, Ronald, 202

Membranes, 107, 182–186

Memory, 269. See also
Remembering attitudes and, 298

biological, 356, 359, 376, 403

brain processes involved in, 423–429

compression of, 362–363

eidetic, 404

episodic, 291, 354

familiarization and novelty and, 297

forms of, 354–355

language and, 355–356

representations and, 252–253



retrieval of, 363, 364, 424, 425

retrospective and a prospective aspects of, 356

short-term, 368

as transformations, 355–356, 365, 380–381

Mentalism, 484, 537

Metabolism, 184, 189

eating behavior and, 226

Metaphors and models, 14, 15, 33–45, 387

Metapsychology, 245, 423

Mettler, Fred, 209, 258–259, 306, 307, 309, 310, 314

Metzger, Wolfgang, 166–167

Micro-waves, 42

Midbrain, 188, 189, 217

Middle basal ganglion, 266–267, 478

Miles, Walter, 34, 389

Miller, George, 21, 154, 156, 158, 174, 204, 241, 251, 273, 301, 322,
360, 367, 401, 523

Milner, Peter, 196–198

Mind-blindness, 468

Mind/brain relationship, 408–411

Descartes and, 486–488, 498

Fourier diagram and, 491–493

Min Xie, 360

Miró, Joan, 19

Mirror images, 134, 137, 280

Mirror neurons, 31, 173, 443



Mirror processing, 172

Mirrors, 137–138

concave, 110, 138, 140, 141

Mirsky, Alan, 230

Mishkin, Mort, 35, 68, 508

Mittlestedt, Horst, 158

Momentum, 493, 495, 496

Moniz, Egaz, 311

Monkeys, 24–25, 434–439

thinking, 505–506

Monod, Jacques, 353, 355, 356

Moon illusion, 140

Morgenstern, O., 274, 278, 279

Motivation, 181, 210, 214, 240

Freud’s view of, 297

hippocampal processing and, 295

Motivational feelings, 295

Motivations, definition of, 295

Motor cortex, 146–150, 165, 167–168

actions and, 162

direct sensory input to, 147–150

post-central, 149, 150, 172

pre-central, 149, 150, 160–161

Motor inhibition, 171

Motor theory of language, 14, 173, 174

Movements, 146, 184



actions distinguished from, 161

motor cortex and, 147

perception of objects and, 118–120, 127, 128

pre-central motor cortex and, 160–161

Multiple-choice experiment, 501, 503–505, 507

Muscles motor cortex and, 147

pre-central motor cortex and, 160–161

Music, 363–364, 485–486

difference between speech and, 120–121

EEG patterns, 454–455

fundamental tones and harmonics, 75–76

Mutations, mutations, 350

N

Nader, K., 368

Nafe, John Paul, 192

Names power of, 420–422

psychology and, 422–423

Narrative “I,” 470–476

Neocortex, 216, 218, 331

Neumann, John von, 274, 278, 279

Neural networks, 444

Neuroeconomics, 278, 279

Neuro-nodal web, 50–53, 55

Neuro-nodal webs, 444, 445

Neuronography, 212, 213, 215



Neville, Katherine, 10

Newton, Isaac, 332, 367, 417, 419–422

Nicolelis, Miguel, 452, 453

Normal distributions, 393

Noticing order, 319, 320

Novelty, 262–265, 269

emotions and, 297

hippocampal processing and, 293–294

uncertainty and, 264–265

Null hypothesis, 389

Núñez, Rafael, 33

Nystagmus (nystagmoid oscillations), 118, 119, 127, 170

O

Object constancy, 126–128, 508

Objective “me,” 468–470, 474–476

Objects, 117–130

as constructions, 121–122

form(ing) of, 127–128

images distinguished from, 120–121

movements and perception of, 118–120, 127, 128

Observations brain’s role in the making of, 517–518

of quanta, 515–516

Obsessive-compulsive disorders, 266, 277, 307, 308, 319, 324

Occulocentric space, 132–135

O’Keefe, John, 292



Olds, James, 196–199

Olds, Nicki, 197–198

Olfaction, 389–392

Olfactory brain. See Limbic system Operant conditioning, 164, 371–373

Optic nerve, 155, 156, 517

Orienting reaction, 252, 255, 259, 265

Oscillations, 94, 95, 103, 104, 108

frequencies of, 75

nystagmoid, 118, 119, 127, 170

point attractors and, 129

trigonometry and, 12–13

Oscilloscopes, 147

P

Pain, 179–182, 184, 187, 189, 191–194, 196, 199, 200

endorphins and, 193, 194, 201–203

fast and slow, 194–196

fear and, 32

privacy of experience of, 463–464

temperature and, 190–196, 199

Papez, James, 180, 181, 213, 216, 219, 298

Papez circuit, 180, 213, 216, 217, 219, 221, 286, 316

Parallelism, 397

Parietal lobe, 507, 508

pain and, 193–195

Particles, 392–393, 420, 490



Pathway optimization, 475

Patterns (brain patterns), 48–49, 60, 62, 66, 67, 69, 75, 317, 319

core-brain sensors, 184–187

fixed action, 169–170

form as, 9

forms as, 72–74

interference, 41–44, 109, 241, 251, 293, 420, 490–491, 516, 520, 523,
539

processes that form, 53–55

spectral processing of, 45

transmission of, 419–420

Patterson, Penny, 435, 436, 439

Patton, Paul, 333

Pavlov, 163, 386

Pavlovian, 450

Peirce, Charles, 146, 387, 500, 518

Pendulum, 119, 395

Penfield, Wilder, 150, 180, 188, 210, 305, 427–429

Penrose, Roger, 92, 142, 526

Perception (of objects), 14

belief and, 122–124

brain processes in, 39–40

context and, 136

movements and, 118–120, 127, 128

nature and nurture in, 135–137

Periaqueductal gray, 189, 194, 201, 202



Perspectives, 140

Pert, Candice, 296

PET scans, 99

Petsche, Hellmuth, 293, 454–456

Phenomenology, 485

Philosophy, 202–203

Phospholipids, 107, 183, 370

Phosphorus part of the membrane, 108, 183

Piagetian stages, 302–304, 478

Pinker, Steven, 234

Pituitary gland, 200, 201, 294

Plaid pattern, 77

Planck, Max, 392

Plane geometry, 81

Plans and the Structure of Behavior (Miller, Galanter, and Pribram), 21,
156–158, 171, 204, 251, 301, 360, 368, 401, 445

Play, reversal of the means-ends relation and, 282

Pleasure, 14, 163, 164, 180–182, 184

self-stimulation of the brain and, 196–200

Poincaré, Henri, 117, 128, 130, 133

Point attractors, 119, 129, 363, 395, 517

Polanyi, Michael, 60

Polyak, Stephen, 34

Pool, Larry, 310

Popper, Karl, 397–398

Post-central motor cortex, 149, 150, 172



Potts, Richard, 350

Pragmatics of language, 442–443

Pre-central motor cortex, 149, 150, 160–161

Preferences hierarchical arrangement of, 277–278

utilities and, 274–275

Pre-frontal cortex, 219–220, 304–306, 315, 317–324, 354, 467

Pressure, 191–193

Pribram, Karl H., 273

Prigogine, Ilya, 349, 365, 366, 372

Primacy, 464, 466

Primates brain cortex of, 10

language and, 434–436

Privacy, 462–466

Process psychotherapy (Reich), 480–481

Project for a Scientific Psychology (Freud), 240–245, 297, 476
Projection, 110–115, 173, 218–220, 323

Prosthesis, Chapin and Nicolelis experiment, 453

Psychoanalysis, 240, 241, 243, 245, 338

Psychology phenomenological, 485

as a science, 422–423, 485

Psychosomatic illnesses, 464

Pythagoras (Pythagorean geometry), 12, 13, 415–417, 419

Q

Quanta of information, 80, 102–103, 106, 108, 109, 114, 263, 486, 496,
524, 525



Quantum physics, weirdness of, 522–525

Quantum theory (quantum physics), 82, 83, 93, 108, 114, 393, 395, 420,
422, 463, 491, 496, 499, 500, 514, 515, 517, 519–522, 524, 528, 532

brain’s role in the making of theories, 521

observing quanta, 515–516

R

Radiation, 121

Ramachandran, V. S., 509

Ramón y Cajal, Santiago, 181, 187

Rapp, Paul, 321, 362, 440

Readiness, 297

hippocampal processing and, 293–294

Readiness potentials, 399

Reading, 450

Reality testing, 243–244, 258

Receptive fields, visual, 43, 49, 51, 52, 61–65, 68–70, 73, 74, 76, 77, 82,
84, 86, 87, 106, 107, 109, 110, 123, 141, 168, 258, 364, 375, 382, 486,
516, 525

Reconsolidation of memory, 368–369, 488

Red-green blindness, 464–465

Redish, A. D., 291

Reduction, 397, 411

in uncertainty, 263–264, 494–495, 498

Reflex arc, 151, 154, 160, 169, 170, 522

Reflexes, 151–152, 157

Reich, Willhelm, 480



Reification, 392–393, 522

Reinforcement, 163–164, 373, 374, 401, 490

Remembering brain processes involved in, 423–429

forms of, 354–355

as self-organization, 367–369

Remembrance-of-the-future, 353–377, 532

Representations in the brain, 79, 251, 252, 293, 417, 444, 448, 521

Restak, Richard, 333

Retina, 49–52, 61, 517

Retinal image, 99–101, 109

Retrieval, memory, 363, 364, 424, 425

Reversal of means and ends, 281–282

Ribonucleic acid (RNA), 369–370, 375

Riemann, Bernhard (Riemannian geometry), 67–68, 100, 117

Right hemisphere, language and injury to, 442

Rioch, David, 214–215

Robinson, Daniel, 351

Rose, Jersey, 24

Rosenzweig, Mark, 358

Roska, Botond, 78, 80

Rossler, Otto, 475

Rosvold, Enger, 230

Rumbaugh, Duane and Sue Savage, 437

Russell, Bertrand, 497

Ryle, Gilbert, 124–126, 332



S

Sagan, Carl, 333

Satiety, 199, 203, 228, 259, 266

Savage, Duane and Sue, 437

Savants, 446–447

Scarr, Sandra, 136

Schachter, S., 268

Schrödinger, Erwin, 82, 83, 392, 519

Sciatic nerve, 147–149, 522

Scotoma of action, 165, 166

Searle, John, 123, 171, 172, 174, 487

Sears, Robert, 239

Sechenov, I. M., 157

Selective breeding, 348–349

Self-organizing processes (or systems), 13–15, 55–57, 163–164, 346,
347, 350

archetypes and, 416

evolution and, 349–352

in language, 439, 447, 449, 459

learning as, 371–374

remembering as, 367–369

theme of, 352

Self-stimulation, pleasure and, 196–200

Semantics, 442

Sensors, core-brain, 184–187

Sensory inhibition, 111, 112, 115, 171



Sensory receptors, 24, 36, 153–154, 180, 240

Sensory specificity in the association cortex, 507–509

Settle, Mary Lee, 131

Sexual behavior, 335–337

amygdalectomy and, 232–234

Shannon, C. E., 263, 264, 440, 486, 494–496, 498–500

Shapes, 35–36, 182–183

olfaction and, 390

Shepard, Roger, 141

Shepherd, Gordon, 183

Sherrington, Sir Charles, 151–152, 157, 169, 170, 324

Shigzal, Peter, 279

Singer, J. E., 268

Size constancy, 127, 138, 140, 508

Skin, pain and, 191, 192, 194

Skinner, B. F., 162–164, 351, 352, 371, 372, 383, 401, 402, 422, 485

Smell, sense of, 389–392

Smets, G., 264

Smolensky, Paul, 363, 365

Social context, brain surgery and, 230–231

Social Darwinism, 351

Sokolov, Eugene, 252–255, 297, 488

Spaces, 131–143

egocentric (body-centered), 132–134, 423, 470, 475, 479, 535, 537

occulocentric (eye-centered), 132–135

Spatial frequencies, 62, 68, 69, 73, 74, 76, 77, 135



Spectra, 45

Speech, 173–175, 459. See also
Language action theory of, 174

music distinguished from, 120–121

Sperry, Roger, 20, 159

Spinal cord, 188, 217

segmental pattern of, 188

Spinelli, Nico, 42, 155

Spirit (spirituality), 531–539

Spreading cortical depression, 149

Stabilities, temporary, 349, 366

Stage theories, 302–304

Stamm, John, 38

Stanford, 238, 239

Stapp, Henry, 491, 515, 521

Statistics, 98–99

Stenger, Isabelle, 366

Sternberg, Robert, 261

Stick figures, 40, 41, 52, 61, 65, 118

Stratton, George, 133, 134

Strawson, Galen, 487

Stress, 294–295

Stripes, 62, 71, 73, 126, 138, 168, 280

Structure of redundancy, 300, 320–322, 364, 369, 510

Superego, 243, 338

Symbolic processes, 445



Symmetry groups, 128–130, 134, 135

Synesthesia, 123

Syngamy, 374

T

Tactics (intentions-inaction), 171–172

Taste, sense of, 225, 226

Teitelbaum, Philip, 227–228

Teleology (teleological reasoning), 394, 412–414

Telephone communication, 103, 141

Temperature, 190–196, 199

Temporal hold, 451, 459, 462

Temporary dominant foci of activity, 450–453, 459, 462

Temporary stabilities, 349, 366

Testosterone, 233, 329, 338

Teuber, Hans Lukas, 30, 64, 158, 159

Thalamic projections, 220, 323

Thalamus, 188

dorsal, 150, 189

hypothalamic region, 189, 191, 200, 202, 216, 226, 227, 259, 297,
337

Thatcher, Robert, 323, 324

Thermodynamics, 94, 243, 412, 493–496, 533

Thermostat, controllable, 152, 154, 156–160, 170, 171, 189, 190, 523

Thinking, 450–452

deep structure of, 457–459



monkeys and, 505–506

as more unconscious and preconscious than conscious, 451

Thirst, 200–201

Tichner, E. B., 167

Tinbergen, Niko, 162, 208

Top-down approach, 60, 63, 64, 67, 109, 123, 153, 384

Topectomy project, 306–310

Touch, 191–193

Toynbee, Arnold, 407, 411

Trance, 39

Transfer of training, 250–251

Transformational realism, 403–404

Transformations (transfer functions), 10, 72

complementary forms and, 396–397

coordination as basis for, 402–403

emergence of psychological properties and, 396–397

Leibniz’s monads as, 418–419

Traumatic avoidance experiment, 261

Treelets, 445, 459

Trigonometry, 12, 13, 82

Triune brain, 331–335, 338

Tsien, Joe Z., 365

Turin, Luca, 389–392

U

Ulcers, 464



Uncertainty novelty and, 264–265

reduction in, 263–264, 494–495, 498

Unconscious processing, 476–481

Ungerleiter, Leslie, 508

Utilities, preferences and, 274–275

Uttal, William, 408

V

Values (valuation), 273–283

Varela, Francisco, 365, 366

Vector matrices (vector descriptions), 81–83, 86, 519

Ventricles, 185–187, 189, 191, 194

Veterans Administration, 311, 313

Vibratory theory of olfaction, 390, 392

Vicky (chimpanzee), 435–436

Viennese neurologists, 23

Vinogradova, Olga, 488

Visceral reactions, 210, 211, 262–264, 269, 271, 297

Vision, blind-sight, 467–468

Visual cortex, 36–37

Visual field, 51, 73, 76, 165, 399

Visual processing, pathways of, 508, 509

Visual receptors, 155

W

Wall, Patrick, 202



Wallace, Rains, 229

Ward, Peter, 349

Washoe (chimpanzee), 436–438

Water, shape of, 183

Watts, Alan, 535–536

Wavelets, 75, 79, 108, 109, 118

Waves, 74–75, 419, 420

interference patterns of, 490–491

Weaver, W., 263, 499

Weiskrantz, Lawrence, 35, 71, 230, 231, 467

Werblin, Frank, 78, 80

Whalen, Richard, 335–336

“What is it?” response, 257, 258, 261, 262, 264, 265, 269, 293, 294, 462

Wiener, Norbert, 98, 154

Wiesel, Thorsten, 40, 41, 61, 68

Wigner, Eugene, 496–497, 514, 515

Wilson, Martha, 509

Windowed Fourier transform, 103–106, 109, 364

Woolsey, Clinton, 148

Writing, 450, 459

Y

Yale University, 208–211

Yasue, Kunio, 107, 183, 492

Yerkes Laboratory of Primate Biology, 20, 40, 209, 217, 238–239, 305,
306, 308, 312, 435, 465



Z

Zajonc, Arthur, 523–525

Zeaman, David, 386–387

Zen Buddhism, 535–536



Acknowledgments
I would like to acknowledge the following individuals for their
contributions and support of my research over these past seven decades:

Wolfgang Köhler, Karl Lashley, B.F. Skinner, Stephen Polyak, Eugene
Sokolov, Aleksandr Romanovich Luria, David Bohm, Ilya Prigogine,
Konrad Lorenz, Ralph Gerard, Russell and Karen devalois.

And to all my students over the years at Yale, Harvard, Stanford, Radford,
George Mason, and Georgetown universities, as well as McGill, Montreal,
Bremen, and Ljubljana: you have taught me as much as or more than I
have taught you.


	Titlepage
	Copyright
	Washngton Academy of Sciences
	Contents
	Prologue
	Preface
	Formulations
	Chapter 1: Correlations
	Chapter 2: Metaphors and Models
	Chapter 3: A Biological Imperative
	Chapter 4: Features and Frequencies

	Navigating Our World
	Chapter 5: From Receptor to Cortex and Back Again
	Chapter 6: Of Objects and Images
	Chapter 7: The Spaces We Navigate and Their Horizons
	Chapter 8: Means to Action

	A World Within
	Chapter 9: Pain and Pleasure
	Chapter 10: The Frontolimbic Forebrain: Initial Forays
	Chapter 11: The Four Fs
	Chapter 12: Freud’s Project

	Choice
	Chapter 13: Novelty: The Capturing of Attention
	Chapter 14: Values
	Chapter 15: Attitude
	Chapter 16: The Organ of Civilization
	Chapter 17: Here Be Dragons

	Re-Formulations
	Chapter 18: Evolution and Inherent Design
	Chapter 19: Remembrance of Things Future
	Chapter 20: Coordination and Transformation
	Chapter 21: Minding the Brain

	Applications
	Chapter 22: Talk and Thought
	Chapter 23: Consciousness
	Chapter 24: Mind and Matter
	Chapter 25: Meaning

	Appendix A
	Appendix B
	Index
	Acknowldgements

